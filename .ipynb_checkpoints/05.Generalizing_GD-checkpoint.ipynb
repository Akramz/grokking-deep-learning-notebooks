{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Multiple Weights at a Time: Generalizing Gradient Descent\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Implement Gradient Descent with multiple inputs.\n",
    "- Analyze the effects of freezing one weight.\n",
    "- Implement gradient descent with multiple outputs.\n",
    "- Implement gradient descent with multiple inputs and outputs.\n",
    "- Visualize weight values. \n",
    "- Visual dot products.\n",
    "\n",
    "> [Richard Branson] \"You don't learn to walk by following rules. You learn by doing and by falling over.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Learning with Multiple Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:400px\" src=\"static/imgs/06/many-to-one.png\" /></div>\n",
    "\n",
    "Let's show how a neural network with multiple inputs can learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an empty network with multiple inputs.\n",
    "def w_sum(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    S = 0\n",
    "    for i in range(len(a)):\n",
    "        S += a[i] * b[i]\n",
    "    return S\n",
    "\n",
    "# init weights.\n",
    "weights = [.1, .2, -.1]\n",
    "\n",
    "# defining the model.\n",
    "def neural_network(X, W):\n",
    "    prediction = w_sum(X, W)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT + COMPARE: Making a Prediction, and Calculating Error & Delta.\n",
    "toes  = [ 8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [ 1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "win_or_lose_binary = [1, 1, 0, 1]\n",
    "\n",
    "true = win_or_lose_binary[0]\n",
    "x0 = [toes[0], wlrec[0], nfans[0]]\n",
    "\n",
    "pred = neural_network(x0, weights)\n",
    "\n",
    "error = (pred - true) ** 2\n",
    "delta = pred - true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.189999999999999, -0.09099999999999994, -0.16799999999999987]\n"
     ]
    }
   ],
   "source": [
    "# LEARN: Calculating each weight_delta and putting it on each weight\n",
    "def ele_mul(number, vector):\n",
    "    return [number * v_i for v_i in vector]\n",
    "\n",
    "# we calculate gradients associated w/ each weight.\n",
    "gradients = ele_mul(delta, x0)\n",
    "\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1119, 0.20091, -0.09832]\n",
      "[-1.189999999999999, -0.09099999999999994, -0.16799999999999987]\n"
     ]
    }
   ],
   "source": [
    "# LEARN: Updating the Weights.\n",
    "alpha = .01\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    weights[i] -= alpha * gradients[i]\n",
    "    \n",
    "print(weights)\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Inputs: Explaination\n",
    "\n",
    "The properties involved in GD with multiple inputs are fascinating and worthy of discussion. Let's take a look at them side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Input: Making a Prediction and calculating error and delta.\n",
    "number_of_toes = [8.5]\n",
    "win_or_lose_binary = [1]\n",
    "\n",
    "input = number_of_toes[0]\n",
    "true = win_or_lose_binary[0]\n",
    "weight = 0.3\n",
    "\n",
    "prediction = input * weight\n",
    "error = (prediction - true) ** 2\n",
    "gradient = 2 * input * (prediction - true)\n",
    "delta = prediction - true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you turn a single delta (on the node) into three weight delta values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:800px;\" src=\"static/imgs/06/Conversation.png\" /></div>\n",
    "\n",
    "`Delta` in this case is a measure of how much higher or lower we want the node's value to be, to predict perfectly given the currrent trraining example. `Weight Delta` is a derivative-based estimate of the direction and amount we should move a weight to reduce `node_delta`, accounting for scaling, negative reversal, and stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Watch several steps of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:800px;\" src=\"static/imgs/06/Iteration_1.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, we made three individual error/weight curves, one for each weight. The slope of each curve is reflected by the gradient values. The gradient is steeper for (a) than the others because (a) has an input value that is significantly higher than the others and thus, has a higher derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:800px;\" src=\"static/imgs/06/Iteration_2.png\" /></div>\n",
    "<div style=\"text-align:center;\"><img style=\"width:800px;\" src=\"static/imgs/06/Iteration_3.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the learning was performed on the weight with the largest input because the input changes the slope significantly. This might be a good idea, in fact, there is a technique called `data normalization` that encourages learning across all weights despite data characteristics such as this. \n",
    "\n",
    "This significant difference in slope will force us to set a learning rate lower that the standard one (.01 instead of .1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing one weight: What does it do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing weights is a great exercise to understand how the weights effect each other. We are going to train again, except weight (a) won't ever be adjusted, we will try to learn using only weights (b) and (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Prediction: 0.96376\n",
      "Error: 0.00131\n",
      "Weights: [0.1119, 0.20091, -0.09832]\n",
      "- - - - - -\n",
      "Iteration: 1\n",
      "Prediction: 0.98401\n",
      "Error: 0.00026\n",
      "Weights: [0.1119, 0.20798, -0.08527]\n",
      "- - - - - -\n",
      "Iteration: 2\n",
      "Prediction: 0.99294\n",
      "Error: 5e-05\n",
      "Weights: [0.1119, 0.2111, -0.07952]\n",
      "- - - - - -\n"
     ]
    }
   ],
   "source": [
    "lr = .3\n",
    "\n",
    "for iter in range(3):\n",
    "    pred = neural_network(x0, weights)\n",
    "    \n",
    "    error = (pred - true) ** 2\n",
    "    delta = (pred - true)\n",
    "    \n",
    "    gradients = ele_mul(delta, x0)\n",
    "    gradients[0] = 0\n",
    "    \n",
    "    print(f\"Iteration: {iter}\")\n",
    "    print(f\"Prediction: {round(pred, 5)}\")\n",
    "    print(f\"Error: {round(error, 5)}\")\n",
    "    print(f\"Weights: {[round(w, 5) for w in weights]}\")\n",
    "    print(f\"- - - - - -\")\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= lr * gradients[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:400px\" src=\"static/imgs/06/Experiment_1.png\" />\n",
    "    <img style=\"width:400px\" src=\"static/imgs/06/Experiment_2.png\" />\n",
    "</div>\n",
    "\n",
    "It is somewhat surprising that the rest of the weights were learned despite removing the biggest contributing feature. However, the graph (a) still finds the bottom of the bowel because the whole gradient loss graph changes with error. The black dot can move horizontally only if the weight is updated, but because (a)'s weight is frozen, the dot must stay fixed. However, this does not stop the error to go to 0.\n",
    "\n",
    "This is an extremely important lesson. If we converge using only (b) and (c) then tried to train (a), it wouldn't move because the error already reached 0. In other words, (a) may be a powerful input with lots of predictive power, but if **the network accidentally figures out how to predict accurately on the training data without it, then it will never learn how to incorporate (a) into its prediction.**\n",
    "\n",
    "The 3 graphs representing the loss function according to each input element are. in reality **2D slices of a 4-dimensional space**. 3 of the dimensions arer. the weight values, and the forth is the error. The shape of the 4-D function represents the **error place** or the curvature of the loss function. The curvature is determined by the training data.\n",
    "\n",
    "Our goal is to find the weight configuration at the global minimum of the loss curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Learning with Multiple Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can also make multiple predictions using only a single input. Beyond that, we understand that a simple mechanism (stochastic gradient descent) is constantly being used to perform learning across a wide veriety of architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty network with multiple outputs.\n",
    "weights = [.3, .2, .9]\n",
    "\n",
    "def neural_network(X, W):\n",
    "    prediction = ele_mul(X, W)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT: Making a prediction and calculating error and delta.\n",
    "wlrec = [.65, 1, 1, .9]\n",
    "hurt = [.1, 0, 0, .1]\n",
    "win = [1, 1, 0, 1]\n",
    "sad = [.1, 0, .1, .2]\n",
    "\n",
    "x0 = wlrec[0]\n",
    "target = [hurt[0], win[0], sad[0]]\n",
    "\n",
    "pred = neural_network(x0, weights)\n",
    "\n",
    "error = [0, 0, 0]\n",
    "pure_error = [0, 0, 0]\n",
    "\n",
    "for i in range(len(target)):\n",
    "    error[i] = (pred[i] - target[i]) ** 2\n",
    "    pure_error[i] = pred[i] - target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.195, 0.13, 0.5850000000000001],\n",
       " [0.009025, 0.7569, 0.2352250000000001],\n",
       " [0.095, -0.87, 0.4850000000000001])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, error, pure_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE: calculating each gradient.\n",
    "gradients = [x0 * pure_error[0], x0 * pure_error[1], x0 * pure_error[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: Updating the Weights.\n",
    "lr = 3\n",
    "for i in range(len(weights)):\n",
    "    weights[i] -= lr * gradients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9753749999999997, 16.12025, -0.38887500000000247]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[input * w for w in weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Inputs & Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:200px;\" src=\"static/imgs/06/many-to-many.png\" /></div>\n",
    "\n",
    "SGD naturally generalize to arbitrary architectures. Let's implement SGD with multiple inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. An empty Network with multiple inputs and outputs.\n",
    "weights = [[.1, .1, -.3], \n",
    "           [.1, .2, 0], \n",
    "           [0, 1.3, .1]]\n",
    "\n",
    "def vect_mat_mul(vect, matrix):\n",
    "    assert(len(vect) == len(matrix))\n",
    "    output = [0, 0, 0]\n",
    "    for i in range(len(vect)):\n",
    "        output[i] = w_sum(vect, matrix[i])\n",
    "    return output\n",
    "\n",
    "def neural_network(X, W):\n",
    "    prediction = vect_mat_mul(X, W)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PREDICT: Making a Prediction & Calculating Error & Delta (Pure Error).\n",
    "\n",
    "# Inputs\n",
    "toes =  [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# Outputs\n",
    "hurt = [.1, 0, 0,.1]\n",
    "win  = [ 1, 1, 0, 1]\n",
    "sad  = [.1, 0,.1,.2]\n",
    "\n",
    "# learning rate\n",
    "alpha = .01\n",
    "\n",
    "x0 = [toes[0], wlrec[0], nfans[0]]\n",
    "target = [hurt[0], win[0], sad[0]]\n",
    "\n",
    "prediction = neural_network(x0, weights)\n",
    "error, delta = [0, 0, 0], [0, 0, 0]\n",
    "for i in range(len(prediction)):\n",
    "    error[i] = (prediction[i] - target[i]) ** 2\n",
    "    delta[i] = prediction[i] - target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. COMPARE: Calculating each weight_delta & Putting it on each weight.\n",
    "\n",
    "# we have 9 gradients, or weight deltas.\n",
    "def outer_prod(vect_a, vect_b):\n",
    "    out = np.zeros((len(vect_a), len(vect_b)))\n",
    "    for i in range(len(vect_a)):\n",
    "        for j in range(len(vect_b)):\n",
    "            out[i][j] = vect_a[i]*vect_b[j]\n",
    "    return out\n",
    "\n",
    "gradients = outer_prod(x0, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LEARN: Updating the Weights\n",
    "for i in range(len(weights)):\n",
    "    for j in range(len(weights[0])):\n",
    "        weights[i][j] -= alpha * gradients[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Do These weights Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we understand \"how\" learning happens, another interesting question is \"**what do the weights store while learning?**\"\n",
    "\n",
    "To answer this question, we will move on to the first real-world dataset. It's called the modified national institure of standards and technology (MNIST) dataset. It consists of digits that high school students and employees of the US census bureau wrote some years ago. The interesting thing is that these images are black-and-white images of people's handwriting. Accompanying each digit is the actual number they were writing (0-9). Each image is 784 pixels (28 x 28). \n",
    "\n",
    "In this case, the neural network must have 784 input features. On the other end, we want to predict **10 probabilities**, one for each digit. The neural network will tell us which digit is most likely to be what was drawn.\n",
    "\n",
    "Let's take a look at the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 9s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a sample.\n",
    "images = X_train[0:1000]\n",
    "labels = y_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 28, 28), (1000,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:500px\" src=\"static/imgs/06/MNIST_clear_model.png\" />\n",
    "</div>\n",
    "\n",
    "This diagram represents the new MNIST classification neural network. If this network could perdict perfectly, It would take in an image's pixels (say, a 2) then predict `1` in the correct output position and `0` everywhere else. If it were able to do this correctly for all images in the dataset, it would have no error.\n",
    "\n",
    ".. But, **what does it mean to modify a bunch of weights to learn an aggregate pattern**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Weight Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:400px;\" src=\"static/imgs/06/MNIST_weight_visualization.png\" />\n",
    "</div>\n",
    "\n",
    "An interesting and intuitive practice in neural network research is to visualize the weights as if they were an image. Each output node has a weight coming from each pixel, and so what is this relationship?\n",
    "- If the weight is high, it means the model believes there's a high degree of correlation between that pixel and the number 2.\n",
    "- If the weight is very low (even negative), the model believes there is a very low correlation between that pixel and the number 2.\n",
    "\n",
    "The network learned to construct artifacts of `2` and `1` in the above figure, which were created using the weights for `2` and `1`. The natural color (red) represents a `0`. The key to understanding why these artifacts were created, we should go back to the notion of the **dot product**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Dot Products (Weighted Sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that the dot product is a mathematical measure of similarity.\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([0, 1, 0, 1])\n",
    "b = np.array([1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dot product is a loose measurement of similarity between two vectors. \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:400px\" src=\"static/imgs/06/Neural_Similarity.png\" />\n",
    "</div>\n",
    "\n",
    "Meaning, if the weight vector for `2` is similar to the input vector, then it will output a high score for `2` resulting in a higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Gradient Descent is a General Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a general learning algorithm. The most important subtext in this chapter is that gradient descent is a very flexible algorithm. If we combine. weights in a way that allows us to calculate the error function and a delta, then gradient descent can show us how to reduce error.\n",
    "\n",
    "We will spend the rest of this book exploring different types of weight combinations and error functions for which gradient descent is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/0.jpg\" />\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/1.jpg\" /><br/>\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/2.jpg\" /><br/>\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/3.jpg\" />\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/4'.jpg\" /><br/>\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/5.jpg\" />\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/6.jpg\" />\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
