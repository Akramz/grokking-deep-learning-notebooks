{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Learning about Edges and Corners: Intro to Convolutional Neural Networks\n",
    "\n",
    "In this Chapter:\n",
    "\n",
    "- Reusing Weights in Multiple Places.\n",
    "- The Convolutional Layer.\n",
    "\n",
    "> \"The Pooling Operation used in Convolutional Neural Networks is a big mistake, and the fact that is works so well is a disaster\". â€” Geoffrey Hinton, from \"Ask me anything\" on Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Weights in multiple places\n",
    "### If you need to detect the same feature in multiple places, use the same weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The greatest challenge in NNs is that of overfitting.\n",
    "- Overfitting is often caused by having more parameters than necessary to learn a specific data set.\n",
    "- when NNs have a lot of parameters and few training examples, it becomes very easy to overfit.\n",
    "- There's a better method than regularization to counter overfitting, we call it **structure**.\n",
    "    - Structure is when you selectively choose the same group of weights in multiple purposes in a NN.\n",
    "        - Because we believe the same pattern needs to be detected in multiple places.\n",
    "- The most famous and used structure in NNs is called a **convolution**.\n",
    "    - When user in a layer it's called a **convolutional layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Layer\n",
    "### Lots of very small linear layers are reused in every position, instead of a single big one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The core idea behind a convolutional layer is that instead of having a big dense layer connected to the whole input, we have small but many layers with a limited number of inputs (no more than 25) & one output, that scraps the input matrices one element at a time.\n",
    "- Each mini-layer is called a convolutional kernel, containing the weights/activation function.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img style=\"width:33%;\" src=\"static/imgs/10/convolutional_layer.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shown here is an example of a **3x3 convolutional layer**, it will predict the center pixel values, moves one pixel to the right, predict again, and moves again, and so on.\n",
    "- repeating until it has made a prediction for every possible position in the image.\n",
    "- the resulting image is a 6x6 image.\n",
    "- in CNNs, we usually use many and many filters.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img style=\"width:25%;\" src=\"static/imgs/10/step_0.png\">\n",
    "    <img style=\"width:25%;\" src=\"static/imgs/10/step_1.png\">\n",
    "    <img style=\"width:25%;\" src=\"static/imgs/10/step_2.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You can either sum the outputs of the kernels (element-wise, sum pooling), take the mean element-wise (mean pooling), or take the max value element-wise (max pooling)**.\n",
    "- The last version turns out to be the most popular.\n",
    "- This final matrix is then forward propagated to later layers.\n",
    "- There are a few things to notice in these figures:\n",
    "    - **The Top right kernel forward propagate a 1 only if it's focused on a horizontal line**.\n",
    "    - **The Bottom left kernel forward propagate a 1 only if it's focused on a diagonal line pointing upward and to the right**.\n",
    "- It's important to realize that this technique allows each kernel to search for a certain pattern and mark its existance on every location of the image.\n",
    "- This small-weights-to-large-datasets has a considerable impact on fighting overfitting and increases the networks ability to generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Implementation in NumPy\n",
    "### Just think mini-linear layers, and you already know what you need to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import keras\n",
    "from keras import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b92b550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADqBJREFUeJzt3X+QVfV5x/HPw7KsskoEo1uKFEkgTgxpiW6oMU5NRhPBMV3tJETqWJyhrjMVRxxN6piZlJlOO0xjYokhtmtkghl/kNQwkoxpVNqUsabWVREEpBKzqTDIYrCC0cCy+/SPPTgb2fO91/vr3OV5v2Z29t7z3HPOw4UP597zvfd8zd0FIJ5xRTcAoBiEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUOMbubMJ1uYnqL2RuwRC+a1+o8N+yMp5bFXhN7P5klZKapH0HXdfkXr8CWrXH9tF1ewSQMJTvqHsx1b8st/MWiStkrRA0tmSFpnZ2ZVuD0BjVfOef56kne7+srsflvSgpK7atAWg3qoJ/zRJr4y4vytb9jvMrNvMes2sd0CHqtgdgFqq+9l+d+9x905372xVW713B6BM1YR/t6TpI+6fkS0DMAZUE/6nJc02s5lmNkHSlZLW16YtAPVW8VCfux8xs6WSfqrhob7V7r61Zp0BqKuqxvnd/RFJj9SoFwANxMd7gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKqqWXrNrE/SQUmDko64e2ctmgJQf1WFP/Npd3+tBtsB0EC87AeCqjb8LulRM3vGzLpr0RCAxqj2Zf8F7r7bzE6X9JiZvejuG0c+IPtPoVuSTtDEKncHoFaqOvK7++7sd7+kdZLmjfKYHnfvdPfOVrVVszsANVRx+M2s3cxOPnpb0mclvVCrxgDUVzUv+zskrTOzo9u5393/tSZdAai7isPv7i9L+qMa9gI0zFdffjZZn9fmVW2/82s3JOu/949PVrX9WmCoDwiK8ANBEX4gKMIPBEX4gaAIPxBULb7Vh4LZ+Py/xn1LPp5cd8rWt5P1cU9sqqinZtC/9Pzc2uzW/0yuO6QJyXrvoZZkffKOgWS9GXDkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOc/Duy65ZgLKL3jmRtWJte9+pfzk/U3u6Yk64O/3p+s19P4GdOT9ZuWfj+39r5x6XH8Ur6y88+S9bafPF3V9huBIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4f6Zl0qRkffDAgQZ1cqxx7e3J+h9c0lfxtr81Y32yfs3p16Q3UOA4/4Fzfz9Z/+LJe3JrOwYGk+v27LswWR+8qyNZl/pK1IvHkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgio5zm9mqyVdJqnf3edky6ZIWivpTA0PaC5099fr12b9vfrnH0nWT/unn9dt3y2npr8z/8Z9pyTr//ahtRXv+9Pf/lKyfsb24qaStra2ZP3UG/sq3vZf3XJjsv5/s9LX5Z+2rvgptqtVzpH/u5LefcWHWyVtcPfZkjZk9wGMISXD7+4bJb37Y1xdktZkt9dIurzGfQGos0rf83e4+9HPTr4qqdRnHQE0mapP+Lm7S/K8upl1m1mvmfUO6FC1uwNQI5WGf6+ZTZWk7Hd/3gPdvcfdO929s1XpEzgAGqfS8K+XtDi7vVjSw7VpB0CjlAy/mT0g6eeSzjKzXWa2RNIKSZ8xs5ckXZzdBzCGlBznd/dFOaWLatxLoeo5jl/q+/glx/E/mh7Hf2sofy74cx6/Ibnuh+/ekaynv/VeX//75XOT9edmpeck+MLOz+XWdl+ce5pKknTGT4v8kzcGn/ADgiL8QFCEHwiK8ANBEX4gKMIPBMWlu2ug1FdPr9v0fLK+YGL629CpoTxJOnf9Tbm1D13/VHLdIge0Bi5OD+X9YMnXS2wh/c/3n2f+S27tsvvTX2WeuK5+Q7/NgiM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOH+ZWiZPzq29+WD6K7kLJj6RrP/4N6cm6393x1XJ+uy7xuaY9CV3bEzWZ7VW98/zmpeuzK2dfv8LyXWHqtrz2MCRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpw/03LK+5L11+/Pn0b7Z3MeTK77xtDhZP2bN+ePR0vSaT8am+P4kvTrv/xEbu3zk75WYu30dRJWvX5Wst7yxbdza4MHD5bY9/GPIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFVynN/MVku6TFK/u8/Jli2XdK2kfdnDbnP3R+rVZCPsWP7hZH3bH95Z8bbbLP1/bPftD6U3cHu6fHDwhNzayrVdyXXPW7AlWb948rZkvaXEN9/PPzH/WgYdLelx/FJWPXdhsj7rteeq2v7xrpwj/3clzR9l+R3uPjf7GdPBByIqGX533yhpfwN6AdBA1bznX2pmm81stZnlX+MKQFOqNPx3SfqgpLmS9kjKnVTNzLrNrNfMegd0qMLdAai1isLv7nvdfdDdhyTdLWle4rE97t7p7p2tJb6oAaBxKgq/mU0dcfcKSelLoQJoOuUM9T0g6VOS3m9muyT9jaRPmdlcSS6pT9J1dewRQB2UDL+7Lxpl8T116KVQQ5OOJOvjqjg3OtEmJOsLT+qveNulLOmu/PMJ5Wi1lmR9wE/MrT36dnty3WX/nb7OwayrGcevBp/wA4Ii/EBQhB8IivADQRF+ICjCDwTFpbszs78zkKxf8LOlDeqkueyfn3/5a0nadmF61Hfz4cHc2qqu9NeNP7B1U7KO6nDkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfP2JPPJ+unPNmgRhrsjavOS9aXzd2QrP/yyG+T9au+d0tubcbWsTv1+PGAIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4/3Fu/MwZyfqyr65N1s8/8ZVk/S+uuylZn/ETxvKbFUd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq5Di/mU2XdK+kDkkuqcfdV5rZFElrJZ0pqU/SQnd/vX6tIs+49vyprgfvSU89fkWJ6cE/eu+XkvWZjOOPWeUc+Y9Iutndz5Z0nqTrzexsSbdK2uDusyVtyO4DGCNKht/d97j7s9ntg5K2S5omqUvSmuxhayRdXq8mAdTee3rPb2ZnSvqYpKckdbj7nqz0qobfFgAYI8oOv5mdJOkhScvc/cDImru7hs8HjLZet5n1mlnvgA5V1SyA2ikr/GbWquHg3+fuP8wW7zWzqVl9qqRRzxy5e4+7d7p7Z6vaatEzgBooGX4zM0n3SNru7t8YUVovaXF2e7Gkh2vfHoB6KecrvZ+UdLWkLWZ2dM7k2yStkPR9M1si6VeSFtanRZTy4u0fya+dtSq57pz/uDZZn/W36UuaDyWraGYlw+/uT0iynPJFtW0HQKPwCT8gKMIPBEX4gaAIPxAU4QeCIvxAUFy6uwnY+PRfw447z0nWt37uW7m1x9+elFx31or0FNtDb72VrGPs4sgPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzt8EfvH3H0/WX/zTbybrbwwN5NbuvPLzyXV98wvJOo5fHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+ZvASWdVN7P5J35wc25tVu9/VbVtHL848gNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUCXH+c1suqR7JXVIckk97r7SzJZLulbSvuyht7n7I/VqNLJLtn4hWZ/95d7cmte6GRw3yvmQzxFJN7v7s2Z2sqRnzOyxrHaHu99ev/YA1EvJ8Lv7Hkl7stsHzWy7pGn1bgxAfb2n9/xmdqakj0l6Klu01Mw2m9lqM5ucs063mfWaWe+ADlXVLIDaKTv8ZnaSpIckLXP3A5LukvRBSXM1/Mrg66Ot5+497t7p7p2taqtBywBqoazwm1mrhoN/n7v/UJLcfa+7D7r7kKS7Jc2rX5sAaq1k+M3MJN0jabu7f2PE8qkjHnaFJC4DC4wh5Zzt/6SkqyVtMbNN2bLbJC0ys7kaHk3qk3RdXToM4PSuF6tan+E8VKKcs/1PSLJRSozpA2MYn/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZe6N+za4me2T9KsRi94v6bWGNfDeNGtvzdqXRG+VqmVvM9z9tHIe2NDwH7Nzs1537yysgYRm7a1Z+5LorVJF9cbLfiAowg8EVXT4ewref0qz9tasfUn0VqlCeiv0PT+A4hR95AdQkELCb2bzzWyHme00s1uL6CGPmfWZ2RYz22Rm+dPfNqaX1WbWb2YvjFg2xcweM7OXst+jTpNWUG/LzWx39txtMrNLC+ptupn9u5ltM7OtZnZjtrzQ5y7RVyHPW8Nf9ptZi6T/kfQZSbskPS1pkbtva2gjOcysT1Knuxc+JmxmfyLpTUn3uvucbNk/SNrv7iuy/zgnu/tfN0lvyyW9WfTMzdmEMlNHziwt6XJJ16jA5y7R10IV8LwVceSfJ2mnu7/s7oclPSipq4A+mp67b5S0/12LuyStyW6v0fA/nobL6a0puPsed382u31Q0tGZpQt97hJ9FaKI8E+T9MqI+7vUXFN+u6RHzewZM+suuplRdGTTpkvSq5I6imxmFCVnbm6kd80s3TTPXSUzXtcaJ/yOdYG7nyNpgaTrs5e3TcmH37M103BNWTM3N8ooM0u/o8jnrtIZr2utiPDvljR9xP0zsmVNwd13Z7/7Ja1T880+vPfoJKnZ7/6C+3lHM83cPNrM0mqC566ZZrwuIvxPS5ptZjPNbIKkKyWtL6CPY5hZe3YiRmbWLumzar7Zh9dLWpzdXizp4QJ7+R3NMnNz3szSKvi5a7oZr9294T+SLtXwGf9fSPpKET3k9PUBSc9nP1uL7k3SAxp+GTig4XMjSySdKmmDpJckPS5pShP19j1JWyRt1nDQphbU2wUafkm/WdKm7OfSop+7RF+FPG98wg8IihN+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n+sClNKop8J1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing one example:\n",
    "plt.imshow(X_train[89].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example\n",
    "image_example = X_train[89].squeeze()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
    "    return layer[row_from:row_to+1, col_from:col_to+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12ba05940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(get_image_section(image_example, 3, 13, 5, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book implementation.\n",
    "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
    "    sub_section = layer[:, row_from:row_to, col_from:col_to]\n",
    "    return sub_section.reshape(-1, 1, row_to-row_from, col_to-col_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12e2295c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(get_image_section(image_example.reshape(1,28,28), 3, 13, 5, 15).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because this method selects a subsection of a batch of input images, you need to call it multiple times (on every location within the image).\n",
    "- Such a \"for\" loop looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = X_train\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_start, batch_end = 0, 7\n",
    "layer_0 = images[batch_start:batch_end]\n",
    "layer_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = list()\n",
    "kernel_rows, kernel_cols = 3, 3\n",
    "for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "    for col_start in range(layer_0.shape[2]-kernel_cols):\n",
    "        section = get_image_section(layer_0, row_start, row_start+kernel_rows, col_start, col_start+kernel_cols)\n",
    "        sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 625, 3, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_input = np.concatenate(sections, axis=1)\n",
    "expanded_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4375, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = expanded_input.shape\n",
    "flattened_input = expanded_input.reshape(es[0]*es[1], -1)\n",
    "flattened_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this code, layer_0 is a batch of images 28x28 in shape.\n",
    "- The for loop slides over every subregion in the images, extract and append it to a list called `sections`.\n",
    "- This list of sections is then concatenated and reshaped in a peculiar way.\n",
    "- Pretend for now that each individual subregion is its own image.\n",
    "- Forward-propagating them through a linear layer with one output neuron is the same as predicting that linear layer over every sub-region in every batch.\n",
    "- The following listing shows the entire NumPy implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import exit\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (1000,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = (x_train[:1000].reshape(1000, 28*28) / 255, y_train[:1000])\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros(shape=(labels.shape[0], 10))\n",
    "for i, v in enumerate(labels):\n",
    "    one_hot_labels[i][v] = 1\n",
    "labels = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = x_test.reshape(x_test.shape[0], 28*28) / 255\n",
    "test_labels = np.zeros(shape=(y_test.shape[0], 10))\n",
    "for i, v in enumerate(y_test):\n",
    "    test_labels[i][v] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def grad_tanh(y):\n",
    "    return 1 - y**2\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = (2, 300)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 128\n",
    "\n",
    "input_rows, input_cols = (28, 28)\n",
    "kernel_rows, kernel_cols = (3, 3)\n",
    "num_kernels = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden is of that size because we are training a 1-D Convolutional Layer, not a 2D, and so it's taking more space (try it).\n",
    "hidden_size = ((input_rows-kernel_rows+1) * (input_cols-kernel_cols+1)) * num_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 16), (10816, 10))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = 0.02*np.random.random((kernel_rows*kernel_cols, num_kernels)) - 0.01\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "kernels.shape, weights_1_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
    "    sub_section = layer[:, row_from:row_to, col_from:col_to]\n",
    "    return sub_section.reshape(-1, 1, row_to-row_from, col_to-col_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):  # for each epoch\n",
    "    correct_count = 0\n",
    "    for batch in range(int(len(images) / batch_size)):  # for each batch\n",
    "        batch_start, batch_end = (batch*batch_size), ((batch+1)*batch_size)\n",
    "        \n",
    "        # get input\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "        \n",
    "        # extract all sections from all batch images\n",
    "        sections = list()\n",
    "        for row_start in range(layer_0.shape[1]-kernel_rows+1):  # +1 by akramz.\n",
    "            for col_start in range(layer_0.shape[2]-kernel_cols+1):\n",
    "                section = get_image_section(layer_0, row_start, row_start+kernel_rows, col_start, col_start+kernel_cols)\n",
    "                sections.append(section)\n",
    "        # [0] 128 batch elements, [1] 676 sections/image, 3x3 section size \n",
    "        expanded_input = np.concatenate(sections, axis=1)\n",
    "        \n",
    "        es = expanded_input.shape\n",
    "        # shape: (128x676, 3x3)\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1], -1)  # WEIRD STUFF ????\n",
    "        \n",
    "        # forward propagation\n",
    "        kernel_output = flattened_input.dot(kernels)  # 16 kernels\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0], -1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask*2\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        # count corrects\n",
    "        for k in range(batch_size):\n",
    "            labelset = labels[batch_start+k:batch_start+k+1]\n",
    "            _inc = int(np.argmax(layer_2[k:k+1]) == np.argmax(labelset))\n",
    "            correct_count += _inc\n",
    "        \n",
    "        # back propagation\n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2)/(batch_size*layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * grad_tanh(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        weights_1_2 += lr * layer_1.T.dot(layer_2_delta)\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_udpate = flattened_input.T.dot(l1d_reshape)\n",
    "        kernels -= lr * k_udpate\n",
    "        \n",
    "        # test\n",
    "        test_correct_count = 0\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "            sections = list()\n",
    "            for row_start in range(layer_0.shape[1]-kernel_rows+1):\n",
    "                for col_start in range(layer_0.shape[2]-kernel_cols+1):\n",
    "                    section = get_image_section(layer_0, row_start, row_start+kernel_rows, col_start, col_start+kernel_cols)\n",
    "                    sections.append(section)\n",
    "            expanded_input = np.concatenate(sections, axis=1)\n",
    "            es = expanded_input.shape\n",
    "            flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "            kernel_output = flattened_input.dot(kernels) \n",
    "            layer_1 = tanh(kernel_output.reshape(es[0],-1)) \n",
    "            layer_2 = np.dot(layer_1,weights_1_2)\n",
    "            test_correct_count += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "    if(epoch % 1 == 0): \n",
    "        print(\"\\n\"+ \"I:\" + str(epoch) + \" Test-Acc:\"+str(test_correct_count/float(len(test_images)))+ \" Train-Acc:\" + str(correct_count/float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, swapping out the first layer from the network in chapter 9 with a convolutional layer gives another few percentage points in error reduction.\n",
    "- **The output of the convolutional layer is itself also a series of two-dimensional images**.\n",
    "- Most uses of convolutional layers stack multiple layers on top of each other.\n",
    "    - such that each layer treats the previous output as its input.\n",
    "- **Stacked Convolutional Layers are one of the main developments that allowed for very deep neural networks.**\n",
    "    - & by extension, the popularization of the phrase \"deep learning\".\n",
    "- **It can't be overstressed that this invension was a landmark moment for the field; without it, we might still be in the previous AI Winter even at the time of writing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Reusing weights is one of the most important innovations in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The notion of reusing weights to increase accuracy is hugely important and has an intuitive basis.\n",
    "- **Consider what you need to understand to detect that a cat is in an image.**\n",
    "    - You first need to understand **colors**.\n",
    "    - then **lines and edges**.\n",
    "    - **Corners & small shapes**.\n",
    "    - & finally the combination of such low-level features that corresponds to a cat.\n",
    "- NNs also need to learn such low-level features.\n",
    "- The intelligence for detecting lines and edges is learned in its weights.\n",
    "- **But if you use different weights to analyze different parts of an image, each section of the weights has to learn what a line is.**\n",
    "- **The Structure Trick**\n",
    "    - **When a Neural Network needs to use the same idea in multiple places, endeavor to use the same weights in multiple places.**\n",
    "    - This will make the weights more intelligent by giving them more samples (sections) to learn from, increasing generalization.\n",
    "- Many of the biggest developments in deep learning over the past five years are iterations of this idea:\n",
    "    - CNNs, RNNs, Word Embeddings, & the recently published capsule networks can all be viewed through this lens.\n",
    "- When you know you'll need the same idea in multiple places, force it to use the same weights in those places."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
