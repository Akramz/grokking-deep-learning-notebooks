{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ggd\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Multiple Weights at a Time: Generalizing Gradient Descent\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This Chapter:\n",
    "\n",
    "- Gradient Descent Learning with multiple Inputs\n",
    "- Freezing One Weight: One does it do?\n",
    "- Gradient Descent Learning with Multiple Outputs\n",
    "- Gradeint Descent Learning with Multiple Inputs and Outputs\n",
    "- Visualizing Weight Values\n",
    "- Visualizing Dot Products\n",
    "\n",
    "> \"You don't learn to walk by following rules. You learn by doing and by falling over.\" - Richard Branson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Learning with Multiple Inputs\n",
    "### Gradient Descent Also works with Multiple Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:500px\" src=\"static/imgs/06/many-to-one.png\" /></div>\n",
    "\n",
    "- The following code shows how a network with multiple inputs can learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an empty network with multiple inputs.\n",
    "def w_sum(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    S = 0\n",
    "    for i in range(len(a)):\n",
    "        S += a[i]*b[i]\n",
    "    return S\n",
    "\n",
    "# init weights.\n",
    "weights = [.1, .2, -.1]\n",
    "\n",
    "# defining the model.\n",
    "def neural_network(input, weights):\n",
    "    prediction = w_sum(input, weights)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT + COMPARE: Making a Prediction, and Calculating Error & Delta.\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "win_or_lose_binary = [1, 1, 0, 1]\n",
    "\n",
    "true = win_or_lose_binary[0]\n",
    "input = [toes[0], wlrec[0], nfans[0]]\n",
    "\n",
    "pred = neural_network(input, weights)\n",
    "\n",
    "error = (pred - true) ** 2\n",
    "delta = pred - true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARN: Calculating each weight_delta and putting it on each weight\n",
    "def ele_mul(number, vector):\n",
    "    return [number * v_i for v_i in vector]\n",
    "\n",
    "# we calculate gradients associated w/ each weight.\n",
    "gradients = ele_mul(delta, input)\n",
    "\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1119, 0.20091, -0.09832]\n",
      "[-1.189999999999999, -0.09099999999999994, -0.16799999999999987]\n"
     ]
    }
   ],
   "source": [
    "# LEARN: Updating the Weights.\n",
    "alpha = .01\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    weights[i] -= alpha * gradients[i]\n",
    "    \n",
    "print(weights)\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Inputs Exaplained\n",
    "### Simple to Execute, and Fascinating to Understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Properties involved are fascinating and worthy of discussion\n",
    "- Let's take a look at them side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Input: Making a Prediction and calculating error and delta.\n",
    "number_of_toes = [8.5]\n",
    "win_or_lose_binary = [1]\n",
    "\n",
    "input = number_of_toes[0]\n",
    "true = win_or_lose_binary[0]\n",
    "weight = 0.3\n",
    "\n",
    "prediction = input * weight\n",
    "error = (prediction - true) ** 2\n",
    "gradient = 2 * input * (prediction - true)\n",
    "delta = prediction - true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you turn a single delta (on the node) into three weight delta values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/06/Conversation.png\" /></div>\n",
    "\n",
    "- Delta: A Measure of how much higher or lower you want a node's value to be, to predict perfectly given the current training example.\n",
    "- Weight delta: A derivative-based Estimate of the direction and amount you should move a weight to reduce node_delta, accounting for scaling, negative reversal, and stopping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Watch several steps of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/06/Iteration_1.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can make three individual error/weight curves, on for each weight.\n",
    "- The Slopes of these curves are reflected by the gradient values.\n",
    "- Why is the gradient steeper for (a) than the others if they share the same prediction and error.\n",
    "    - Because (a) has an input value that's significantly higher than the others and thus, a higher derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/06/Iteration_2.png\" /></div>\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/06/Iteration_3.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the learning (weight changing) was performaed on the weight with the largest input.\n",
    "    - Because the input changes the slope significantly.\n",
    "- This isn't necessarily advantageous in all settings.\n",
    "    - A subfield called *normalization* helps encourage learning across all weights despite dataset characteristics such as this.\n",
    "- This significant difference in slope forced me to set alpha lower than i wanted.(.01 instead of .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing one weight: What does it do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This experiment is a bit advanced in terms of theory.\n",
    "    - But i think it's a great exercise to understand how the weights effect each other.\n",
    "- you're going to train again, except weight (a) won't ever be adjusted.\n",
    "    - You'll try to learn the training example using only weights (b) and (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0\n",
      "Pred : 0.8600000000000001\n",
      "Error : 0.01959999999999997\n",
      "Weights : [0.1, 0.2, -0.1]\n",
      "Iteration : 1\n",
      "Pred : 0.9382250000000001\n",
      "Error : 0.003816150624999989\n",
      "Weights : [0.1, 0.2273, -0.04960000000000005]\n",
      "Iteration : 2\n",
      "Pred : 0.97274178125\n",
      "Error : 0.000743010489422852\n",
      "Weights : [0.1, 0.239346125, -0.02736100000000008]\n"
     ]
    }
   ],
   "source": [
    "lr = .3\n",
    "\n",
    "for iter in range(3):\n",
    "    pred = neural_network(input, weights)\n",
    "    \n",
    "    error = (pred - true) ** 2\n",
    "    delta = (pred - true)\n",
    "    \n",
    "    gradients = ele_mul(delta, input)\n",
    "    gradients[0] = 0\n",
    "    \n",
    "    print(\"Iteration : \" + str(iter))\n",
    "    print(\"Pred : \" + str(pred))\n",
    "    print(\"Error : \" + str(error))\n",
    "    print(\"Weights : \" + str(weights))\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= lr * gradients[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:30%\" src=\"static/imgs/06/Experiment_1.png\" />\n",
    "    <img style=\"width:30%\" src=\"static/imgs/06/Experiment_2.png\" />\n",
    "</div>\n",
    "\n",
    "- Perhaps you are surprised that you remained weights learned despite removing the biggest contributing feature.\n",
    "- graph (a) still finds bottom of bowel because even though numerical value of weight didn't chnage, gradient loss graph changes (w/ error).\n",
    "    - The black dot can move horizontally only if the weight is updated.\n",
    "    - because (a) weight is frozen for the experiment, the dot must stay fixed, but the error goes to zero.\n",
    "- This is an extremely important Lesson.\n",
    "    - First, if you converged (reached Error == 0) with (b) and (c) weights & then tried to train (a), (a) wouldn't move.\n",
    "        - Why ? error == 0 means gradient = 2 * input * (pred - true) == 0.\n",
    "        - This Reveals a potentially damaging property of neural networks:\n",
    "            - (a) may be a powerful input with lots of predictive power, But **if the network accidentally figures out how to predict accurately on the training data without it, then it will never learn how to incorporate (a) into its prediction.**\n",
    "- Notice How (a) finds the Bottom of the bowel. Instead of the black dot moving, the curve seems to move to the left. What does this mean?\n",
    "    - The Black dot can move horizontally only if the weight is updated.\n",
    "        - **But it can move vertically**.\n",
    "- The 3 graphs representing Loss function according to each input element are in reality **2D Slices of a 4-dimensional Space**.\n",
    "    - 3 of the dimensions are the weight values, and the forth is the error.\n",
    "- The shape of the 4-D function represents the **error plane**.\n",
    "    - **Its Curvature is determined by the training data**.\n",
    "- *Error* is determined by training data.\n",
    "    - Any network can have any weight value\n",
    "        - But the value of error given any weight configuration is 100% determined by data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Learning with Multiple Outputs\n",
    "### Neural Networks can also make multiple predictions using only a single input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this Point, I hope it's clear that a simple mechanism (stochastic gradient descent) is constantly used to perform learning across a wide veriety of architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty network with multiple outputs.\n",
    "weights = [.3, .2, .9]\n",
    "def neural_network(input, weights):\n",
    "    prediction = ele_mul(input, weights)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT: Making a prediction and calculating error and delta.\n",
    "wlrec = [.65, 1, 1, .9]\n",
    "hurt = [.1, 0, 0, .1]\n",
    "win = [1, 1, 0, 1]\n",
    "sad = [.1, 0, .1, .2]\n",
    "\n",
    "input = wlrec[0]\n",
    "target = [hurt[0], win[0], sad[0]]\n",
    "\n",
    "pred = neural_network(input, weights)\n",
    "\n",
    "error = [0, 0, 0]\n",
    "pure_error = [0, 0, 0]\n",
    "\n",
    "for i in range(len(target)):\n",
    "    error[i] = (pred[i] - target[i]) ** 2\n",
    "    pure_error[i] = pred[i] - target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.195, 0.13, 0.5850000000000001],\n",
       " [0.009025, 0.7569, 0.2352250000000001],\n",
       " [0.095, -0.87, 0.4850000000000001])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, error, pure_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE: Calculating each gradient.\n",
    "gradients = [input * pure_error[0], input * pure_error[1], input * pure_error[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: Updating the Weights.\n",
    "lr = 3\n",
    "for i in range(len(weights)):\n",
    "    weights[i] -= lr * gradients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07458749999999997, 1.2327249999999998, -0.02973750000000019]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[input * w for w in weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Inputs & Outputs\n",
    "### Gradient Descent generalizes to arbitrary large networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:25%\" src=\"static/imgs/06/many-to-many.png\" /></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. An empty Network with multiple inputs and outputs.\n",
    "weights = [[.1, .1, -.3], [.1, .2, 0], [0, 1.3, .1]]\n",
    "\n",
    "def vect_mat_mul(vect, matrix):\n",
    "    assert(len(vect) == len(matrix))\n",
    "    output = [0,0,0]\n",
    "    for i in range(len(vect)):\n",
    "        output[i] = w_sum(vect, matrix[i])\n",
    "    return output\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = vect_mat_mul(input, weights)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PREDICT: Making a Prediction & Calculating Error & Delta (Pure Error).\n",
    "\n",
    "# Inputs.\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65,0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# Outputs.\n",
    "hurt = [0.1, 0.0, 0.0, 0.1]\n",
    "win = [1, 1, 0, 1]\n",
    "sad = [0.1, 0.0, 0.1, 0.2]\n",
    "\n",
    "# learning rate.\n",
    "alpha = .01\n",
    "\n",
    "input = [toes[0], wlrec[0], nfans[0]]\n",
    "target = [hurt[0], win[0], sad[0]]\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "error, delta = [0, 0, 0], [0, 0, 0]\n",
    "for i in range(len(prediction)):\n",
    "    error[i] = (prediction[i] - target[i]) ** 2\n",
    "    delta[i] = prediction[i] - target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. COMPARE: Calculating each weight_delta & Putting it on each weight.\n",
    "\n",
    "# we have 9 gradients, or weight deltas.\n",
    "def outer_prod(vect_a, vect_b):\n",
    "    out = np.zeros((len(vect_a), len(vect_b)))\n",
    "    for i in range(len(vect_a)):\n",
    "        for j in range(len(vect_b)):\n",
    "            out[i][j] = vect_a[i]*vect_b[j]\n",
    "    return out\n",
    "\n",
    "gradients = outer_prod(input, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LEARN: Updating the Weights\n",
    "for i in range(len(weights)):\n",
    "    for j in range(len(weights[0])):\n",
    "        weights[i][j] -= alpha * gradients[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Do These weights Learn?\n",
    "### Each Weight Tries to reduce the Error, But what do they learn in aggregate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Congratulations! This is the part of the book where we move on to the first real-world dataset.\n",
    "    - As Luck would have it, **It's one with historical significance**.\n",
    "- It's called the modified national institute of standards and technology (MNIST) dataset.\n",
    "    - It consits of digits that high school students and employees of the US census bureau wrote some years ago.\n",
    "    - The interesting thing is that these images are black-and-white images of people's handwriting.\n",
    "    - Accompanying each digit is the actual number they were writing (0-9).\n",
    "- Each Image is 784 pixels (28x28).\n",
    "- Each Training example contain 784 values.\n",
    "    - So the neural network must have 784 input values.\n",
    "- **You want to predict 10 probabilities**\n",
    "    - One for each digit.\n",
    "    - the neural network tells you which digit is most likely to be what was drawn.\n",
    "- Let's Take a look at the MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a sample.\n",
    "images = X_train[0:1000]\n",
    "labels = y_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 28, 28), (1000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/MNIST_clear_model.png\" />\n",
    "</div>\n",
    "\n",
    "- This Diagram represents the new MNIST classification Neural Network.\n",
    "- If this network could predict perfectly, It would take in an image's pixels (say, a 2) & predict a 1.0 in the correct output position.\n",
    "    - .. and a 0 everywhere else.\n",
    "- if it were able to do this correctly for all images in the dataset, It would have no error.\n",
    "- But, **what does it mean to modify a bunch of weights to learn a pattern in aggregate**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Weight Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:33%\" src=\"static/imgs/06/MNIST_weight_visualization.png\" />\n",
    "</div>\n",
    "\n",
    "- An interesting & intuitive practice in neural network research is to visualize the weights as if they were an image.\n",
    "- Each output node has a weight coming from every pixel.\n",
    "- What is this relationship?\n",
    "    - If the **weight is high, It means the model believes there's a high degree of correlation between that pixel and the number 2**.\n",
    "    - If the **weight is very low (even negatives), the model believes there is a very low correlation between that pixel and the number 2**.\n",
    "- A very vague 2 and 1 appear in the two images, which were created using the weights for 2 and 1.\n",
    "- The Bright Areas are high weights, and the dark areas are negative weights.\n",
    "- The neutral color (red) represents a 0.\n",
    "- Why does it turn out this way?\n",
    "    - This Takes us back to **Dot Products**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Dot Products (Weighted Sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that the dot product is a mathematical measure of similarity.\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([0, 1, 0, 1])\n",
    "b = np.array([1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A Dot Product is a Loose measurement of similarity between two vectors.**\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/Neural_Similarity.png\" />\n",
    "</div>\n",
    "\n",
    "- Meaning, If the weight vector is similar to the *Input* Vector for 2, then it will output a high score because the two scores are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Gradient Descent is a General Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Most important Subtext of this chapter is that gradient descent is a very flexible learning algorithm.\n",
    "- If you Combine weights in a way that allows you to calculate an error function and a delta, gradient descent can show you how to reduce error.\n",
    "- We'll spend the rest of this book exploring different types of weight combinations & Error Functions for which gradient descent is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/0.jpg\" />\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/1.jpg\" /><br/>\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/2.jpg\" /><br/>\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/3.jpg\" />\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/4'.jpg\" /><br/>\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/5.jpg\" />\n",
    "    <img style=\"width:333px\" src=\"static/imgs/06/6.jpg\" />\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
