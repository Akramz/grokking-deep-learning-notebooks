{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Building your first Deep Neural Network: Introduction to Backpropagation\n",
    "\n",
    "In this Chapter:\n",
    "- The Streetlight Problem.\n",
    "- Matrices and the Matrix relationship.\n",
    "- Full, Batch, and Stochastic Gradient Descent.\n",
    "- Neural Networks learn Correlation.\n",
    "- Overfitting.\n",
    "- Creating your Own correlation.\n",
    "- Backpropagation: Long-distance error attribution.\n",
    "- Linear versus Non-Lienar.\n",
    "- The Secret to Sometimes Correlation.\n",
    "- Your first Deep Network.\n",
    "- Backpropagation in Code: Bringing it all together.\n",
    "\n",
    "> *\"O Deep Thought Computer,\" he Said, \"The task we have designed you to perform is this. We want you to tell us...\" he paused. \"The Answer\"* - Douglas Adams, The Hitchhiker's Guide to the Galaxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Streetlight Problem\n",
    "### This toy problem considers how a network learn entire datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider yourself approaching a street corner in a foreign country.\n",
    "    - As you approach, you look up and realize that the street light is unfamiliar.\n",
    "        - How can you know when it's safe to cross the street?\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:33%\" src=\"static/imgs/06/streetlight_data.png\" />\n",
    "</div>\n",
    "\n",
    "- To Solve this problem, you might sit at a street corner for a few minutes observing the correlation between each light combination and whether people around you choose to stop or walk.\n",
    "- You can say that there is a perfect correlation between the middle light and whether it's safe to walk.\n",
    "- You learned this pattern by observing all individual data points and **searching for correlation** .\n",
    "    - **This is What you're going to train a neural network to do!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "### Neural Networks Don't Read Sreetlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have two datasets\n",
    "    - On the one hand, you have six streetlight states.\n",
    "    - On the other hand, you have six observations of whether people walked.\n",
    "- To Prepare this data for the neural network, you have to first split it into two groups\n",
    "    - What you know\n",
    "    - What you want to know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices & the Matrix Relationship\n",
    "### translate the Streelight into math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:33%\" src=\"static/imgs/06/symbols-to-numbers.png\" />\n",
    "    <img style=\"width:20%\" src=\"static/imgs/06/output-pattern.png\" />\n",
    "</div>\n",
    "\n",
    "- You want to teach a neural network to translate a streetlight pattern into the correct stop/walk pattern.\n",
    "- What you really want to do is mimic the mattern of the streetlight in the form of numbers.\n",
    "- In data matrices, it's convention to give each recorded example a single row.\n",
    "- It's also convention to give each thing being recorded a single column.\n",
    "    - This makes the matrix easy to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Data Matrices perfectly mimic the outside world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data matrix doesn't have to be all 1s and 0s.\n",
    "- A Matrix should mimic the patterns that exist in the real world.\n",
    "    - So you can ask the computer to mimic them.\n",
    "- An infinite number of matrices exist that perfectly reflect the streetlight patterns in the dataset.\n",
    "- The underlying pattern isn't the same as the matrix.\n",
    "    - **It's a Property of the Matrix**.\n",
    "    - **the Pattern is what the matrix is expressing.**\n",
    "    - The Pattern also existed in the streetlights.\n",
    "- the Resulting Matrix is called a **Lossless Representation** \n",
    "    - because you can perfectly convert back and forth between your stop/walk notes and the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Matrix or Two in Python\n",
    "### Import the Matrices into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's Create the Streetlight pattern matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetlights = np.array([[1,0,1], [0,1,1], [0,0,1], [1,1,1], [0,1,1], [1,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is NumPy ?\n",
    "    - Numpy is really just a fancy wrapper for an array of arrays that provides special, matrix-oriented functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_vs_stop = np.array([[0],[1],[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 0.9104498947095488 Reality : 0 Error : 0.8289190107766286\n",
      "Prediction : 0.5462699368257292 Reality : 0 Error : 0.2984108438795862\n",
      "Prediction : 0.3277619620954375 Reality : 0 Error : 0.10742790379665101\n",
      "Prediction : 0.1966571772572625 Reality : 0 Error : 0.03867404536679436\n",
      "Prediction : 0.11799430635435748 Reality : 0 Error : 0.013922656332045966\n",
      "Prediction : 0.07079658381261444 Reality : 0 Error : 0.005012156279536542\n",
      "Prediction : 0.04247795028756862 Reality : 0 Error : 0.0018043762606331512\n",
      "Prediction : 0.025486770172541195 Reality : 0 Error : 0.0006495754538279355\n",
      "Prediction : 0.01529206210352474 Reality : 0 Error : 0.00023384716337805747\n",
      "Prediction : 0.009175237262114888 Reality : 0 Error : 8.418497881610151e-05\n",
      "Prediction : 0.005505142357268955 Reality : 0 Error : 3.030659237379679e-05\n",
      "Prediction : 0.0033030854143614174 Reality : 0 Error : 1.0910373254567137e-05\n",
      "Prediction : 0.0019818512486168283 Reality : 0 Error : 3.927734371644081e-06\n",
      "Prediction : 0.0011891107491700526 Reality : 0 Error : 1.4139843737917637e-06\n",
      "Prediction : 0.0007134664495019871 Reality : 0 Error : 5.090343745649715e-07\n",
      "Prediction : 0.00042807986970117007 Reality : 0 Error : 1.8325237484337073e-07\n",
      "Prediction : 0.00025684792182067984 Reality : 0 Error : 6.597085494360206e-08\n",
      "Prediction : 0.0001541087530924079 Reality : 0 Error : 2.3749507779696742e-08\n",
      "Prediction : 9.246525185546695e-05 Reality : 0 Error : 8.549822800694933e-09\n",
      "Prediction : 5.547915111325796e-05 Reality : 0 Error : 3.0779362082477123e-09\n"
     ]
    }
   ],
   "source": [
    "# I will attempt to build it.\n",
    "ws = np.random.rand(streetlights.shape[1])\n",
    "x_i = streetlights[0]\n",
    "y_i = walk_vs_stop[0]\n",
    "lr = .1\n",
    "\n",
    "for interation in range(20):\n",
    "    # predict.\n",
    "    prediction = x_i.dot(ws)\n",
    "    # MSE error.\n",
    "    error = (prediction - y_i) ** 2\n",
    "    # update weights.\n",
    "    for j in range(len(ws)):\n",
    "        gradient = 2 * x_i[j] * (prediction - y_i)\n",
    "        ws[j] -= lr * gradient\n",
    "    print(\"Prediction : \" + str(prediction) + \" Reality : \" + str(y_i[0]) + \" Error : \" + str(error[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[0.04] Prediction: -0.19999999999999996\n",
      "Error:[0.0256] Prediction: -0.15999999999999992\n",
      "Error:[0.016384] Prediction: -0.1279999999999999\n",
      "Error:[0.01048576] Prediction: -0.10239999999999982\n",
      "Error:[0.00671089] Prediction: -0.08191999999999977\n",
      "Error:[0.00429497] Prediction: -0.06553599999999982\n",
      "Error:[0.00274878] Prediction: -0.05242879999999994\n",
      "Error:[0.00175922] Prediction: -0.04194304000000004\n",
      "Error:[0.0011259] Prediction: -0.03355443200000008\n",
      "Error:[0.00072058] Prediction: -0.02684354560000002\n",
      "Error:[0.00046117] Prediction: -0.021474836479999926\n",
      "Error:[0.00029515] Prediction: -0.01717986918399994\n",
      "Error:[0.00018889] Prediction: -0.013743895347199997\n",
      "Error:[0.00012089] Prediction: -0.010995116277759953\n",
      "Error:[7.73712525e-05] Prediction: -0.008796093022207963\n",
      "Error:[4.95176016e-05] Prediction: -0.007036874417766459\n",
      "Error:[3.1691265e-05] Prediction: -0.0056294995342132115\n",
      "Error:[2.02824096e-05] Prediction: -0.004503599627370569\n",
      "Error:[1.29807421e-05] Prediction: -0.003602879701896544\n",
      "Error:[8.30767497e-06] Prediction: -0.002882303761517324\n"
     ]
    }
   ],
   "source": [
    "# Book Implementation.\n",
    "weights = np.array([.5, .48, -.7])\n",
    "alpha = .1\n",
    "\n",
    "input = streetlights[0]\n",
    "goal_prediction = walk_vs_stop[0]\n",
    "\n",
    "for iteration in range(20):\n",
    "    prediction = input.dot(weights)\n",
    "    error = (goal_prediction - prediction) ** 2\n",
    "    delta = prediction - goal_prediction\n",
    "    weights = weights - (alpha * (input * delta))\n",
    "    print(\"Error:\" + str(error) + \" Prediction: \" + str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Whole Dataset\n",
    "### The Neural Network has been learning only one streetlight. Don't we want it to learn them all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus far, you've trained neural networks that learned how to model a single training example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 1.115349567974397 Reality : 0 Error : 1.2440046587806741\n",
      "Prediction : 1.3518921380661582 Reality : 1 Error : 0.12382807683277211\n",
      "Prediction : 0.5656721671456411 Reality : 0 Error : 0.3199850006832461\n",
      "Prediction : 1.1311599954363323 Reality : 1 Error : 0.017202944402858703\n",
      "Prediction : 1.0455368512360337 Reality : 1 Error : 0.0020736048204926623\n",
      "Prediction : 0.42412551132053866 Reality : 0 Error : 0.17988244935290837\n",
      "Prediction : 0.2544753067923232 Reality : 0 Error : 0.06475768176704702\n",
      "Prediction : 0.8916019471190477 Reality : 1 Error : 0.011750137868381727\n",
      "Prediction : 0.30315781133565767 Reality : 0 Error : 0.09190465857382621\n",
      "Prediction : 0.7455365893202238 Reality : 1 Error : 0.06475162737478443\n",
      "Prediction : 0.9761149702762076 Reality : 1 Error : 0.0005704946449064446\n",
      "Prediction : 0.22029560260112177 Reality : 0 Error : 0.04853015252539137\n",
      "Prediction : 0.13217736156067306 Reality : 0 Error : 0.017470854909140892\n",
      "Prediction : 0.9151743893333657 Reality : 1 Error : 0.007195384224967422\n",
      "Prediction : 0.24466646645020773 Reality : 0 Error : 0.059861679805230626\n",
      "Prediction : 0.7517764129295008 Reality : 1 Error : 0.06161494917814569\n",
      "Prediction : 0.9994607751381775 Reality : 1 Error : 2.9076345160744196e-07\n",
      "Prediction : 0.14673552558025332 Reality : 0 Error : 0.021531314467313177\n",
      "Prediction : 0.08804131534815196 Reality : 0 Error : 0.007751273208232738\n",
      "Prediction : 0.9527210968972254 Reality : 1 Error : 0.00223529467860155\n",
      "Prediction : 0.20798614798150436 Reality : 0 Error : 0.04325823775218423\n",
      "Prediction : 0.784329850389976 Reality : 1 Error : 0.046513613432810116\n",
      "Prediction : 1.016303488386044 Reality : 1 Error : 0.00026580373355387184\n",
      "Prediction : 0.10369070239994599 Reality : 0 Error : 0.010751761764194165\n",
      "Prediction : 0.06221442143996758 Reality : 0 Error : 0.003870634235109898\n",
      "Prediction : 0.9766010682636437 Reality : 1 Error : 0.000547510006402663\n",
      "Prediction : 0.17776101220928803 Reality : 0 Error : 0.031598977461670646\n",
      "Prediction : 0.8146558655182924 Reality : 1 Error : 0.03435244818677332\n",
      "Prediction : 1.0245460923090115 Reality : 1 Error : 0.0006025106476425136\n",
      "Prediction : 0.07568467210027496 Reality : 0 Error : 0.00572816959092614\n",
      "Prediction : 0.045410803260164986 Reality : 0 Error : 0.002062141052733411\n",
      "Prediction : 0.990508560313319 Reality : 1 Error : 9.008742732590274e-05\n",
      "Prediction : 0.15204761106721787 Reality : 0 Error : 0.023118476031247955\n",
      "Prediction : 0.8409927728007651 Reality : 1 Error : 0.0252832983015891\n",
      "Prediction : 1.0274985048542418 Reality : 1 Error : 0.0007561677692187581\n",
      "Prediction : 0.05683843758883725 Reality : 0 Error : 0.0032306079875401472\n",
      "Prediction : 0.03410306255330234 Reality : 0 Error : 0.0011630188755144522\n",
      "Prediction : 0.9983108028841172 Reality : 1 Error : 2.8533868963066987e-06\n",
      "Prediction : 0.13008937271752158 Reality : 0 Error : 0.016923244894038247\n",
      "Prediction : 0.8636789114246024 Reality : 1 Error : 0.0185834391903814\n",
      "Prediction : 1.027497042617125 Reality : 1 Error : 0.0007560873526879909\n",
      "Prediction : 0.04381082931838769 Reality : 0 Error : 0.001919388765564898\n"
     ]
    }
   ],
   "source": [
    "# let's generalize the algorithm ourselves.\n",
    "# I will attempt to build it.\n",
    "ws = np.random.rand(streetlights.shape[1])\n",
    "lr = .1\n",
    "epoches = 7\n",
    "\n",
    "for interation in range(epoches):\n",
    "    for i in range(len(streetlights)):\n",
    "        # predict.\n",
    "        prediction = streetlights[i].dot(ws)\n",
    "        # MSE error.\n",
    "        error = (prediction - walk_vs_stop[i]) ** 2\n",
    "        # update weights.\n",
    "        for j in range(len(ws)):\n",
    "            gradient = 2 * streetlights[i][j] * (prediction - walk_vs_stop[i])\n",
    "            ws[j] -= lr * gradient\n",
    "        print(\"Prediction : \" + str(prediction) + \" Reality : \" + str(walk_vs_stop[i][0]) + \" Error : \" + str(error[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.026286497591032618,\n",
       "  1.0077360597065974,\n",
       "  0.11707414150199424,\n",
       "  0.9169484157956358,\n",
       "  1.0077360597065974,\n",
       "  0.026286497591032618],\n",
       " array([[1, 0, 1],\n",
       "        [0, 1, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 1, 1],\n",
       "        [1, 0, 1]]),\n",
       " array([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our final weights predictions. Compared with ground truths\n",
    "[ws.dot(streetlight) for streetlight in streetlights], streetlights, walk_vs_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[2.65612311]\n",
      "Error:[0.96287018]\n",
      "Error:[0.55091659]\n",
      "Error:[0.36445837]\n",
      "Error:[0.25167687]\n",
      "Error:[0.17797575]\n",
      "Error:[0.12864461]\n",
      "Error:[0.09511037]\n",
      "Error:[0.07194564]\n",
      "Error:[0.05564915]\n",
      "Error:[0.04394764]\n",
      "Error:[0.03535797]\n",
      "Error:[0.028907]\n",
      "Error:[0.02395166]\n",
      "Error:[0.02006311]\n",
      "Error:[0.01695209]\n",
      "Error:[0.01442082]\n",
      "Error:[0.01233174]\n",
      "Error:[0.01058739]\n",
      "Error:[0.00911723]\n"
     ]
    }
   ],
   "source": [
    "# Book Implementation.\n",
    "weights = np.array([.5, .48, -.7])\n",
    "alpha = .1\n",
    "\n",
    "for iteration in range(20):\n",
    "    error_for_all_lights = 0\n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row_index]\n",
    "        goal_prediction = walk_vs_stop[row_index]\n",
    "        \n",
    "        prediction = input.dot(weights)\n",
    "        \n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_for_all_lights += error\n",
    "        \n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - (alpha * (input * delta))\n",
    "    print(\"Error:\" + str(error_for_all_lights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full, Batch, and Stochastic Gradient Descent\n",
    "### Stochastic Gradient Descent updates weights one example at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This Idea of learning one example at a time is called **Stochastic Gradient Descent**.\n",
    "    - It performs a prediction and weight update for each training example separately.\n",
    "    - It iterates through the entire dataset many times until it can find a weight configuration that works well for the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Full) Gradient Descent updates weights one dataset at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of updating the weights once for each training example, the weight calculates the loss over the entire dataset.\n",
    "    - Changing the weights only each time it computes a full average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent updates weights taking in n examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of updating the weights using one example or after the entire dataset of examples, you choose a batch size (typically between 8 and 256).\n",
    "    - After which the weights are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Learn Correlation\n",
    "### What did the last neural network learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correlation is located wherever the weights were set to high numbers.\n",
    "- Inversely, randomness with respect to the input is found wherever the weights converge to 0.\n",
    "- How did the network Identify Correlation?\n",
    "    - In the Process of Gradient Descent, each training example asserts either *up pressure* or *down pressure* on the weights.\n",
    "    - On average, there was more *up pressure* for the middle weight and more *down pressure* for the other two.\n",
    "    - Where does the pressure come from?\n",
    "    - Why is it different for different weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up and Down Pressue\n",
    "### It comes from the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each node is individually trying to correctly predict the output given the input.\n",
    "- For the most part, each node ignores all other nodes when attempting to do so.\n",
    "- **The Only cross communication that occurs is that all 3 weights must share the same error measure**.\n",
    "- **The weight update is nothing more than taking this shared error measure and multiplying it by each respective error**.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/weight_update.png\" />\n",
    "</div>\n",
    "\n",
    "- **A Key part of why neural networks learn is error attribution, which means given a shared error, the network needs to figure out which weights contributed (so they can be adjusted) and which weights did not contribute (so they can be left alone)**.\n",
    "- On Average, this causes the network to find the correlation present between the middle weight and the output to be the dominant predictive force.\n",
    "    - & Ofcoures making the network quite accurate.\n",
    "- Bottom Line\n",
    "    - The prediction is a weighted sum of the inputs.\n",
    "    - The Learning algorithm rewards inputs that correlate with the output with upward pressure (toward 1)\n",
    "    - & Penalize inputs that discorrale with the output with downward pressure (toward 0)\n",
    "    - The weighted sum of the inputs find perfect correlation between the input and the output by weighting decorrelated inputs to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case: Overfitting\n",
    "### Sometimes Correlation happens accidentally "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error is shared among all of the weigths\n",
    "- If a particular configuration of weights accidentally creates perfect correlation between the prediction and the output dataset\n",
    "    - **The Neural Network will stop Learning**\n",
    "- In essence, **It memorized** the two training examples instead of finding the correlation that will generalize to any possible streetlight configuration.\n",
    "- The greatest challenge you'll face with deep learning is convincing your neural network to **generalize** instead of just **memorize**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case: Conflicting Pressure\n",
    "### Sometimes correlation fights itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As other nodes learn, they absorb some of the error; they absorb part of the correlation.\n",
    "- This causes the network to predict with moderate correlative power, which reduces the error.\n",
    "- the other weights then only try to adjust their weights to correctly predict what's left.\n",
    "- **Regularization** forces weights with conflicting pressure to move toward 0.\n",
    "- Regularization aims to say that only weights with really strong correlation can stay on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/latent_correlation.png\" />\n",
    "</div>\n",
    "\n",
    "- in the case of one input and out output layers, each weight learn for itself and finds correlation between the associated column and the output.\n",
    "- How about when the correlation is indirect, when a linear combination of the inputs is correlated with the output and not distinct columns.\n",
    "    - To Solve this, We use the **Multi-Layer Perceptron** Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Indirect Correlation\n",
    "### If your Data doesn't have correlation, create intermediate data that does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks search for correlation between their input and output **layers**.\n",
    "- Because the Input dataset doesn't correlate with the output dataset, you'll use the input dataset to create an intermediate dataset that does have correlation with the output.\n",
    "    - It's kind of like **cheating**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Correlation\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/hidden-layer.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the middle layer represents the intermediate dataset.\n",
    "- **this network is still just a function**\n",
    "    - It has a bunch of weights that are collected together in a particular way.\n",
    "    - Gradient Descent still works because you can calculate how much each weight contributes to the error and adjust it to reduce the error to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Neural Networks: A Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you look at the stacked neural network architecture and ignore the lower weights & only consider their output to be the dataset.\n",
    "    - the top half of the neural network is just like the networks trained in the preceding chapter.\n",
    "    - You can use all the same learning logic to help them learn.\n",
    "- The part that you don't yet understand is how to update the weights of the first layer.\n",
    "    - What do they use as their error measure?\n",
    "- The cached/normalized error measure is called *delta*.\n",
    "    - You want to figure out how to know the *delta* values at the first layer so they can help the second layer make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: Long-distance Error Attribution\n",
    "### The Weighted average error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/backpropagation.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do you use the *delta* at layer 2 to figure out the *delta* at layer 1?\n",
    "    - You multiply it by each of the respective weights for layer 1.\n",
    "    - It's like the prediction logic in reverse.\n",
    "- this process of moving the *delta* signal around is called **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: Why does this work?\n",
    "### The weigthed average delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The delta variable told you the **direction and amount** the value of this node should change next time.\n",
    "- All backpropagation lets you do is say:\n",
    "    - Hey, If you want this node to be x amount higher, then each of these previous four nodes needs to be x*weights_1_2 amount higher/lower.\n",
    "    - Because these weights were amplifying the prediction by weights_1_2 times.\n",
    "- Once you know this, you can update each weight matrix as you did before.\n",
    "    - For each weight, multiply its output delta by its input value.\n",
    "    - & adjust the weight by that much (or you can scale it by alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear vs. Nonlinear\n",
    "### This is probably the **hardest** Concept in the Book, Let's Take it slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As it turns out, you need **one more piece** to make this neural network train.\n",
    "- Let's take it from two perspectives:\n",
    "    - The first will show why the neural network can't train without it.\n",
    "    - Second will show how to fix the problem.\n",
    "- The Problem lies in the following statement:\n",
    "    - All linear mappings of linear mappings produce linear mappings.\n",
    "        - Meaning, no matter how many stacked layers you add, there exist an equivalent NN w/ one layer.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:66%\" src=\"static/imgs/06/linearity-problem.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the Neural Network still doesn't work\n",
    "### If you trained the three layer network as it is now, it wouldn't converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The middle nodes don't get to add anything to the conversation.\n",
    "    - they don't get to have correlation of their own.\n",
    "    - they're more or less correlated to various input nodes.\n",
    "- But because you know that in teh new dataset there is no correlation between any of the inputs/outputs, how can the middle layer help?\n",
    "    - It mixes up a bunch of correlation that's already useless.\n",
    "    - **What you really need is for the middle layer to be able to selectively correlate with the input**.\n",
    "- **You want the middle layer to sometimes correlate with an input, and sometimes not correlate**.\n",
    "    - That gives it correlation of its own.\n",
    "- This gives the middle layer the opportunity to not just always be x% correlated with one input and y% correlated with another input.\n",
    "    - Instead, it can be x% correlated with one input only when it wants to be.\n",
    "- this is called **Conditional Correlation** or **sometimes correlation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Secret to Sometimes Correlation\n",
    "### Turn off the node when the value is below 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If a Node's value dropped below 0, normally the node would still have the same correlation to the input as always.\n",
    "    - it would just happen to be negative in value.\n",
    "    - But if you turn off the node when it would be negative, then it has zero correlation to any inputs whenever It's negative.\n",
    "    - What does this mean ?\n",
    "- **The Node can now pick & choose when it wants to be correlated to something.**\n",
    "    - This allows it to say something like:\n",
    "        - Make me perfectly correlated to the left input, but only when the right input is turned off.\n",
    "- This wasn't possible before, Now the node can be conditional.\n",
    "    - Now **It can speak of itself**\n",
    "- the fancy term for this \"if the node would be negative, set it to 0\" logic is **nonlinearity**.\n",
    "    - Without this tweak, the neural network is linear.\n",
    "- There are many kinds of nonlinearities, but the one discussed here is, in many cases, the best one to use.\n",
    "    - It's also the simplest. (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Deep Neural Network\n",
    "### Here's how to make the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/06/introducing-relu.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return (x > 0) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .1\n",
    "hidden_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 1], \n",
    "              [0, 1, 1], \n",
    "              [0, 0, 1], \n",
    "              [1, 1, 1]])\n",
    "y = np.array([[1, 1, 0, 0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 4), (4, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights to connect the 3 layers.\n",
    "ws_0_1 = 2*np.random.random((3, hidden_size)) - 1\n",
    "ws_1_2 = 2*np.random.random((hidden_size, 1)) - 1\n",
    "ws_0_1.shape, ws_1_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0 = X[0]\n",
    "layer_1 = ReLU(np.dot(layer_0, ws_0_1))\n",
    "layer_2 = np.dot(layer_1, ws_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagating in Code\n",
    "### You can learn the amount that each weight contributes to the final error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_grad(x):\n",
    "    return (x > 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: [0.]\n",
      "error: [0.07710452]\n",
      "error: [0.03764561]\n",
      "error: [0.00240581]\n",
      "error: [9.22274338e-06]\n",
      "error: [0.]\n",
      "error: [0.]\n",
      "error: [0.]\n",
      "error: [0.]\n",
      "error: [0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.5910955 ],\n",
       "       [ 1.13962134],\n",
       "       [-0.94522481],\n",
       "       [ 1.11202675]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for i in range(len(X)):\n",
    "        # get input X[i] & target y[i]\n",
    "        x_i, y_i = X[i], y[i]\n",
    "        \n",
    "        # calculate prediction\n",
    "        hs = ReLU(np.dot(x_i, ws_0_1))\n",
    "        prediction = np.dot(hs, ws_1_2)\n",
    "        \n",
    "        # calculate error, pure error.\n",
    "        error = (prediction - y_i) ** 2\n",
    "        delta = prediction - y_i\n",
    "        \n",
    "        # calculate gradients of 1st layer.\n",
    "        grad_0_1 = np.zeros(ws_0_1.shape)\n",
    "        for line_i in range(len(ws_0_1)):\n",
    "            for col_i in range(len(ws_0_1[0])):\n",
    "                grad_0_1[line_i][col_i] = 2 * delta * x_i[line_i] * ws_1_2[col_i] * ReLU_grad(hs[col_i])\n",
    "        \n",
    "        # update weights of 1st layer.\n",
    "        ws_0_1 -= lr * grad_0_1\n",
    "        \n",
    "        # calculate gradients of 2nd layer.\n",
    "        grad_1_2 = np.zeros(ws_1_2.shape)\n",
    "        for line_i in range(len(ws_1_2)):\n",
    "            grad_1_2[line_i]= 2 * delta * hs[line_i]\n",
    "        \n",
    "        # update weights of 2nd layer.\n",
    "        ws_1_2 -= lr * grad_1_2\n",
    "    if (epoch % 10 == 0):\n",
    "        print('error: ' + str(error))\n",
    "ws_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() (1, 1) (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Book Implementation.\n",
    "for iteration in range(100):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(X)):\n",
    "        layer_0 = X[i:i+1]\n",
    "        layer_1 = ReLU(np.dot(layer_0, ws_0_1))\n",
    "        layer_2 = np.dot(layer_1, ws_1_2)\n",
    "        \n",
    "        layer_2_error += np.sum((layer_2 - y[i:i+1]) ** 2)\n",
    "        layer_2_delta = (layer_2 - y[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(ws_1_2.T)*ReLU_grad(layer_1)\n",
    "        \n",
    "        ws_1_2 -= lr * layer_1.T.dot(layer_2_delta)\n",
    "        ws_0_1 -= lr * layer_0.T.dot(layer_1_delta)\n",
    "    if (iteration % 10 == 0):\n",
    "        print(\"Error : \" + str(layer_2_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember, the goal is **error attribution**.\n",
    "    - It's about figuring out how much each weight contributed to the overall error.\n",
    "- Now that you know how much the final prediction should move up or down, you need to figure out how much each middle node should move up/down.\n",
    "    - These are effectively **intermediate predictions**\n",
    "- Once you have the delta at layer 1, you can use the same processes as before for calculating a weight update.\n",
    "- Backpropagation is abount calculating *deltas* for intermediate layers so you can perform gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do Deep Networks Matter?\n",
    "### What's the point of creating \"intermediate datasets\" that have correlation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The two layer network might have a problem classifying cat vs. non-cats pectures, why ?\n",
    "- Just like the last streetlight dataset, no individual pixel correlates with whether there's a cat in the picture.\n",
    "    - only different configuration of pixels correlate with whether there's a cat.\n",
    "- **Deep Learning is all about creating intermediate layers (datasets) wherein each node in an intermediate layer represents the presence or absence of a different configuration of inputs**.\n",
    "- Because intermediate layers detect (presence or not) various pixel configurations, it then gives the final layer the information it needs to correctly predict the presence/absence of cat.\n",
    "- Some Neural Networks have hundreds of layers.\n",
    "- **The Rest of this book will be dedicated to studying different phenomena within these layers in an effort to explore the full power of deep neural networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Build a 3-layer Neural Network from Memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (4, 1))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 0, 1], \n",
    "              [0, 1, 1], \n",
    "              [0, 0, 1], \n",
    "              [1, 1, 1]])\n",
    "y = np.array([[1, 1, 0, 0]]).T\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 4), (4, 1))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init weights.\n",
    "ws_1 = np.random.rand(X.shape[1], 4)\n",
    "ws_2 = np.random.rand(4, y.shape[1])\n",
    "\n",
    "ws_1.shape, ws_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def grad_relu(x):\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error :  [[2.05239252]]\n",
      "Error :  [[0.00109065]]\n",
      "Error :  [[1.64704463e-09]]\n",
      "Error :  [[1.55952904e-15]]\n",
      "Error :  [[1.26140843e-21]]\n",
      "Error :  [[1.09092376e-27]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.79483996e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.79483996e-31]]\n",
      "Error :  [[1.79483996e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.79483996e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.73333695e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.10933565e-31]]\n",
      "Error :  [[1.50992908e-31]]\n",
      "Error :  [[1.30192864e-31]]\n",
      "Error :  [[1.50992908e-31]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        # get input/output\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # calculate prediction\n",
    "        layer_1 = relu(layer_in.dot(ws_1))\n",
    "        layer_out = layer_1.dot(ws_2).reshape(1, 1)\n",
    "        \n",
    "        # calculate delta 2\n",
    "        delta_2 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # calc error for logs\n",
    "        error = delta_2 ** 2\n",
    "        \n",
    "        # calculate delta 1\n",
    "        # delta_2.dot(ws_2.T) -> (1, 4)\n",
    "        # grad_relu(hs) -> (4,)\n",
    "        # * : element wise multiplication.\n",
    "        delta_1 = delta_2.dot(ws_2.T)*grad_relu(layer_1)\n",
    "        \n",
    "        # update weights\n",
    "        ws_2 -= lr * (layer_1.T.reshape(4,1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(3,1).dot(delta_1))\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Error : \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  [1]  y_hat:  0.9999999999999999\n",
      "y:  [1]  y_hat:  0.9999999999999997\n",
      "y:  [0]  y_hat:  2.7755575615628914e-16\n",
      "y:  [0]  y_hat:  3.608224830031759e-16\n"
     ]
    }
   ],
   "source": [
    "# test weights.\n",
    "for i in range(len(X)):\n",
    "    x_i, y_i = X[i], y[i]\n",
    "    print('y: ', y_i, ' y_hat: ', relu(x_i.dot(ws_1)).dot(ws_2).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:66%\" src=\"static/imgs/06/backprop.jpg\" /><br/>\n",
    "    <span style=\"color:gray;\">* Error Note: last layer doesn't have an activation function</span><br/>\n",
    "    <img style=\"width:66%\" src=\"static/imgs/06/more-explanation.jpg\" />\n",
    "    <img style=\"width:66%\" src=\"static/imgs/06/thought_understand.jpg\" />\n",
    "    <img style=\"width:66%\" src=\"static/imgs/06/debug_mode.jpg\" />\n",
    "    <img style=\"width:66%\" src=\"static/imgs/06/falsy.jpg\" />\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
