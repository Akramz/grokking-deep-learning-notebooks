{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reg\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Signal & Ignoring Noise: Introduction to Regularization & Batching\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter:\n",
    "- Overfitting\n",
    "- Dropout\n",
    "- Batch Gradient Descent\n",
    "\n",
    "> \"With four Parameters I can fit an Elephant, & with five I can make him wiggle his trunk.\" â€” John von Neumann, Mathematician, Physicist, Computer Scientist, & Polymath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Layer Network on MNIST\n",
    "### Let's return to the MNIST Dataset & Attempt to Classify it with the New Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do you know the network is creating good correlation?\n",
    "- If we froze one weight, train the network until the error was sufficiently small, than unfroze the weight in an attempt to optimize against it, it won't change because the network had already learnt the existing correlation in the data.\n",
    "- **What if the network had figured out a way to accurately predict the games in the training dataset, but it somehow forgot to include a valuable input?**\n",
    "- **Overfitting is extremely common in Neural Networks**.\n",
    "- The More Powerful the Neural Networks expressive power (more layers & weights), the more prone the network is to overfit.\n",
    "- We're going to study the basics of **Regularization**, which is key to combatting overfitting in neural networks.\n",
    "- We're going to train our latest & greatest neural network with 3 layers on the MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = X_train[:1000].reshape(1000, 28*28)/255., y_train[:1000]; X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding y\n",
    "y_ = np.zeros((y.shape[0], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate(y):\n",
    "    y_[index][value] = 1\n",
    "y = y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for test\n",
    "X_test, y_test = X_test[:1000].reshape(1000, 28*28)/255., y_test[:1000]\n",
    "one_hot_y_test = np.zeros((y_test.shape[0], 10))\n",
    "for index, value in enumerate(y_test):\n",
    "    one_hot_y_test[index][value] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    bottom = np.sum([np.exp(v_i) for v_i in v])\n",
    "    return np.array([np.exp(v_i)/bottom for v_i in v])\n",
    "relu = lambda x: (x >= 0) * x\n",
    "grad_relu = lambda x: (x >= 0)\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x)) \n",
    "lr, epochs, hidden_size, total_pixels, num_labels = 0.005, 300, 100, 784, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing two weight matrices\n",
    "W_1 = 0.2*np.random.random((total_pixels,hidden_size)) - 0.1\n",
    "W_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:0 Error:0.802 Correct:0.501\n",
      " I:1 Error:0.451 Correct:0.766\n",
      " I:2 Error:0.270 Correct:0.837\n",
      " I:3 Error:0.203 Correct:0.88\n",
      " I:4 Error:0.168 Correct:0.891\n",
      " I:5 Error:0.145 Correct:0.912\n",
      " I:6 Error:0.128 Correct:0.921\n",
      " I:7 Error:0.115 Correct:0.929\n",
      " I:8 Error:0.104 Correct:0.936\n",
      " I:9 Error:0.095 Correct:0.945\n",
      " I:10 Error:0.086 Correct:0.952\n",
      " I:11 Error:0.079 Correct:0.96\n",
      " I:12 Error:0.072 Correct:0.962\n",
      " I:13 Error:0.065 Correct:0.967\n",
      " I:14 Error:0.059 Correct:0.972\n",
      " I:15 Error:0.053 Correct:0.974\n",
      " I:16 Error:0.048 Correct:0.977\n",
      " I:17 Error:0.043 Correct:0.984\n",
      " I:18 Error:0.038 Correct:0.986\n",
      " I:19 Error:0.034 Correct:0.99\n",
      " I:20 Error:0.030 Correct:0.992\n",
      " I:21 Error:0.026 Correct:0.993\n",
      " I:22 Error:0.023 Correct:0.996\n",
      " I:23 Error:0.020 Correct:0.996\n",
      " I:24 Error:0.017 Correct:0.996\n",
      " I:25 Error:0.014 Correct:0.997\n",
      " I:26 Error:0.012 Correct:1.0\n",
      " I:27 Error:0.010 Correct:1.0\n",
      " I:28 Error:0.009 Correct:1.0\n",
      " I:29 Error:0.008 Correct:1.0\n",
      " I:30 Error:0.007 Correct:1.0\n",
      " I:31 Error:0.006 Correct:1.0\n",
      " I:32 Error:0.005 Correct:1.0\n",
      " I:33 Error:0.005 Correct:1.0\n",
      " I:34 Error:0.004 Correct:1.0\n",
      " I:35 Error:0.004 Correct:1.0\n",
      " I:36 Error:0.003 Correct:1.0\n",
      " I:37 Error:0.003 Correct:1.0\n",
      " I:38 Error:0.002 Correct:1.0\n",
      " I:39 Error:0.002 Correct:1.0\n",
      " I:40 Error:0.002 Correct:1.0\n",
      " I:41 Error:0.002 Correct:1.0\n",
      " I:42 Error:0.002 Correct:1.0\n",
      " I:43 Error:0.001 Correct:1.0\n",
      " I:44 Error:0.001 Correct:1.0\n",
      " I:45 Error:0.001 Correct:1.0\n",
      " I:46 Error:0.001 Correct:1.0\n",
      " I:47 Error:0.001 Correct:1.0\n",
      " I:48 Error:0.001 Correct:1.0\n",
      " I:49 Error:0.001 Correct:1.0\n",
      " I:50 Error:0.001 Correct:1.0\n",
      " I:51 Error:0.001 Correct:1.0\n",
      " I:52 Error:0.000 Correct:1.0\n",
      " I:53 Error:0.000 Correct:1.0\n",
      " I:54 Error:0.000 Correct:1.0\n",
      " I:55 Error:0.000 Correct:1.0\n",
      " I:56 Error:0.000 Correct:1.0\n",
      " I:57 Error:0.000 Correct:1.0\n",
      " I:58 Error:0.000 Correct:1.0\n",
      " I:59 Error:0.000 Correct:1.0\n",
      " I:60 Error:0.000 Correct:1.0\n",
      " I:61 Error:0.000 Correct:1.0\n",
      " I:62 Error:0.000 Correct:1.0\n",
      " I:63 Error:0.000 Correct:1.0\n",
      " I:64 Error:0.000 Correct:1.0\n",
      " I:65 Error:0.000 Correct:1.0\n",
      " I:66 Error:0.000 Correct:1.0\n",
      " I:67 Error:0.000 Correct:1.0\n",
      " I:68 Error:0.000 Correct:1.0\n",
      " I:69 Error:0.000 Correct:1.0\n",
      " I:70 Error:0.000 Correct:1.0\n",
      " I:71 Error:0.000 Correct:1.0\n",
      " I:72 Error:0.000 Correct:1.0\n",
      " I:73 Error:0.000 Correct:1.0\n",
      " I:74 Error:0.000 Correct:1.0\n",
      " I:75 Error:0.000 Correct:1.0\n",
      " I:76 Error:0.000 Correct:1.0\n",
      " I:77 Error:0.000 Correct:1.0\n",
      " I:78 Error:0.000 Correct:1.0\n",
      " I:79 Error:0.000 Correct:1.0\n",
      " I:80 Error:0.000 Correct:1.0\n",
      " I:81 Error:0.000 Correct:1.0\n",
      " I:82 Error:0.000 Correct:1.0\n",
      " I:83 Error:0.000 Correct:1.0\n",
      " I:84 Error:0.000 Correct:1.0\n",
      " I:85 Error:0.000 Correct:1.0\n",
      " I:86 Error:0.000 Correct:1.0\n",
      " I:87 Error:0.000 Correct:1.0\n",
      " I:88 Error:0.000 Correct:1.0\n",
      " I:89 Error:0.000 Correct:1.0\n",
      " I:90 Error:0.000 Correct:1.0\n",
      " I:91 Error:0.000 Correct:1.0\n",
      " I:92 Error:0.000 Correct:1.0\n",
      " I:93 Error:0.000 Correct:1.0\n",
      " I:94 Error:0.000 Correct:1.0\n",
      " I:95 Error:0.000 Correct:1.0\n",
      " I:96 Error:0.000 Correct:1.0\n",
      " I:97 Error:0.000 Correct:1.0\n",
      " I:98 Error:0.000 Correct:1.0\n",
      " I:99 Error:0.000 Correct:1.0\n"
     ]
    }
   ],
   "source": [
    "for j in range(epochs):\n",
    "    error, correct_count = (0.0, 0)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        layer_0 = X[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(W_1))\n",
    "        layer_2 = softmax(layer_1.dot(W_2))\n",
    "        \n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(one_hot_y[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = (layer_2 - one_hot_y[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(W_2.T)*grad_relu(layer_1)\n",
    "        \n",
    "        error += np.sum(layer_2_delta ** 2)\n",
    "        \n",
    "        W_2 -= lr * layer_1.T.dot(layer_2_delta) \n",
    "        W_1 -= lr * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "    print(\"\\r\"+\" I:\"+str(j)+\" Error:\" + str(error/float(len(X)))[0:5] + \" Correct:\" + str(correct_count/float(len(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well, That was easy!\n",
    "### The Neural Network learned to predict all 1,000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've reached 100% accuracy on the sample of 1000 images.\n",
    "- But how well will it do on an image that wasn't part of the original sample of 1,000 images?\n",
    "- Let's Evaluate the network on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.292 Correct:0.823\n"
     ]
    }
   ],
   "source": [
    "error, correct_count = (0.0, 0)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    layer_0 = X_test[i:i+1]\n",
    "    layer_1 = relu(layer_0.dot(W_1))\n",
    "    layer_2 = softmax(layer_1.dot(W_2))\n",
    "    \n",
    "    correct_count += int(np.argmax(layer_2) == np.argmax(one_hot_y_test[i:i+1]))\n",
    "    layer_2_delta = (layer_2 - one_hot_y_test[i:i+1])\n",
    "    error += np.sum(layer_2_delta ** 2)\n",
    "\n",
    "print(\"Error:\" + str(error/float(len(X_test)))[0:5] + \" Correct:\" + str(correct_count/float(len(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book's Results is:\n",
    "    - Error: .653\n",
    "    - Correct: .7073\n",
    "- I think that my results are better due to the use of the softmax function on the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The Network did horribly!**\n",
    "- Why does it do so terribly on these new testing images when it learned to predict with a 100% accuracy on the training set?\n",
    "- This **0.823** is called the **test accuracy**.\n",
    "- This number is important because it simulates how the neural network will do in production (the real world).\n",
    "    - **This is the score that matters, the test accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorization vs. Generalizatio\n",
    "### Memorizing 1,000 is easier than generalizing to all images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's rewind on how a neural network actually learns:\n",
    "    - It Adjusts each number on each weight matrix so the overall error is minimized.\n",
    "- **If we trained the NN to predict labels on 1,000 images, which it did perfectly, why does it work on other images at all**?\n",
    "- The NN is guaranteed to work well on a new image only if the new image is nearly identical images in the training data set.\n",
    "    - Because the NN learned to transform the input data to output data for a specific data set with a specific overall configuration\n",
    "- If The NN works only on nearly identical data points (in comparison to training set), then what's the purpose of it anyway.\n",
    "    - **We want a Neural Network that can work well on images different from the training set data points**\n",
    "    - That's what we call: **Generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting in Neural Networks\n",
    "### Neural Networks can get worse if you train them too much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some reason, the test accuracy went up for the first 20 iterations & then slowly decreased as the network trained more and more.\n",
    "- This is common in neural networks.\n",
    "- Overfitting is over-optimizing for the training data points,\n",
    "    - **Just like when you're molding a material for 3 forks you keep molding them until you get a very specific shape that works for all 3 but has nothing to do with the shape of a general-purpose fork.\n",
    "- **You can Visualize Weights as High Dimensional Shapes**.\n",
    "- As you train, this shapes molds around the shape of the data, learning one pattern after another.\n",
    "- A more official definition of a neural network that overfits:\n",
    "    - A Neural Network that has **learned the noise** in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where Overfitting Comes from?\n",
    "### What causes neural networks to overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider these two dog pictures:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%\" src=\"static/imgs/08/Dogs.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Everything** that makes these pictures **unique** **beyond** what captures **the essense of \"Dog\" is** included in the term **\"Noise\"**.\n",
    "- On the Left: The Pillow & the background are both Noise.\n",
    "- On the Right: the Blackness can also be considered Noise.\n",
    "    - It's really the edges that tells you it's a Dog.\n",
    "- How do you get NNs to train only on the Signal (the essense of a dog) & Ignore the Noise?\n",
    "    - One Way is **Early Stopping**.\n",
    "        - It turns out a large amount of noise comes in the fine grained details of an image.\n",
    "        - & most of the signal is found on the general shape & perhaps color of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simplest Regularization: Early Stopping\n",
    "### Stop training the network what it starts getting worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You don't let the network train long enough to learn the details.**\n",
    "- **Early Stopping** is the cheapest form of regularization, and you're in a pinch, it can be quite effective.\n",
    "- **Regularization** is a subset of methods for getting a model to generalize to new data points.\n",
    "    - Instead of just memorizing the training data.\n",
    "- It's a subset of methods that helps the neural network learn the signal and ignore the noise.\n",
    "    - Often done by **increasing the difficulty** for a model to learn the fine-grained details in teh training data.\n",
    "- You know how to stop by using the **validation set** while training, and stop when the validation score gets worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry Standard Regularization: Dropout\n",
    "### The Method: Randomly turn off neurons (setting them to 0) during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This causes the neural network to train execlusively using **random subsections of the neural network**.\n",
    "- This Regularization method is generally accepted as the go-to, state-of-the-art regularization technique for the vast majority of networks.\n",
    "- Its methodology is simple & inexpensive, although the intuitions behind why it works are a bit more complex.\n",
    "- **Why Dropout Works?**\n",
    "    - **Dropout makes a big network train like a little one by randomly training little subsections of the network at a time.\n",
    "        - **and little networks don't overfit**.\n",
    "- The Smaller a Neural Network is, the less it's able to overfit.\n",
    "    - **Because small neural network have a smaller number of weights, meaning the network's hypothesis space is small**\n",
    "    - **Because small neural networks don't have much expressive power**.\n",
    "- Small Neural Networks have enough room to only capture the big, obvious, high-level features.\n",
    "- The Notion of room/capacity is very important to keep in your mind.\n",
    "- Remember the Clay analogy?\n",
    "    - **Imagine if the clay was made of sticky rocks the size of dimes**.\n",
    "        - Those stones are much like **weights**.\n",
    "    - **Now Imagine a clay made of millions & millions of small stones.**\n",
    "        - This is a big Model.\n",
    "- How do you get the power of a large neural network with the resistance to overfitting of the small neural network?\n",
    "    - **Dropout**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Dropout works? Ensembling works?\n",
    "### Dropout is a form of training a bunch of networks and averaging them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although it's likely that large, unregularized neural networks will overfit to noise, It's unlikely they will overfit to the same noise.\n",
    "    - Because they start randomly.\n",
    "- Neural networks, even though they're randomly generated, still start by learning the biggest, most broadly sweeping features before learning much about the noise\n",
    "- If you allowed a bunch of overfitted neural networks to vote equally, their noise will tend to cancel out, revealing only what they all learned in common ..\n",
    "    - **The Signal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout in Code\n",
    "### Here's how to use dropout in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.0 Test-Acc:0.6 Train-Err:1.764 Train-Acc:0.386\n",
      "\n",
      "I:10 Test-Err:0.0 Test-Acc:0.745 Train-Err:1.286 Train-Acc:0.765\n",
      "\n",
      "I:20 Test-Err:0.0 Test-Acc:0.776 Train-Err:1.228 Train-Acc:0.81\n",
      "\n",
      "I:30 Test-Err:0.0 Test-Acc:0.794 Train-Err:1.208 Train-Acc:0.822\n",
      "\n",
      "I:40 Test-Err:0.0 Test-Acc:0.804 Train-Err:1.174 Train-Acc:0.843\n",
      "\n",
      "I:50 Test-Err:0.0 Test-Acc:0.825 Train-Err:1.162 Train-Acc:0.854\n",
      "\n",
      "I:60 Test-Err:0.0 Test-Acc:0.814 Train-Err:1.145 Train-Acc:0.866\n",
      "\n",
      "I:70 Test-Err:0.0 Test-Acc:0.814 Train-Err:1.144 Train-Acc:0.882\n",
      "\n",
      "I:80 Test-Err:0.0 Test-Acc:0.807 Train-Err:1.134 Train-Acc:0.864\n",
      "\n",
      "I:90 Test-Err:0.0 Test-Acc:0.823 Train-Err:1.133 Train-Acc:0.869\n",
      "\n",
      "I:100 Test-Err:0.0 Test-Acc:0.814 Train-Err:1.133 Train-Acc:0.866\n",
      "\n",
      "I:110 Test-Err:0.0 Test-Acc:0.809 Train-Err:1.125 Train-Acc:0.893\n",
      "\n",
      "I:120 Test-Err:0.0 Test-Acc:0.817 Train-Err:1.127 Train-Acc:0.879\n",
      "\n",
      "I:130 Test-Err:0.0 Test-Acc:0.801 Train-Err:1.125 Train-Acc:0.888\n",
      "\n",
      "I:140 Test-Err:0.0 Test-Acc:0.796 Train-Err:1.105 Train-Acc:0.896\n",
      "\n",
      "I:150 Test-Err:0.0 Test-Acc:0.803 Train-Err:1.096 Train-Acc:0.892\n",
      "\n",
      "I:160 Test-Err:0.0 Test-Acc:0.802 Train-Err:1.101 Train-Acc:0.902\n",
      "\n",
      "I:170 Test-Err:0.0 Test-Acc:0.798 Train-Err:1.125 Train-Acc:0.892\n",
      "\n",
      "I:180 Test-Err:0.0 Test-Acc:0.786 Train-Err:1.108 Train-Acc:0.904\n",
      "\n",
      "I:190 Test-Err:0.0 Test-Acc:0.79 Train-Err:1.107 Train-Acc:0.892\n",
      "\n",
      "I:200 Test-Err:0.0 Test-Acc:0.792 Train-Err:1.106 Train-Acc:0.896\n",
      "\n",
      "I:210 Test-Err:0.0 Test-Acc:0.791 Train-Err:1.092 Train-Acc:0.907\n",
      "\n",
      "I:220 Test-Err:0.0 Test-Acc:0.792 Train-Err:1.091 Train-Acc:0.89\n",
      "\n",
      "I:230 Test-Err:0.0 Test-Acc:0.774 Train-Err:1.082 Train-Acc:0.908\n",
      "\n",
      "I:240 Test-Err:0.0 Test-Acc:0.789 Train-Err:1.084 Train-Acc:0.902\n",
      "\n",
      "I:250 Test-Err:0.0 Test-Acc:0.8 Train-Err:1.077 Train-Acc:0.899\n",
      "\n",
      "I:260 Test-Err:0.0 Test-Acc:0.801 Train-Err:1.063 Train-Acc:0.916\n",
      "\n",
      "I:270 Test-Err:0.0 Test-Acc:0.795 Train-Err:1.079 Train-Acc:0.91\n",
      "\n",
      "I:280 Test-Err:0.0 Test-Acc:0.796 Train-Err:1.071 Train-Acc:0.916\n",
      "\n",
      "I:290 Test-Err:0.0 Test-Acc:0.793 Train-Err:1.067 Train-Acc:0.922\n"
     ]
    }
   ],
   "source": [
    "for j in range(epochs):\n",
    "    error, correct_count = (0.0, 0)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        layer_0 = X[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(W_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = layer_1.dot(W_2)\n",
    "    \n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(one_hot_y[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = (layer_2 - one_hot_y[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(W_2.T)*grad_relu(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        error += np.sum(layer_2_delta ** 2)\n",
    "        \n",
    "        W_2 -= lr * layer_1.T.dot(layer_2_delta) \n",
    "        W_1 -= lr * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    if j % 10 == 0:\n",
    "        test_error, test_correct_count = 0, 0\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            layer_0 = X_test[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(W_1))\n",
    "            layer_2 = softmax(layer_1.dot(W_2))\n",
    "\n",
    "            test_correct_count += int(np.argmax(layer_2) == np.argmax(one_hot_y_test[i:i+1]))\n",
    "            layer_2_delta = (layer_2 - one_hot_y_test[i:i+1])\n",
    "            error += np.sum(layer_2_delta ** 2)\n",
    "        \n",
    "    \n",
    "        print(\"\\n\" + \"I:\" + str(j) + \" Test-Err:\" + str(test_error/ float(len(X_test)))[0:5] + \" Test-Acc:\" + str(test_correct_count/ float(len(X_test)))+ \" Train-Err:\" + str(error/ float(len(X)))[0:5] + \" Train-Acc:\" + str(correct_count/ float(len(X))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dropout mask uses what's called a 50% bernoulli distribution.\n",
    "- You multiply `layer_1` by 2, why do you do this ?\n",
    "    - Remember that `layer_2` will perform a weighted sum of `layer_1`\n",
    "        - Even though it's weighted, It's still a sum over the values of `layer_1`.\n",
    "    - If you turn off half of the nodes in `layer_1`, `layer_2` would increase its sensitivity to `layer_1`.\n",
    "    - But @ test time, you no longer would need dropout, this would throw off `layer_2` sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After introducing Dropout, Not only the network peak at a score of 80%, it also doesn't overfit nearly as badly.\n",
    "- Notice also that the dropout slows down training accuracy, it previously converged to 100% pretty fastly, now, it finishes at 90%.\n",
    "- **Dropout is Noise**, we are introducing noise to the network to help it concentrate its training on the true signal and avoid memorizing data-point-specific noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "### Here's a Method for increasing the Speed of training & the rate of convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also called: mini-batched stochastic gradient descent.\n",
    "- It's something that's largely taken for granted in neural network training.\n",
    "- It's a simple concept that doesn't get more advanced even with the most state-of-the-art neural networks.\n",
    "- Previously, we trained one training example each iteration.\n",
    "- Now, let's train 100 training examples at a time, **averaging the weight updates among all 100 examples**.\n",
    "- As it turns out, **individual training examples are very noisy** in terms of the weight updates they generate.\n",
    "    - Thus, averaging them makes for a smoother learning process.\n",
    "- Let's do this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "lr, epochs = .001, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **prev & for every epoch, we predict a data point @ a time**\n",
    "- **now & for every epoch, we predict a batch @ a time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do the book implementation from scratch while debugging.\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the necessary data.\n",
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (X_train[:1000].reshape((1000, -1))/255.), y_train[:1000]\n",
    "X_test = (X_test.reshape((X_test.shape[0], -1))/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotting y\n",
    "one_hots = np.zeros((y.shape[0], 10))\n",
    "for i, _ in enumerate(one_hots):\n",
    "    one_hots[i][y[i]] = 1\n",
    "y = one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotting y\n",
    "one_hots = np.zeros((y_test.shape[0], 10))\n",
    "for i, _ in enumerate(one_hots):\n",
    "    one_hots[i][y_test[i]] = 1\n",
    "y_test = one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x >= 0) * x\n",
    "\n",
    "def grad_relu(x):\n",
    "    return x >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "lr, epochs = .001, 300\n",
    "pixels_per_image, num_labels, hidden_size = 784, 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights\n",
    "W_1 = .2 * np.random.random((pixels_count, hidden_size)) - .1\n",
    "W_2 = .2 * np.random.random((hidden_size, num_labels)) - .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.815 Test-Acc:0.3832 Train-Err:1.284 Train-Acc:0.165\n",
      "I:10 Test-Err:0.568 Test-Acc:0.7173 Train-Err:0.591 Train-Acc:0.672\n",
      "I:20 Test-Err:0.510 Test-Acc:0.7571 Train-Err:0.532 Train-Acc:0.729\n",
      "I:30 Test-Err:0.485 Test-Acc:0.7793 Train-Err:0.498 Train-Acc:0.754\n",
      "I:40 Test-Err:0.468 Test-Acc:0.7877 Train-Err:0.489 Train-Acc:0.749\n",
      "I:50 Test-Err:0.458 Test-Acc:0.793 Train-Err:0.468 Train-Acc:0.775\n",
      "I:60 Test-Err:0.452 Test-Acc:0.7995 Train-Err:0.452 Train-Acc:0.799\n",
      "I:70 Test-Err:0.446 Test-Acc:0.803 Train-Err:0.453 Train-Acc:0.792\n",
      "I:80 Test-Err:0.451 Test-Acc:0.7968 Train-Err:0.457 Train-Acc:0.786\n",
      "I:90 Test-Err:0.447 Test-Acc:0.795 Train-Err:0.454 Train-Acc:0.799\n",
      "I:100 Test-Err:0.448 Test-Acc:0.793 Train-Err:0.447 Train-Acc:0.796\n",
      "I:110 Test-Err:0.441 Test-Acc:0.7943 Train-Err:0.426 Train-Acc:0.816\n",
      "I:120 Test-Err:0.442 Test-Acc:0.7966 Train-Err:0.431 Train-Acc:0.813\n",
      "I:130 Test-Err:0.441 Test-Acc:0.7906 Train-Err:0.434 Train-Acc:0.816\n",
      "I:140 Test-Err:0.447 Test-Acc:0.7874 Train-Err:0.437 Train-Acc:0.822\n",
      "I:150 Test-Err:0.443 Test-Acc:0.7899 Train-Err:0.414 Train-Acc:0.823\n",
      "I:160 Test-Err:0.438 Test-Acc:0.797 Train-Err:0.427 Train-Acc:0.811\n",
      "I:170 Test-Err:0.440 Test-Acc:0.7884 Train-Err:0.418 Train-Acc:0.828\n",
      "I:180 Test-Err:0.436 Test-Acc:0.7935 Train-Err:0.407 Train-Acc:0.834\n",
      "I:190 Test-Err:0.434 Test-Acc:0.7935 Train-Err:0.410 Train-Acc:0.831\n",
      "I:200 Test-Err:0.435 Test-Acc:0.7972 Train-Err:0.416 Train-Acc:0.829\n",
      "I:210 Test-Err:0.434 Test-Acc:0.7923 Train-Err:0.409 Train-Acc:0.83\n",
      "I:220 Test-Err:0.433 Test-Acc:0.8032 Train-Err:0.396 Train-Acc:0.832\n",
      "I:230 Test-Err:0.431 Test-Acc:0.8036 Train-Err:0.393 Train-Acc:0.853\n",
      "I:240 Test-Err:0.430 Test-Acc:0.8047 Train-Err:0.397 Train-Acc:0.844\n",
      "I:250 Test-Err:0.429 Test-Acc:0.8028 Train-Err:0.386 Train-Acc:0.843\n",
      "I:260 Test-Err:0.431 Test-Acc:0.8038 Train-Err:0.394 Train-Acc:0.843\n",
      "I:270 Test-Err:0.428 Test-Acc:0.8014 Train-Err:0.384 Train-Acc:0.845\n",
      "I:280 Test-Err:0.430 Test-Acc:0.8067 Train-Err:0.401 Train-Acc:0.846\n",
      "I:290 Test-Err:0.428 Test-Acc:0.7975 Train-Err:0.383 Train-Acc:0.851"
     ]
    }
   ],
   "source": [
    "for j in range(epochs):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    for i in range(int(len(X) / batch_size)):\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))\n",
    "\n",
    "        layer_0 = X[batch_start:batch_end]\n",
    "        layer_1 = relu(np.dot(layer_0,W_1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = np.dot(layer_1,W_2)\n",
    "\n",
    "        error += np.sum((y[batch_start:batch_end] - layer_2) ** 2)\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(y[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "            layer_2_delta = (y[batch_start:batch_end]-layer_2)/batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(W_2.T)* grad_relu(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "\n",
    "            W_2 += lr * layer_1.T.dot(layer_2_delta)\n",
    "            W_1 += lr * layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "    if(j%10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            layer_0 = X_test[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0, W_1))\n",
    "            layer_2 = np.dot(layer_1, W_2)\n",
    "\n",
    "            test_error += np.sum((y_test[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(y_test[i:i+1]))\n",
    "\n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(X_test)))[0:5] +\\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(X_test)))+\\\n",
    "                         \" Train-Err:\" + str(error/ float(len(X)))[0:5] +\\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Concept behind this is the fact that we don't alter the network's parameters each data point prediction at a time.\n",
    "    - we instead predict a whole batch, get an average delta and update all parameters.\n",
    "        - This means we get rid of point level noise, by averging a batch of prediction, we have a better sense of what direction we should move the internal parameters.\n",
    "- Notice that the learning rate is 20 times larger than before.\n",
    "    - Because we are much more confident in the direction the weights should take to change.\n",
    "- Because the example take an average of a noisy signal (the average weight change over 100 training examples), it can take bigger steps.\n",
    "- You'll generally see **batching ranging from 8 to as high as 256**.\n",
    "- Generally, Researchers take numbers randomly until they find a `batch_size` & `lr` pair that seems to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Following Chapters, we'll pivot from sets of tools that are universally applicable to nearly all neural networks, to special purpose architectures that are advantageous for modeling specific types of phenomena in data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
