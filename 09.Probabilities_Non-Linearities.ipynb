{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nonlin\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Probabilities & Non-Linearities: Activation Functions\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter:\n",
    "\n",
    "- What is an Activation Function?\n",
    "- Standard Hidden Activation Functions\n",
    "    - Sigmoid\n",
    "    - Tanh\n",
    "- Standard output activation functions\n",
    "    - Softmax\n",
    "- Activation Function Installation Instructions\n",
    "\n",
    "> \"I Know that 2 and 2 make 4 –– & should be glad to prove it too if I could –– though I must say if by any sort of process I could convert 2 & 2 into 5 it would give me much greater pleasure\" –– George Gordon Byron, Letter to Anabella Milbanke. November 10, 1813."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Activation Function?\n",
    "### It's a function applied to the neurons in a layer during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You've been using an activation function called `ReLU`.\n",
    "- the ReLU function had the effect of turning all negative numbers to zero.\n",
    "- An activation function is any function that can take one number and return another number.\n",
    "    - **but there are an infinite number of functions in the universe, & not all of them are useful as activation functions**.\n",
    "- There are many constraints on the nature of activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint 1: The Function must be Continuous & Infinite in Domain\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:66%;\" src=\"static/imgs/09/continuous-vs-non.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It must have an output number for any input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint 2: Good Activation Functions are Monotonic (Increasing/Decreasing)\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:66%;\" src=\"static/imgs/09/monotonic.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The activation function must **never change direction**\n",
    "    - It must be always increasing or always decreasing.\n",
    "- This particular constraint isn't technically a requirement.\n",
    "    - **But consider the implication of having multiple input values map to the same output value**.\n",
    "        - Multiple input values \"mean\" the same thing.\n",
    "- **If there are multiple ways to get the correct answer, then the network has multiple possible perfect configurations.**\n",
    "    - You can't know the correct direction to go, because multiple direction will give a positive signal while the optimal minima is in one location.\n",
    "- **For an advanced look into this subject, look more into convex versus non-convex optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint 3: Good Activation Functions are Non-Linear\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:66%;\" src=\"static/imgs/09/non-linearity.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Functions Scale Values, they don't effect how correlated a neuron is to various inputs.\n",
    "- It makes the collective correlation that's represented louder or softer.\n",
    "    - But the linear activation function **doesn't allow one weight to effect how correlated the neuron is to other weights.**\n",
    "- What you really want is **selective correlation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint 4: Good Activation Functions (& their derivatives) should be efficiently computable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you'll be calling this function and its gradient a lot\n",
    "- `relu` has become very popular mostly because it's (and its derivative) are effecient to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Hidden-Layer Activation Functions\n",
    "### Of the infinite possible functions, which ones are most commonly used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid is the bread & butter Activation\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:33%;\" src=\"static/imgs/09/sigmoid.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sigmoid is great because it smoothly squinshes the infinite amount of input to an output between 0 and 1.\n",
    "- This lets you interpret the output of any neuron as a **probability**.\n",
    "- People use this non-linearity both in hidden and outputs layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh is better than sigmoid for hidden layers\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:33%;\" src=\"static/imgs/09/tanh.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tanh is the same as sigmoid except it's between -1 and 1.\n",
    "- This means it can also throw in some **negative** correlation.\n",
    "- **this aspect of negative correlation is powerful for hidden layers.**\n",
    "- **on many problems, tanh will outperform sigmoid in hidden layers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard output layer activation functions\n",
    "### Choosing the best one depends on what you're trying to predict\n",
    "\n",
    "- Broadly speaking, there are 3 major types of output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 1: Predicting Raw Data Values (Regression) — No activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One example might be predicting the average temperature in colorado given the average temperature in surrounding states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 2: Predicting Unrelated Yes/No Probabilities (Binary Classification) — Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's best to use the sigmoid function, Because it models individual probabilities separately for each output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3: Predicting which-one probabilities (Categorical Classification) — Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By far the common use case in neural networks is predicting a single label out of many. \n",
    "- It's better to have an activation function that models the idea that \"The more likely it's one label, The less likely it's any of the other labels\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Issue: Inputs have similarity\n",
    "### Different numbers share characteristics. It's good to let the network believe that\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img style=\"width:33%;\" src=\"static/imgs/09/similarity.png\">\n",
    "    <img style=\"width:22%;\" src=\"static/imgs/09/weights.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average 2 shares quite a bit with the average 3.\n",
    "- As a general rule, similar inputs create similar outputs.\n",
    "    - When you take some numbers and multiply them by a matrix, if the starting numbers are pretty similar, the ending numbers will be pretty similar.\n",
    "- Sigmoid will penelize the network for recognizing a 2 by anything other than features that are exclusively related to 2s.\n",
    "    - It penelizes the network for recognizing a 2 based on, say, the top curve.\n",
    "- Most Images share lots of pixels in the middle of images.\n",
    "    - **So the network will start trying to focus on the edges**.\n",
    "- As you can see on the Weight Image, The light areas are probably the best individual indicators of a 2.\n",
    "    - **But the best overall is a network that sees the entire shape for what it is.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Computation\n",
    "### Softmax raises each input value exponentially and then divides by the layer's sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The nice thing about softmax is that the higher the network predicts one value, the lower it predicts all the others.\n",
    "- **It encourages the network to predict one output with very high probability.**\n",
    "    - To adjust how aggresively it does this:\n",
    "        - use numbers slightly higher or lower than $e$.\n",
    "            - Lower numbers will result in lower *attenuation* & higher numbers will result in bigger *attenuation*.\n",
    "    - but most people will stick to $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Installation Instructions\n",
    "### How do you add your favorite activation function to any layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Input to a layer refers to the value before the nonlinearity.\n",
    "- The Slope of ReLU for positive numbers is exactly 1.\n",
    "- The Slope of ReLU for negative numbers is exactly 0.\n",
    "- Modifying the input to this function (by a tiny amount) will have a 1:1 effect if it was predicting positively.\n",
    "    - & a 0:1 effect if it was predicting negatively.\n",
    "- This slope is a measure of how much the output of relu will change given a change in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus, when you backpropagate, in order to generate `layer_1_delta`, multiply the backpropagated `delta` from `layer_2` (`layer_2_delta.dot(W_2.T)`) by the slope of ReLU at the point predicted in forward propagation.\n",
    "- For some deltas the slope is 1 (positive numbers) & for others it's 0 (negative numbers).\n",
    "- **The important thing to remember is that the slop is an indicator of how much a tiny change to the input effects the output.**\n",
    "- **The update effect encourages the network to leave weights alone if adjusting them will have little to no effect**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying Delta by the Slope\n",
    "### To compute layer_delta, multiply the backpropagated delta by the layer's slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img style=\"width:33%;\" src=\"static/imgs/09/layer_delta.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `layer_1_delta[0]` represents how much higher or lower the first hidden node of layer 1 should be in order to reduce the error of the network.\n",
    "- But the end goal of delta on a neuron is to inform the weights whether they should move.\n",
    "- **If moving them would have no effect, they should be left alone.**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img style=\"width:66%;\" src=\"static/imgs/09/activations.png\">\n",
    "</div>\n",
    "\n",
    "-  This is obvious for ReLU, which is either on or off.\n",
    "    - **sigmoid** is, perhaps, more nuanced.\n",
    "- Sigmoid's sensitivity to change in the input slowly increases as the input approaches 0 from either direction.\n",
    "- But **very positive** and **very negative** inputs approach a slope of very near 0.\n",
    "- Thus, as the input becomes very positive/negative, small changes to the incoming weights become less relevant to the neuron's error at this training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Output to Slope (Derivative)\n",
    "### Most great activations can convert their output to their slope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img style=\"width:75%;\" src=\"static/imgs/09/Pariston.png\">\n",
    "</div>\n",
    "\n",
    "- **Most Great Activations have a means by whitch the output of the layer (at forward propagation) can be used to compute the derivative**.\n",
    "- This has become the standard practice for computing derivatives in neural networks, and it's quite handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrading the MNIST Network\n",
    "### Let's Upgrade the MNIST Network to reflect what you've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theoretically, the `tanh` function should make for a better hidden-layer activation\n",
    "- and `softmax` should make for a better output layer activation function.\n",
    "    - But things aren't always as simple as they seem.\n",
    "- For `Tanh` i had to reduce the standard diviation for the incoming weights.\n",
    "    - I adjusted weigth values to be between -.01 & +.01.\n",
    "- I Had to revisit the **Alpha** tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (1000,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = (X_train[:1000].reshape(1000, 28*28))/255, y_train[:1000]\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels = np.zeros((labels.shape[0], 10))\n",
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = X_test.reshape(X_test.shape[0], 28*28)/255\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_test_labels = np.zeros((y_test.shape[0], 10))\n",
    "one_hot_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, l in enumerate(y_test):\n",
    "    one_hot_test_labels[i][l] = 1\n",
    "test_labels = one_hot_test_labels\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions.\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, iterations, hidden_size = (2, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 100), (100, 10))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1 = 0.02*np.random.random((pixels_per_image,hidden_size)) - 0.01\n",
    "W_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "W_1.shape, W_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.3806 Train-Acc:0.154\n",
      "\n",
      "I:10 Test-Acc:0.677 Train-Acc:0.699\n",
      "\n",
      "I:20 Test-Acc:0.7042 Train-Acc:0.734\n",
      "\n",
      "I:30 Test-Acc:0.7408 Train-Acc:0.77\n",
      "\n",
      "I:40 Test-Acc:0.7724 Train-Acc:0.796\n",
      "\n",
      "I:50 Test-Acc:0.7932 Train-Acc:0.816\n",
      "\n",
      "I:60 Test-Acc:0.8101 Train-Acc:0.841\n",
      "\n",
      "I:70 Test-Acc:0.8232 Train-Acc:0.849\n",
      "\n",
      "I:80 Test-Acc:0.8295 Train-Acc:0.876\n",
      "\n",
      "I:90 Test-Acc:0.8347 Train-Acc:0.879\n",
      "\n",
      "I:100 Test-Acc:0.8402 Train-Acc:0.883\n",
      "\n",
      "I:110 Test-Acc:0.8429 Train-Acc:0.897\n",
      "\n",
      "I:120 Test-Acc:0.8467 Train-Acc:0.896\n",
      "\n",
      "I:130 Test-Acc:0.8496 Train-Acc:0.91\n",
      "\n",
      "I:140 Test-Acc:0.8525 Train-Acc:0.904\n",
      "\n",
      "I:150 Test-Acc:0.8537 Train-Acc:0.911\n",
      "\n",
      "I:160 Test-Acc:0.8568 Train-Acc:0.921\n",
      "\n",
      "I:170 Test-Acc:0.8588 Train-Acc:0.911\n",
      "\n",
      "I:180 Test-Acc:0.8603 Train-Acc:0.924\n",
      "\n",
      "I:190 Test-Acc:0.8607 Train-Acc:0.926\n",
      "\n",
      "I:200 Test-Acc:0.8633 Train-Acc:0.929\n",
      "\n",
      "I:210 Test-Acc:0.8634 Train-Acc:0.928\n",
      "\n",
      "I:220 Test-Acc:0.865 Train-Acc:0.937\n",
      "\n",
      "I:230 Test-Acc:0.8668 Train-Acc:0.935\n",
      "\n",
      "I:240 Test-Acc:0.8666 Train-Acc:0.929\n",
      "\n",
      "I:250 Test-Acc:0.8684 Train-Acc:0.935\n",
      "\n",
      "I:260 Test-Acc:0.8682 Train-Acc:0.948\n",
      "\n",
      "I:270 Test-Acc:0.8707 Train-Acc:0.941\n",
      "\n",
      "I:280 Test-Acc:0.869 Train-Acc:0.943\n",
      "\n",
      "I:290 Test-Acc:0.8704 Train-Acc:0.944\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for j in range(iterations):  # epoches\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):  # batches\n",
    "        batch_start, batch_end = (i * batch_size), ((i+1) * batch_size)\n",
    "        \n",
    "        # Forward Propagation\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = tanh(np.dot(layer_0, W_1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(np.dot(layer_1, W_2))\n",
    "        \n",
    "        # benchmarking\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "        \n",
    "        # backpropagation\n",
    "        layer_2_delta = (labels[batch_start:batch_end] - layer_2) / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(W_2.T)*tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        # optimization\n",
    "        W_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        W_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    test_correct_cnt = 0\n",
    "    \n",
    "    for i in range(len(test_images)):  # test images\n",
    "        # predict\n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_1 = tanh(np.dot(layer_0,W_1)) \n",
    "        layer_2 = np.dot(layer_1,W_2) \n",
    "        \n",
    "        # benchmark\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "    \n",
    "    if(j % 10 == 0):\n",
    "        print(\"\\n\"+ \"I:\" + str(j) + \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\" Train-Acc:\" + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to make sure that we understand batch stochastic gradient descent + the new activation function by **Understanding the Implementation** of both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import keras\n",
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data.\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light data pre-processing\n",
    "x_train, y_train = (X_train[:1000].reshape((1000, 28*28))/255.), (y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hotting `y_train`\n",
    "labels_train = np.zeros((y_train.shape[0], 10))\n",
    "for i, v in enumerate(y_train):\n",
    "    labels_train[i][v] = 1\n",
    "labels_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same to testing data.\n",
    "x_test = X_test.reshape((-1, 28*28))/255.\n",
    "labels_test = np.zeros((y_test.shape[0], 10))\n",
    "for i, v in enumerate(y_test):\n",
    "    labels_test[i][v] = 1\n",
    "labels_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 10), (10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, labels_train.shape, x_test.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions.\n",
    "def ReLU(x):\n",
    "    return (x > 0) * x\n",
    "def grad_ReLU(x):\n",
    "    return (x > 0).astype('int')\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def tanh2deriv(x):\n",
    "    return 1 - (x ** 2)\n",
    "def softmax(x):\n",
    "    \"\"\" Softmax works when we provide it with lines and rows, each line represents a sample ..\n",
    "     ... & each row is a feature.\n",
    "    \"\"\"\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "lr, epoches, hidden_size = 2, 100, 100\n",
    "pixels_count, labels_count = 784, 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Wights Initialization\n",
    "W_0 = 0.02*np.random.random((784,100)) - 0.01\n",
    "W_1 = 0.02*np.random.random((100,10)) - 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 100), (100, 10))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_0.shape, W_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:0 Test-Acc:0.098 Train-Acc:0.088\n",
      "\n",
      "Epoch:10 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:20 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:30 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:40 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:50 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:60 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:70 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:80 Test-Acc:0.098 Train-Acc:0.097\n",
      "\n",
      "Epoch:90 Test-Acc:0.098 Train-Acc:0.097\n"
     ]
    }
   ],
   "source": [
    "# now to the training loop\n",
    "# Add -> Dropout!\n",
    "for epoch in range(epoches):\n",
    "    \n",
    "    # cuz each epoch passes through all training data, we calc error each epoch\n",
    "    correct_count = []\n",
    "    \n",
    "    for batch_i in range(int(x_train.shape[0]/batch_size)):\n",
    "        # get batch\n",
    "        batch_start, batch_end = (batch_i * batch_size), ((batch_i+1) * batch_size)\n",
    "        X = x_train[batch_start:batch_end]\n",
    "        y = labels_train[batch_start:batch_end]\n",
    "        \n",
    "        # forward propagation\n",
    "        layer_0 = X\n",
    "        layer_1 = ReLU(np.matmul(layer_0, W_0))\n",
    "        layer_2 = softmax(np.matmul(layer_1, W_1))\n",
    "        \n",
    "        # Benchmarking, loop over the batch\n",
    "        for k in range(batch_size):\n",
    "            # we want to loop over the batch images.\n",
    "            x_i, y_i, y_i_hat = X[k:k+1], y[k:k+1], layer_2[k:k+1]\n",
    "            if np.argmax(y_i_hat.squeeze()) == np.argmax(y_i.squeeze()):\n",
    "                correct_count.append(1)\n",
    "            else:\n",
    "                correct_count.append(0)\n",
    "        \n",
    "        # backpropagation\n",
    "        # TODO: dividing by 10,000 solved the problem .. KNOW WHY!\n",
    "        layer_2_delta = (layer_2 - y) # / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(W_1.T)*grad_ReLU(layer_1)\n",
    "        \n",
    "        # Optimization\n",
    "        W_1 -= lr * (layer_1.T.dot(layer_2_delta))\n",
    "        W_0 -= lr * (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "    test_correct_count = []\n",
    "    \n",
    "    # evaluate over test dataset.\n",
    "    for i in range(x_test.shape[0]):\n",
    "        # get data\n",
    "        x_i, y_i = x_test[i:i+1], labels_test[i:i+1]\n",
    "        \n",
    "        # forward propagation\n",
    "        layer_0 = x_i\n",
    "        layer_1 = ReLU(layer_0.dot(W_0))\n",
    "        layer_2 = softmax(layer_1.dot(W_1))\n",
    "        \n",
    "        if np.argmax(layer_2.squeeze()) == np.argmax(y_i.squeeze()):\n",
    "            test_correct_count.append(1)\n",
    "        else:\n",
    "            test_correct_count.append(0)\n",
    "        \n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"\\n\"+ \"Epoch:\" + str(epoch) + \\\n",
    "              \" Test-Acc:\"+str(np.sum(np.array(test_correct_count))/np.array(test_correct_count).shape[0])+ \\\n",
    "              \" Train-Acc:\" + str(np.sum(np.array(correct_count))/np.array(correct_count).shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
