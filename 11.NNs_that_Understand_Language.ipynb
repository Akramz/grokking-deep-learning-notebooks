{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks that understand Language: King - Man + Woman == ?\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter:\n",
    "- Natural Language Processing\n",
    "- Supervised NLP\n",
    "- Capturing Word Correlation in Input Data\n",
    "- Intro to an Embedding Layer\n",
    "- Neural Architecture\n",
    "- Comparing Word Embeddings\n",
    "- Filling in the Blank\n",
    "- Meaning is derived from Loss\n",
    "- Word Analogies\n",
    "\n",
    "> \"Man is a Slow, Sloppy, and Brilliant Thinker; Computers are Fast, Accurate, and Stupid!\" — John Pfeiffer, Fortune, 1961"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it mean to understand language?\n",
    "### What kinds of predictions do people make about language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different datasets often justify different styles of neural network training according to the challenges hidden in the data.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:33%;\" src=\"static/imgs/11/domains_intersections.png\"/></div>\n",
    "\n",
    "- **NLP: Natural Language Processing is a much older field that overlaps deep learning**\n",
    "- This field is dedicated exlusively to the automated task of understanding human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "### NLP is divided into a collection of tasks and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are a few types of classification problems that are common in NLP:\n",
    "    - Using the Characters of a document to predict where words start and end.\n",
    "    - Using the Words of a document to predict where sentences start and end.\n",
    "    - Using the Words in a sentence to predict the part of speech of each word.\n",
    "    - Using the Words of a sentence to predict where phrases start and end.\n",
    "    - Using words in a sentence to predict where named entities (person, place, thing) references start and end.\n",
    "    - Using sentences in a document to predict which pronouns refer to the same person/place/thing.\n",
    "    - Using words in a sentence to predict the sentiment of a sentence.\n",
    "- NLP tasks seek to do one of three things:\n",
    "    - **label a region of text**.\n",
    "    - **Link two or more regions of Text**.\n",
    "    - **Try to fill in missing information based on Context**.\n",
    "- Until recently, most of the SoTA NLP Algorithms where **advanced, probabilistic, non-parametric** models (but not Deep Learning).\n",
    "- The recent development and popularization of two major neural algorithms have swept the field of NLP:\n",
    "    - **Neural Word Embeddings**.\n",
    "    - **Recurrent Neural Networks**.\n",
    "- NLP plays a very special role in **AGI** (Artificial General Intelligence), because **language is the bedrock of consious logic and communication in humans**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised NLP\n",
    "### Words go in, & predictions come out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Up until now, we represented inputs as numbers.\n",
    "    - **But NLP uses text as input. How do you process it?**\n",
    "- Because NNs only map input numbers to output numbers, we need to convert our words into their corresponding numerical representation.\n",
    "    - As it turns out, How we do this is exteremly important!\n",
    "    \n",
    "<div style=\"text-align:center;\"><img style=\"width:75%;\" src=\"static/imgs/11/input_text.png\"/></div>\n",
    "\n",
    "- In order to find the optimal numerical representation for text, we need to look at the underlying input-to-output problem, let's take an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Reviews Dataset\n",
    "### You can predict whether people post positive/negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The IMDB Reviews Dataset is a collection of Review/Rating Pairs that often looks like the following:\n",
    "\n",
    "> \"This Movie was terrible, The Plot was Dry, The acting unconvincing, and I spilled popcorn on my shirt!\" — Rating: 1 Stars.\n",
    "\n",
    "- The entire dataset consists of around 50K reviews\n",
    "    - The **Input Reviews are usually a few sentences** & the **Output rating is between 1 and 5 stars**.\n",
    "    - It should be obvious that this sentiment dataset might be very different from other sentiment datasets, such as product reviews or hospital patient reviews.\n",
    "- Data Processing:\n",
    "    - You'll adjust the range of stars from 1 to 5 into 0 to 1.\n",
    "        - So you can use Binary Softmax (Sigmoid).\n",
    "    - The input data is a list of characters, this presents a few problems:\n",
    "        - The input data is text instead of numbers.\n",
    "        - **Input is Variable-Length Text.**\n",
    "- \"What about the Input Text will have Correlation with the Output?\"\n",
    "    - Representing that property might work well.\n",
    "    - I wouldn't expect any characters (in a list of characters) to have correlation with the output (rating).\n",
    "    - But several words would have a bit of correlation with the output rating\n",
    "        - \"Terrible\", \"Unconvincing\" are such word examples.\n",
    "    - These words have significant **negative** correlation with the rating.\n",
    "        - By **Negative**, I mean as the frequency of these words increases, ratings tend to decrease in number of stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Word Correlation in Input Data\n",
    "### Bag of words: Given a review's Vocabulary, predict the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/imdb_master.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/imdb_master.csv\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls $IMDB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>A funny thing happened to me while watching \"M...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10004_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>This German horror film has to be one of the w...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10005_2.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review label         file\n",
       "0  test  Once again Mr. Costner has dragged out a movie...   neg      0_2.txt\n",
       "1  test  This is an example of why the majority of acti...   neg  10000_4.txt\n",
       "2  test  First of all I hate those moronic rappers, who...   neg  10001_1.txt\n",
       "3  test  Not even the Beatles could write songs everyon...   neg  10002_3.txt\n",
       "4  test  Brass pictures (movies is not a fitting word f...   neg  10003_3.txt\n",
       "5  test  A funny thing happened to me while watching \"M...   neg  10004_2.txt\n",
       "6  test  This German horror film has to be one of the w...   neg  10005_2.txt"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(IMDB_PATH, encoding=\"ISO-8859-1\", index_col=0)  # added encoding to fix error\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n",
       " 'neg')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at one review:\n",
    "df.loc[0].review, df.loc[0].label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's commonly done in this case is to create a matrix where each row represents a review.\n",
    "- **Each column represents whether a review contains a particular word in the vocabulary.**\n",
    "- To create a vector for a review, you just loop over the content and put $1$s in places where the corresponding vocabulary words are present in the review.\n",
    "- How big are these vectors?\n",
    "    - Well, it depends on the global vocabulary of the reviews.\n",
    "    - If you have 2,000 unique words, you need vectors of length 2,000.\n",
    "- This form of storage, called **one-hot encoding**, is the most common way to store binary information, in our case, the presence/absence of particular vocabulary words from the text of a review.\n",
    "- If our vocabulary have only 4 words, than the one-hot encoding might actually look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = {}\n",
    "one_hots['cat'] = np.array([1, 0, 0, 0])\n",
    "one_hots['the'] = np.array([0, 1, 0, 0])\n",
    "one_hots['dog'] = np.array([0, 0, 1, 0])\n",
    "one_hots['sat'] = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:75%;\" src=\"static/imgs/11/one-hots.png\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent Encoding:[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "sentence = ['the', 'cat', 'sat']\n",
    "x = one_hots[sentence[0]] + one_hots[sentence[1]] + one_hots[sentence[2]]\n",
    "print('Sent Encoding:' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We create a vector for each term in the vocabulary.\n",
    "- This allows you to use vector addition to represent a set of words present in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Movie Reviews\n",
    "### With the Previous Strategy and the previous network, you can predict sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You build a vector for each word and use the two-layer network to predict sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the strategy we just identified, you can build a vector for each word in the sentiment dataset and use the previous two-layer network to predict the sentiment.\n",
    "- I Strongly recommend attempting this from memory.\n",
    "- Open a new Jupyter Notebook, load in the dataset, build you one-hot vectors, and then build a neural network to predict the rating of each movie review (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "IMDB_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/imdb_master.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(IMDB_PATH, encoding=\"ISO-8859-1\", index_col=0)\n",
    "df = df[df['label'].isin(['neg', 'pos'])]\n",
    "all_reviews_text = \" \".join(df.review.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11557297, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get unique tokens\n",
    "all_tokens = all_reviews_text.split(\" \")\n",
    "unique_tokens = [v for (v, _) in Counter(all_tokens).most_common(10000)]\n",
    "len(all_tokens), len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function out of it\n",
    "def get_tokens(text):\n",
    "    return list(set(text.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot representations of each token\n",
    "word_to_index, index_to_word = {}, {}\n",
    "for i, word in enumerate(unique_tokens):\n",
    "    word_to_index[word], index_to_word[i] = i, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_count'] = df['review'].apply(lambda x: len(x.split(\" \"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>231.145940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>171.326419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2470.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words_count\n",
       "count  50000.000000\n",
       "mean     231.145940\n",
       "std      171.326419\n",
       "min        4.000000\n",
       "25%      126.000000\n",
       "50%      173.000000\n",
       "75%      280.000000\n",
       "max     2470.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **we will take a word one-hot vector of size 10,000**.\n",
    "- & so the review length doesn't matter, we'll just add up each word in the review to get a final representation of the review in a 10,000 vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's Preprocess the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df.type == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we delete columns we're not interested in\n",
    "del([train['type'], train['file'], train['words_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# now we transform label into a number\n",
    "train['y'] = train.apply(lambda row: int(row.label == 'pos'), axis=1)\n",
    "del train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle train now ..\n",
    "df = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for _, r in df.iterrows():\n",
    "    review, label = r['review'], r['y']\n",
    "    one_hot = np.zeros(10000)\n",
    "    tokens = get_tokens(review)\n",
    "    for token in tokens:\n",
    "        if token in word_to_index:\n",
    "            one_hot[word_to_index[token]] = int(1)\n",
    "    x.append(one_hot)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 10000), (25000,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the representations we need to move forward and create a dense neural network to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it From Memory Later.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to an embedding layer\n",
    "### Here is one more trick to make the network faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:33%\" src=\"static/imgs/11/dumb_network.png\"></div>\n",
    "\n",
    "- The first layer is the **dataset**.\n",
    "- this is followed by what's called a **linear layer**.\n",
    "- this is followed by a **ReLU Layer**, \n",
    "- Another Linear Layer.\n",
    "- and Then the Output, which is the **prediction layer**.\n",
    "- As it turns out, you can take a bit of a **shortcut** to **layer 1** by **replacing the 1st linear layer with an embedding layer.**\n",
    "- Taking a vector of 1s and 0s is mathematically equivalent to **summing several rows of a matrix**.\n",
    "- **We just sum W_0's rows that mark available words to form the unique \"embedding layer\"**.\n",
    "- Thus, **it's much more efficient to select the relevant rows of W_0 and sum them as opposed to doing a big vector-matrix multiplication**.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%\" src=\"static/imgs/11/embedding_layer.png\"></div>\n",
    "\n",
    "- Because the sentiment vocabulary is on the order of 70k words, most of the vector matrix multiplication is spent multiplying zeros in the input vector by weights before summing them, embeddings are much more efficient.\n",
    "- the only difference is that summing a bunch of rows is much **faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "IMDB_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/reviews.txt'\n",
    "IMDB_LABEL_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(IMDB_PATH, mode='r')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(IMDB_LABEL_PATH, mode='r')\n",
    "raw_labels = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_reviews), len(raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python's map object is an iterator\n",
    "# you can also convert map objects to lists, tupes, ..\n",
    "tokens = list(map(lambda x: set(x.split(\" \")), raw_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract the vocab\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if (len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all reviews to vectors\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for target data\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == \"positive\\n\":\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = 0.01, 2\n",
    "embedding_layer_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = 0.2 * np.random.random((len(vocab), embedding_layer_size)) - 0.1\n",
    "W1 = 0.2 * np.random.random((embedding_layer_size, 1)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: <built-in function iter>  Progress: 0 .  % Training Accuracy: 0.0 %\n",
      "Iter: <built-in function iter>  Progress: 04 .  % Training Accuracy: 0.45854145854145856 %\n",
      "Iter: <built-in function iter>  Progress: 08 .  % Training Accuracy: 0.591704147926037 %\n",
      "Iter: <built-in function iter>  Progress: 12 .  % Training Accuracy: 0.6691102965678107 %\n",
      "Iter: <built-in function iter>  Progress: 16 .  % Training Accuracy: 0.7000749812546864 %\n",
      "Iter: <built-in function iter>  Progress: 2 .  % Training Accuracy: 0.7204559088182364 %\n",
      "Iter: <built-in function iter>  Progress: 24 .  % Training Accuracy: 0.7382102982836194 %\n",
      "Iter: <built-in function iter>  Progress: 28 .  % Training Accuracy: 0.7546064847878875 %\n",
      "Iter: <built-in function iter>  Progress: 32 .  % Training Accuracy: 0.7671541057367829 %\n",
      "Iter: <built-in function iter>  Progress: 36 .  % Training Accuracy: 0.7763581824241751 %\n",
      "Iter: <built-in function iter>  Progress: 4 .  % Training Accuracy: 0.785921407859214 %\n",
      "Iter: <built-in function iter>  Progress: 44 .  % Training Accuracy: 0.792018907372057 %\n",
      "Iter: <built-in function iter>  Progress: 48 .  % Training Accuracy: 0.7962669777518541 %\n",
      "Iter: <built-in function iter>  Progress: 52 .  % Training Accuracy: 0.8014768094761942 %\n",
      "Iter: <built-in function iter>  Progress: 56 .  % Training Accuracy: 0.8062281265623884 %\n",
      "Iter: <built-in function iter>  Progress: 6 .  % Training Accuracy: 0.8081461235917605 %\n",
      "Iter: <built-in function iter>  Progress: 64 .  % Training Accuracy: 0.8090119367539529 %\n",
      "Iter: <built-in function iter>  Progress: 68 .  % Training Accuracy: 0.8115993176871948 %\n",
      "Iter: <built-in function iter>  Progress: 72 .  % Training Accuracy: 0.8143436475751347 %\n",
      "Iter: <built-in function iter>  Progress: 76 .  % Training Accuracy: 0.8165359717909584 %\n",
      "Iter: <built-in function iter>  Progress: 8 .  % Training Accuracy: 0.8197090145492726 %\n",
      "Iter: <built-in function iter>  Progress: 84 .  % Training Accuracy: 0.8218656254464073 %\n",
      "Iter: <built-in function iter>  Progress: 88 .  % Training Accuracy: 0.8243261669924095 %\n",
      "Iter: <built-in function iter>  Progress: 92 .  % Training Accuracy: 0.825920612147298 %\n",
      "Test Accuracy:  0.849\n",
      "Iter: <built-in function iter>  Progress: 0 .  % Training Accuracy: 0.8491508491508492 %\n",
      "Iter: <built-in function iter>  Progress: 04 .  % Training Accuracy: 0.8675662168915542 %\n",
      "Iter: <built-in function iter>  Progress: 08 .  % Training Accuracy: 0.8760413195601466 %\n",
      "Iter: <built-in function iter>  Progress: 12 .  % Training Accuracy: 0.8837790552361909 %\n",
      "Iter: <built-in function iter>  Progress: 16 .  % Training Accuracy: 0.8830233953209358 %\n",
      "Iter: <built-in function iter>  Progress: 2 .  % Training Accuracy: 0.8845192467922013 %\n",
      "Iter: <built-in function iter>  Progress: 24 .  % Training Accuracy: 0.8835880588487359 %\n",
      "Iter: <built-in function iter>  Progress: 28 .  % Training Accuracy: 0.8853893263342082 %\n",
      "Iter: <built-in function iter>  Progress: 32 .  % Training Accuracy: 0.8879013442950783 %\n",
      "Iter: <built-in function iter>  Progress: 36 .  % Training Accuracy: 0.8887111288871112 %\n",
      "Iter: <built-in function iter>  Progress: 4 .  % Training Accuracy: 0.8910099081901646 %\n",
      "Iter: <built-in function iter>  Progress: 44 .  % Training Accuracy: 0.8917590200816599 %\n",
      "Iter: <built-in function iter>  Progress: 48 .  % Training Accuracy: 0.8921621413737405 %\n",
      "Iter: <built-in function iter>  Progress: 52 .  % Training Accuracy: 0.8932933361902721 %\n",
      "Iter: <built-in function iter>  Progress: 56 .  % Training Accuracy: 0.894140390640624 %\n",
      "Iter: <built-in function iter>  Progress: 6 .  % Training Accuracy: 0.8946315855259046 %\n",
      "Iter: <built-in function iter>  Progress: 64 .  % Training Accuracy: 0.8933592141638728 %\n",
      "Iter: <built-in function iter>  Progress: 68 .  % Training Accuracy: 0.893894783623132 %\n",
      "Iter: <built-in function iter>  Progress: 72 .  % Training Accuracy: 0.8945844955528657 %\n",
      "Iter: <built-in function iter>  Progress: 76 .  % Training Accuracy: 0.8956052197390131 %\n",
      "Iter: <built-in function iter>  Progress: 8 .  % Training Accuracy: 0.8965763535069758 %\n",
      "Iter: <built-in function iter>  Progress: 84 .  % Training Accuracy: 0.8969137766465161 %\n",
      "Iter: <built-in function iter>  Progress: 88 .  % Training Accuracy: 0.8973522890309117 %\n",
      "Iter: <built-in function iter>  Progress: 92 .  % Training Accuracy: 0.896879296695971 %\n",
      "Test Accuracy:  0.845\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "correct, total = (0, 0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # leave last 1000 for testing\n",
    "    for i in range(len(input_dataset) - 1000):\n",
    "        # Forward Propagation\n",
    "        x, y = input_dataset[i], target_dataset[i]\n",
    "        layer_1 = sigmoid(np.sum(W0[x], axis=0))\n",
    "        layer_2 = sigmoid(layer_1.dot(W1))\n",
    "        \n",
    "        # Gradients Calc\n",
    "        layer_2_delta = (layer_2 - y)\n",
    "        layer_1_delta = layer_2_delta.dot(W1.T)\n",
    "        \n",
    "        # Backpropagation\n",
    "        W0[x] -= layer_1_delta*lr  # update only corresponding embeddings (w/o attached input to gradient).\n",
    "        W1 -= np.outer(layer_1, layer_2_delta) * lr\n",
    "        \n",
    "        # training accuracy\n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if (i%1000 == 0):\n",
    "            progress = str(i/float(len(input_dataset)))\n",
    "            print('\\rIter:', str(iter), ' Progress:', progress[2:4], '.', progress[4:6], '% Training Accuracy:', str(correct/float(total)), '%')\n",
    "            \n",
    "    # test set evaluation\n",
    "    correct, total = (0, 0)\n",
    "    for i in range(len(input_dataset) - 1000, len(input_dataset)):\n",
    "        x, y = input_dataset[i], target_dataset[i]\n",
    "        layer_1 = sigmoid(np.sum(W0[x], axis=0))\n",
    "        layer_2 = sigmoid(layer_1.dot(W1))\n",
    "        if(np.abs(layer_2-y) < 0.5):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print(\"Test Accuracy: \", str(correct/float(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Output\n",
    "### What did the Neural Network learn along the way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Network was looking for correlation between the input data points and the output data points.\n",
    "- It's extremely beneficial to know what kind of patterns the network detected while training and took as signal for predicting sentiment.\n",
    "- Just because the network was able to find correlation between the input and the output doesn't mean that it found every pattern of language.\n",
    "- Understanding what the difference between what the network is able to currently learn from data sets and what it should learn to truly understand language is very important & essential to solve artificial general intelligence.\n",
    "- What about language was our network able to learn?\n",
    "    - Let's start by considering was what **presented to the network**\n",
    "        - Presented Each review's vocabulary and asked for the network to classify if it's positive or negative.\n",
    "    - You'd expect the network to know which words have strong correlation with negative opinions and which are positive.\n",
    "- But this isn't the whole story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Architecture\n",
    "### How did the choice of architecture effect what the network was able to learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hidden layers are about grouping input data points coming from the previous layer into n groups.\n",
    "    - Each hidden neuron takes in a data point and asks \"is this data point in my group?\"\n",
    "    - As the hidden layer learns, it searches for useful groupings.\n",
    "- What are useful groupings?\n",
    "    - The grouping must be useful in the prediction of the output label.\n",
    "        - If it's not useful in predicting the output label, the network summarization won't allow the layer to find the groupings.\n",
    "    - A Grouping is useful if it finds hidden and interesting structure in the data.\n",
    "        - bad groupings just memorize data.\n",
    "        - good groupings capture phenomenas that are useful linguistically.\n",
    "- For example, understanding the difference between \"terrible\" and \"not terrible\" is a powerful grouping.\n",
    "- But because the input to the network is a vocabulary and not a sequence, \"It is Great, Not terribe\" will be interpreted exactly like \"It is Terrible, Not Good\".\n",
    "- If you can construct two examples with the same activation hidden layer & the pattern is present in the first example while absent in the 2nd, then the network couldn't detect the pattern you're interested in.\n",
    "- 2 Data Points (Movie Reviews) get the same prediction if they subscribe to most of the trained groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should you see in the weights connecting weights to hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Words that have similar predictive power should subscribe to similar groups.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:25%\" src=\"static/imgs/11/embedding_weights.png\"></div>\n",
    "\n",
    "- Words that have similar labels (positive or negative) will have similar weights.\n",
    "- Words that subscribe to similar groups, having similar weights, will have similar linguistic meaning with regards to the task at hand (sentiment analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Word Embeddings\n",
    "### How Can you Visualize Weight Similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can get the embedding of each word by simply extracting the corresponding row from the first weight matrix.\n",
    "- You do word-to-word comparison by simply calculating the euclidian distance between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = W0[index] - W0[target_index]\n",
    "        squared_difference = raw_difference**2\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will allow you to easily find out the similar words to a target word, examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beautiful', -0.0), ('beautifully', -0.7234740527429394), ('episodes', -0.7431900463968468), ('realistic', -0.757880033742247), ('beauty', -0.759955286643346), ('atmosphere', -0.7601443881195916), ('recommended', -0.7610825019283651), ('fun', -0.7701812509518946), ('great', -0.7834113731660364), ('bit', -0.7858516072936133)]\n"
     ]
    }
   ],
   "source": [
    "print(similar('beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrible', -0.0), ('disappointment', -0.7148509282587314), ('annoying', -0.7671560931689875), ('boring', -0.7899973527799934), ('worse', -0.796032596035763), ('laughable', -0.797488012028028), ('avoid', -0.8111283880874369), ('dull', -0.8210389457432301), ('mess', -0.833565219519962), ('disappointing', -0.8393998496880327)]\n"
     ]
    }
   ],
   "source": [
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('average', -0.0), ('manipulative', -0.6258354687765075), ('barbershop', -0.6512567886916747), ('gyneth', -0.6527740640628336), ('kolya', -0.6560375177956016), ('neverheless', -0.6569103115275596), ('broker', -0.6607965580519881), ('ghatak', -0.6613206616357246), ('triumf', -0.6639680365540314), ('shawn', -0.6645095432239714)]\n"
     ]
    }
   ],
   "source": [
    "print(similar('average'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', -0.0), ('know', -0.6807296076265122), ('classic', -0.7058610625390093), ('although', -0.7089204276967219), ('genius', -0.7132786925108467), ('touched', -0.7141661782554475), ('delicious', -0.7160007781267519), ('packed', -0.7161515900219529), ('satire', -0.7171565782119897), ('carrie', -0.7175034627599344)]\n"
     ]
    }
   ],
   "source": [
    "print(similar('love'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a standard phenomenon in the correlation summarization.\n",
    "- It seeks to create similar latent representations within the network to facilitate information compression to arrive to the correct target label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the meaning of a neuron?\n",
    "### Meaning is entirely based on the target labels being predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that \"Beautiful\" & \"Atmosphere\" are nearly identical, but **only in the context of sentiment prediction**.\n",
    "    - but in the other hand, their meaning is quite different (one is an adjective & the other is a noun).\n",
    "- The meaning of a neuron in the network depends entirely on the target labels.\n",
    "- The Neural Network is entirely ignorant of any other meaning outside the task it was trained on.\n",
    "- How do you make the meaning of a neuron more broad?\n",
    "    - Well, if you give it a task that requires broad understanding of language, it will learn new complexities and its neurons will become much more general.\n",
    "- The Task you'll use to learn more interesting word embeddings is the \"fill in the blank\" task.\n",
    "    - There is nearly infinite training data (the internet).\n",
    "        - Which means infinite signal to the network.\n",
    "    - Being able to learn to fill the blank requires at least some context language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the Blank\n",
    "### Learn Richer Word Meanings by having A Richer Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This example uses almost the same previous architecture with minor modifications.\n",
    "- You'll split the text into 5 words sentences, then remove one word (focus term), and train the network to predict the focus term.\n",
    "- you'll use a technique called **negative sampling** to make the network train a bit more faster.\n",
    "- Consider that in order to predict the focus term, you need one label for each possible word.\n",
    "    - This would require several thousand labels, which would cause the network to train slowly.\n",
    "    - To overcome this, let's randomly ignore most of the labels for each forward propagation.\n",
    "        - Although this seems crude, it's a technique that works well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "IMDB_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/reviews.txt'\n",
    "IMDB_LABEL_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(IMDB_PATH, 'r')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185, 127, 537)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(map(lambda x: x.split(\" \"), raw_reviews))\n",
    "len(tokens[0]), len(tokens[1]), len(tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in tokens:\n",
    "    for token in review:\n",
    "        word_counter[token] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = word_counter.most_common()  # least common in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`most_common()` just sorts out the data, it doesn't take the Top N most common tokens unless you force it to (by giving it an argument i think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(map(lambda x: x[0], word_counter.most_common())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for review in tokens:\n",
    "    review_indices = list()\n",
    "    for token in review:\n",
    "        try:\n",
    "            review_indices.append(word2index[token])\n",
    "            concatenated.append(word2index[token])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(review_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = (.05, 2)\n",
    "hidden_size, window, negative = 50, 2, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\n",
    "W1 = np.random.rand(len(vocab), hidden_size)*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74075, 50), (74075, 50))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W0.shape, W1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W1` Could simply be replaced by: `np.zeros(len(vocab), hidden_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_target = np.zeros(negative+1)\n",
    "layer_2_target[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = W0[index] - W0[target_index]\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0 [('terrible', -0.0), ('cognac', -0.3738881216269329), ('derisive', -0.38706720946311435), ('grease', -0.38977976159778677), ('accessory', -0.39072454239376825), ('constructs', -0.3918349061764808), ('misreads', -0.3923191534753908), ('rambles', -0.39428401242305783), ('mecha', -0.39500026848095804), ('starlets', -0.39594037298290563)]\n",
      "Progress:0.01 [('terrible', -0.0), ('ill', -0.6255941821256622), ('awesome', -0.6375994436921958), ('competent', -0.6407977227851339), ('troubled', -0.6531069241561729), ('jr', -0.6534538265536488), ('blockbuster', -0.6563001235939745), ('brief', -0.6606079018272691), ('marty', -0.6607438906298844), ('aubrey', -0.6627754935036241)]\n",
      "Progress:0.02 [('terrible', -0.0), ('superb', -1.0578247838536894), ('awesome', -1.2031760650904362), ('compelling', -1.203498585352637), ('pointless', -1.2124866217538417), ('solid', -1.216145731204235), ('fantastic', -1.2247588874668456), ('sad', -1.2297465716290998), ('okay', -1.2341042974148884), ('perfect', -1.2502682371546312)]\n",
      "Progress:0.03 [('terrible', -0.0), ('superb', -1.4173156292288298), ('brilliant', -1.4875116922641962), ('perfect', -1.5170961849725428), ('horrible', -1.5732355723485705), ('compelling', -1.5815101395261246), ('pointless', -1.6060673108285273), ('badly', -1.6468294582841656), ('stupid', -1.6488984383079257), ('l', -1.6518984452663394)]\n",
      "Progress:0.04 [('terrible', -0.0), ('nice', -1.683959997768794), ('horrible', -1.7672745714606068), ('fantastic', -1.8066476678564944), ('solid', -1.8253155697827514), ('tragic', -1.8281140356567775), ('l', -1.8396191035745912), ('offered', -1.8427081985972027), ('hilarious', -1.8475482678940365), ('cute', -1.8604495512809063)]\n",
      "Progress:0.05 [('terrible', -0.0), ('fantastic', -1.7564688069262742), ('ridiculous', -1.8343097595982265), ('solid', -1.843036191101936), ('essentially', -1.862819359929147), ('perfectly', -1.8945823965636441), ('horrible', -1.9269976010367644), ('okay', -1.9873786149160608), ('cute', -2.002522987747186), ('superb', -2.038013500856629)]\n",
      "Progress:0.06 [('terrible', -0.0), ('fantastic', -1.5907144132818436), ('fine', -1.740056287624307), ('brilliant', -1.7481647060197738), ('solid', -1.7848664966171082), ('rare', -1.8973813479793888), ('pure', -1.8988931914062774), ('superb', -1.911135800067484), ('essentially', -1.9155299440297227), ('magnificent', -1.9211939087368792)]\n",
      "Progress:0.07 [('terrible', -0.0), ('fantastic', -1.9307404652909796), ('teenager', -2.0242149273983916), ('solid', -2.0396191900714813), ('remarkable', -2.0540900669800206), ('brilliant', -2.056155455895821), ('fine', -2.0943223550532393), ('l', -2.0970854130645815), ('essentially', -2.1049908836385174), ('magnificent', -2.119813035821666)]\n",
      "Progress:0.08 [('terrible', -0.0), ('brilliant', -2.1573804177384326), ('fantastic', -2.2054813183792814), ('unique', -2.205794040972634), ('student', -2.2058162358278706), ('magnificent', -2.2490557020027055), ('remarkable', -2.264642428899016), ('essentially', -2.297700281145124), ('superb', -2.309181793638192), ('l', -2.3130805275452353)]\n",
      "Progress:0.09 [('terrible', -0.0), ('unique', -1.9630919416089674), ('fantastic', -2.0017978782819768), ('brilliant', -2.0115258979749178), ('manner', -2.113221728858162), ('student', -2.150926559163908), ('remarkable', -2.1678453823681), ('bizarre', -2.180299630618594), ('terrific', -2.1918570386195984), ('magnificent', -2.1925882438873225)]\n",
      "Progress:0.1 [('terrible', -0.0), ('unique', -2.02151020498893), ('fantastic', -2.0813491086642415), ('student', -2.147945067647603), ('teenager', -2.1729225111939443), ('dreadful', -2.192235865755016), ('brilliant', -2.23070301673223), ('magnificent', -2.2319957580080834), ('manner', -2.2419896635097984), ('terrific', -2.2506979890320093)]\n",
      "Progress:0.11 [('terrible', -0.0), ('fantastic', -2.3168089920155888), ('painful', -2.3266853862642494), ('dreadful', -2.3290950212721757), ('lame', -2.3642578911010124), ('bizarre', -2.369931187801232), ('remarkable', -2.3720170156094884), ('superb', -2.372061354714734), ('worthwhile', -2.3725085611023746), ('pleasant', -2.3811028219427346)]\n",
      "Progress:0.12 [('terrible', -0.0), ('lame', -2.3573421435559703), ('brilliant', -2.391402038723787), ('remarkable', -2.4114585303152554), ('bizarre', -2.4462875880208723), ('fantastic', -2.450324469372958), ('painful', -2.4643121222565854), ('worthwhile', -2.4826372313668417), ('format', -2.5329476035469005), ('desperate', -2.5464341479260217)]\n",
      "Progress:0.13 [('terrible', -0.0), ('lame', -2.33859580353737), ('brilliant', -2.4357519045570863), ('ridiculous', -2.5251522445430843), ('remarkable', -2.5745306165121518), ('painful', -2.596220289648302), ('fantastic', -2.610795105522625), ('laughable', -2.6615759389412004), ('horrible', -2.6788430511057686), ('bizarre', -2.6910619524599357)]\n",
      "Progress:0.14 [('terrible', -0.0), ('lame', -2.414217897492103), ('ridiculous', -2.4426944555115107), ('brilliant', -2.522655956850569), ('fantastic', -2.7372657365855613), ('horrible', -2.7591080021125456), ('dumb', -2.772351104710049), ('laughable', -2.788620030170762), ('remarkable', -2.8129700346752684), ('magnificent', -2.815448415667037)]\n",
      "Progress:0.15 [('terrible', -0.0), ('lame', -2.574654836846541), ('brilliant', -2.745990424687773), ('horrible', -2.908739975653882), ('fantastic', -2.910928136904808), ('ridiculous', -2.9361261613932017), ('dumb', -2.9741038347785436), ('hilarious', -2.987939515056298), ('magnificent', -3.0720018531771953), ('terrific', -3.0856641564248104)]\n",
      "Progress:0.16 [('terrible', -0.0), ('lame', -2.645191599403652), ('horrible', -2.9023544876882506), ('brilliant', -2.9362584250853185), ('terrific', -3.141542083933236), ('poor', -3.1519976150292273), ('hilarious', -3.1899855291119223), ('ridiculous', -3.191819365917436), ('magnificent', -3.2502556923295214), ('fantastic', -3.2663297418038963)]\n",
      "Progress:0.17 [('terrible', -0.0), ('horrible', -2.830254682249065), ('lame', -2.907228813694319), ('brilliant', -3.17236785320915), ('fantastic', -3.3439621459443494), ('ridiculous', -3.348598059543297), ('terrific', -3.3691563150239636), ('superb', -3.4218623345382992), ('hilarious', -3.4472865736157123), ('dumb', -3.4539699986762007)]\n",
      "Progress:0.18 [('terrible', -0.0), ('horrible', -2.5072729054906064), ('lame', -2.850755952257121), ('brilliant', -2.9832245382458757), ('superb', -3.186094209153268), ('fantastic', -3.2640191519598933), ('terrific', -3.2782088714976334), ('weak', -3.298794160886496), ('fascinating', -3.3610986749666116), ('magnificent', -3.3791976492104765)]\n",
      "Progress:0.19 [('terrible', -0.0), ('horrible', -2.7642326110888344), ('brilliant', -3.1541110782382815), ('superb', -3.308760119311531), ('fantastic', -3.3385058210297442), ('lame', -3.361227413349471), ('weak', -3.4086592003712717), ('compelling', -3.4809043134909636), ('poor', -3.4965046290468775), ('dreadful', -3.563763961481057)]\n",
      "Progress:0.2 [('terrible', -0.0), ('horrible', -2.927752839793251), ('brilliant', -3.429295832912943), ('fantastic', -3.4595304862158947), ('superb', -3.5896065584985704), ('lame', -3.629101884230367), ('dreadful', -3.680165013460219), ('compelling', -3.714928706450099), ('fascinating', -3.7301713086965713), ('poor', -3.7676582111221912)]\n",
      "Progress:0.21 [('terrible', -0.0), ('horrible', -3.076741153421989), ('superb', -3.6983442914294127), ('lame', -3.7237628650876227), ('weak', -3.7539766748232566), ('boring', -3.7774470923062404), ('unrealistic', -3.7794915871901167), ('brilliant', -3.8046178877757897), ('moving', -3.815421377712222), ('dreadful', -3.8296522192547386)]\n",
      "Progress:0.22 [('terrible', -0.0), ('horrible', -3.0547317910176397), ('fantastic', -3.355578928810344), ('brilliant', -3.4240304979280234), ('superb', -3.53247840304693), ('lame', -3.5529059291345604), ('remarkable', -3.6714615288559895), ('laughable', -3.7091680668061926), ('compelling', -3.7277432728180258), ('hilarious', -3.7497939427005686)]\n",
      "Progress:0.23 [('terrible', -0.0), ('horrible', -3.2705597975021155), ('brilliant', -3.314712095933311), ('lame', -3.3518738285992495), ('remarkable', -3.5563957379128404), ('fantastic', -3.584503285522282), ('dreadful', -3.6220446116989073), ('boring', -3.6242330530004385), ('dumb', -3.642124204486252), ('unique', -3.6509023278507424)]\n",
      "Progress:0.24 [('terrible', -0.0), ('horrible', -3.2797615093129218), ('brilliant', -3.2886120065096947), ('fantastic', -3.579494043630086), ('lame', -3.668514925745507), ('superb', -3.739984103337931), ('remarkable', -3.7577465389796716), ('great', -3.871126358921808), ('boring', -3.8716290118910877), ('limited', -3.924013004328151)]\n",
      "Progress:0.25 [('terrible', -0.0), ('horrible', -3.0598779141463703), ('brilliant', -3.2827998852030276), ('lame', -3.5818984128491667), ('fantastic', -3.6272513430652906), ('superb', -3.803219948779769), ('great', -3.8741019735580227), ('remarkable', -3.8897773367140536), ('lousy', -3.890195184598431), ('boring', -3.896875983133506)]\n",
      "Progress:0.26 [('terrible', -0.0), ('horrible', -3.116602999288676), ('brilliant', -3.3080882913143097), ('remarkable', -3.6915554795591565), ('lame', -3.731040889666555), ('superb', -3.756428354960779), ('fantastic', -3.773788440753402), ('pathetic', -3.805557175021458), ('wonderful', -3.922350618041276), ('weak', -3.9660546529282694)]\n",
      "Progress:0.27 [('terrible', -0.0), ('horrible', -2.9880629424591953), ('brilliant', -3.347982070320388), ('remarkable', -3.765660528439731), ('superb', -3.8160081118121236), ('fantastic', -3.856183280700724), ('wonderful', -3.885613821085064), ('lousy', -3.9536504943777877), ('dreadful', -3.9696802867986776), ('pathetic', -4.0026106950305556)]\n",
      "Progress:0.28 [('terrible', -0.0), ('horrible', -3.08941442209383), ('brilliant', -3.4744333497141215), ('lousy', -3.6505773850137455), ('laughable', -3.7647490067530254), ('fantastic', -3.8092452691249785), ('dumb', -3.8143795399770517), ('mediocre', -3.8335026329879307), ('remarkable', -3.835116010889034), ('pathetic', -3.8356200420109197)]\n",
      "Progress:0.29 [('terrible', -0.0), ('horrible', -3.283623663808263), ('brilliant', -3.724834614714983), ('laughable', -3.7533934302028737), ('fantastic', -3.7994013182531283), ('lousy', -3.8748228435543606), ('dreadful', -3.8962098539789496), ('mediocre', -3.8976022353052673), ('lame', -3.9508504094595516), ('remarkable', -3.950944231991155)]\n",
      "Progress:0.3 [('terrible', -0.0), ('horrible', -3.404903793504964), ('laughable', -3.8678471165465944), ('breathtaking', -3.8741566378846506), ('dreadful', -3.8781256267214936), ('mediocre', -3.880634980569056), ('fantastic', -3.8821806572477864), ('lousy', -3.8853688452745443), ('pathetic', -3.95840238178235), ('marvelous', -3.9585673802970334)]\n",
      "Progress:0.31 [('terrible', -0.0), ('horrible', -3.585210309445897), ('fantastic', -3.7682870979781264), ('weak', -3.9053949135706776), ('pathetic', -3.9404661700614607), ('superb', -3.998683036447182), ('dreadful', -3.9990444448287925), ('breathtaking', -4.005435087870906), ('funeral', -4.048159885043211), ('marvelous', -4.0638729729952745)]\n",
      "Progress:0.32 [('terrible', -0.0), ('horrible', -3.295398906731389), ('fantastic', -3.743872705547643), ('weak', -3.8118712912947985), ('pathetic', -3.8639271199554135), ('great', -3.939121746279988), ('superb', -3.9508044526940007), ('lousy', -4.0273971477734305), ('breathtaking', -4.028465562393784), ('mediocre', -4.060745930946747)]\n",
      "Progress:0.33 [('terrible', -0.0), ('horrible', -3.5114333036777587), ('pathetic', -3.797795476943267), ('laughable', -4.050281523561719), ('breathtaking', -4.059292131342258), ('weak', -4.0743757208560965), ('fantastic', -4.113123071728292), ('great', -4.1794392025148825), ('funeral', -4.182509643975104), ('marvelous', -4.182807525259816)]\n",
      "Progress:0.34 [('terrible', -0.0), ('horrible', -3.234690966658438), ('brilliant', -3.7749716526517014), ('stupid', -3.9572010349451494), ('lame', -3.968293666900254), ('pathetic', -4.001433729378176), ('fantastic', -4.044102531694206), ('weak', -4.094349207383565), ('laughable', -4.1114468268263415), ('dumb', -4.149484047093075)]\n",
      "Progress:0.35 [('terrible', -0.0), ('horrible', -3.121842703452971), ('brilliant', -3.7761125151632458), ('lame', -3.9570489139239298), ('pathetic', -3.9749678916674385), ('weak', -3.9870059783552465), ('laughable', -4.078667073850455), ('stupid', -4.148054454740136), ('breathtaking', -4.167801198869453), ('dumb', -4.190480287199553)]\n",
      "Progress:0.36 [('terrible', -0.0), ('horrible', -3.063833005265869), ('brilliant', -3.6203743062755716), ('lame', -3.9024296346663188), ('weak', -3.9346223656328645), ('pathetic', -3.951192123951121), ('superb', -4.031309487084784), ('breathtaking', -4.035236145134111), ('fantastic', -4.105398294697464), ('stupid', -4.105993046040433)]\n",
      "Progress:0.37 [('terrible', -0.0), ('horrible', -2.8685446695081884), ('brilliant', -3.41900914582724), ('superb', -3.9036135591406564), ('laughable', -3.9847487729486066), ('pathetic', -3.9892747632563257), ('lame', -3.998008269336891), ('weak', -4.10949540593884), ('fantastic', -4.19613853409303), ('breathtaking', -4.20449381320781)]\n",
      "Progress:0.38 [('terrible', -0.0), ('horrible', -2.7461339757429237), ('brilliant', -3.1938379242417714), ('laughable', -3.810733185304745), ('superb', -3.852831850322113), ('fantastic', -3.8601622663268644), ('lame', -3.9173474630917644), ('pathetic', -3.9775402663636426), ('breathtaking', -4.000899829396734), ('marvelous', -4.080114253658501)]\n",
      "Progress:0.39 [('terrible', -0.0), ('horrible', -2.7047378804998976), ('brilliant', -3.212656515425186), ('superb', -3.5487718043246876), ('fantastic', -3.7153789901973826), ('great', -3.9377185784149678), ('pathetic', -3.9476039577284925), ('laughable', -3.997742361448161), ('breathtaking', -4.0288613638212984), ('lame', -4.09433854250181)]\n",
      "Progress:0.4 [('terrible', -0.0), ('horrible', -2.6601338239137795), ('brilliant', -3.397999174364445), ('laughable', -3.7354570440785357), ('pathetic', -3.7505335935327992), ('superb', -3.755228603482433), ('lame', -3.8176977353618375), ('fantastic', -3.8748410412834753), ('breathtaking', -3.879859908160721), ('remarkable', -3.9621713732302415)]\n",
      "Progress:0.41 [('terrible', -0.0), ('horrible', -2.723913619118524), ('brilliant', -3.356315775014877), ('superb', -3.5127554095203064), ('laughable', -3.654284796257907), ('pathetic', -3.7027689398609156), ('fantastic', -3.802129966420995), ('bad', -3.830493183231909), ('lame', -3.836810604457255), ('wonderful', -3.8407130119169244)]\n",
      "Progress:0.42 [('terrible', -0.0), ('horrible', -3.07746334039869), ('brilliant', -3.193579711547745), ('superb', -3.3174126390259815), ('wonderful', -3.6484668308286072), ('laughable', -3.795550928219548), ('pathetic', -3.876556455593788), ('breathtaking', -3.877731921309592), ('fantastic', -3.9403505418977014), ('hilarious', -4.033289103517084)]\n",
      "Progress:0.43 [('terrible', -0.0), ('brilliant', -3.2759341801836865), ('horrible', -3.332535960779125), ('superb', -3.450915115646111), ('wonderful', -3.736073235604008), ('great', -3.8770923114423304), ('fantastic', -3.953520942341758), ('breathtaking', -4.001972259106058), ('stupid', -4.024976367920043), ('pathetic', -4.031657342379318)]\n",
      "Progress:0.44 [('terrible', -0.0), ('horrible', -3.2028381673926494), ('brilliant', -3.269424745519839), ('superb', -3.598476042818096), ('laughable', -3.8670405943159656), ('wonderful', -3.921350117316704), ('pathetic', -3.967969607661597), ('fantastic', -3.9911581150606956), ('stupid', -3.993494320552381), ('hilarious', -3.994752078276405)]\n",
      "Progress:0.45 [('terrible', -0.0), ('horrible', -2.76125969755545), ('brilliant', -3.0972769273708867), ('superb', -3.5778936913269246), ('bad', -3.8051113211650023), ('fantastic', -3.840252606612717), ('terrific', -3.877918225429699), ('pathetic', -3.8982546803037494), ('laughable', -3.9077405277554456), ('wonderful', -3.9307065519942928)]\n",
      "Progress:0.46 [('terrible', -0.0), ('horrible', -2.8502118880957608), ('superb', -3.2065352384338097), ('brilliant', -3.269422329972904), ('laughable', -3.7342180447598925), ('breathtaking', -3.8299548621613977), ('lame', -3.8486592795689245), ('pathetic', -3.8882063647296867), ('fantastic', -3.9407940187429285), ('wonderful', -3.961607993581969)]\n",
      "Progress:0.47 [('terrible', -0.0), ('horrible', -2.828447511394193), ('brilliant', -3.4739010546738633), ('superb', -3.4763893590887167), ('fantastic', -3.581061496216113), ('lame', -3.763450316209067), ('laughable', -3.7901003761664827), ('breathtaking', -3.9015600040405602), ('pathetic', -3.930328461414635), ('terrific', -3.937555087217021)]\n",
      "Progress:0.48 [('terrible', -0.0), ('horrible', -2.764055614866774), ('brilliant', -3.255540942187197), ('superb', -3.3448397624491815), ('lame', -3.622227718088523), ('fantastic', -3.692683943977049), ('terrific', -3.760776750440425), ('pathetic', -3.8169940139951146), ('laughable', -3.912082496437288), ('hilarious', -3.951208495187475)]\n",
      "Progress:0.49 [('terrible', -0.0), ('superb', -3.069033157557233), ('brilliant', -3.075599826366681), ('horrible', -3.1371459667387254), ('fantastic', -3.486136820103545), ('pathetic', -3.4891231980994424), ('terrific', -3.613779679296899), ('laughable', -3.7082004724298336), ('breathtaking', -3.7271089590477686), ('lame', -3.8111642345911108)]\n",
      "Progress:0.5 [('terrible', -0.0), ('horrible', -3.1115513244796436), ('brilliant', -3.325645463248577), ('superb', -3.3919999558850993), ('fantastic', -3.678968812961023), ('pathetic', -3.7144591103087508), ('lame', -3.943067389305184), ('breathtaking', -3.9898967755073125), ('terrific', -4.034545537691212), ('bad', -4.077745111616144)]\n",
      "Progress:0.51 [('terrible', -0.0), ('horrible', -3.2151202263959062), ('superb', -3.3319872856113015), ('brilliant', -3.585345975146444), ('fantastic', -3.877295766108294), ('lame', -3.9305456605232334), ('pathetic', -3.9361484798169575), ('laughable', -3.970858224271965), ('breathtaking', -4.103949284803123), ('terrific', -4.146250533133547)]\n",
      "Progress:0.52 [('terrible', -0.0), ('superb', -3.137481852849991), ('horrible', -3.236794071887919), ('brilliant', -3.306736013265137), ('laughable', -3.611518747188236), ('fantastic', -3.681776700322966), ('pathetic', -3.8116919163194964), ('breathtaking', -3.9083620063294666), ('ridiculous', -3.9850643328320907), ('lame', -4.008609747349892)]\n",
      "Progress:0.53 [('terrible', -0.0), ('horrible', -3.1187953952376297), ('superb', -3.182637874102477), ('brilliant', -3.2824376482394824), ('laughable', -3.6519872759248497), ('fantastic', -3.721926078785065), ('hilarious', -3.8794689701493223), ('pathetic', -3.9638059582174265), ('phenomenal', -4.0568740204380065), ('ridiculous', -4.090693703376699)]\n",
      "Progress:0.54 [('terrible', -0.0), ('horrible', -3.018596848206083), ('brilliant', -3.258034509683753), ('laughable', -3.2807684385238396), ('superb', -3.287483809014398), ('boring', -3.4728842738093135), ('fantastic', -3.663201961070087), ('hilarious', -3.6835412327372583), ('awful', -3.7864336064239987), ('pathetic', -3.894217179719121)]\n",
      "Progress:0.55 [('terrible', -0.0), ('horrible', -3.153659889053645), ('superb', -3.3433915053689005), ('brilliant', -3.4460779267443677), ('laughable', -3.545883850011711), ('fantastic', -3.714918885210454), ('ridiculous', -3.74489374574429), ('breathtaking', -3.766092568227881), ('hilarious', -3.845552826705626), ('fabulous', -3.8902879904219825)]\n",
      "Progress:0.56 [('terrible', -0.0), ('horrible', -3.101220277024857), ('brilliant', -3.1523171229315747), ('superb', -3.2358767524346272), ('fantastic', -3.536309498831494), ('laughable', -3.589715282629644), ('ridiculous', -3.7208229810238502), ('breathtaking', -3.7274359635157785), ('fabulous', -3.7919928115968435), ('hilarious', -3.8146400946044694)]\n",
      "Progress:0.57 [('terrible', -0.0), ('horrible', -3.116003971443442), ('brilliant', -3.218689960125203), ('fantastic', -3.437372639576054), ('laughable', -3.5612736490176014), ('superb', -3.6720490963195656), ('ridiculous', -3.832729568139751), ('boring', -3.855254773663552), ('breathtaking', -3.8555376692364667), ('pathetic', -3.8573814470780494)]\n",
      "Progress:0.58 [('terrible', -0.0), ('horrible', -3.281888459768499), ('brilliant', -3.338826309359405), ('fantastic', -3.4612613718969327), ('laughable', -3.5130288745616274), ('superb', -3.634648582801948), ('ridiculous', -3.6812968714088483), ('breathtaking', -3.7912556272033546), ('magnificent', -3.8104038039199937), ('hilarious', -3.8322968149565564)]\n",
      "Progress:0.59 [('terrible', -0.0), ('horrible', -3.115825353382858), ('fantastic', -3.330601629871516), ('brilliant', -3.395119922672406), ('laughable', -3.6262326755136316), ('ridiculous', -3.6561416051953914), ('superb', -3.695231099437528), ('breathtaking', -3.74078202325651), ('pathetic', -3.7857209515497536), ('haunting', -3.8420642539858147)]\n",
      "Progress:0.6 [('terrible', -0.0), ('horrible', -2.8195865492956487), ('fantastic', -3.232370381335543), ('brilliant', -3.335976565938482), ('breathtaking', -3.687858063147062), ('pathetic', -3.768774204478926), ('superb', -3.7890197338099973), ('magnificent', -3.799677006426847), ('ridiculous', -3.869594433662081), ('lame', -3.894094686314818)]\n",
      "Progress:0.61 [('terrible', -0.0), ('horrible', -3.0728680550525693), ('fantastic', -3.4148789487094), ('brilliant', -3.6405216038887076), ('breathtaking', -3.878636724146299), ('superb', -3.8841296097657922), ('magnificent', -3.9715760314093904), ('laughable', -4.0179682248157205), ('haunting', -4.026108383185635), ('dreadful', -4.071013289446259)]\n",
      "Progress:0.62 [('terrible', -0.0), ('horrible', -2.716024673260475), ('brilliant', -3.478029319867433), ('fantastic', -3.5914692184144514), ('magnificent', -3.896096576227817), ('breathtaking', -3.9669945204203403), ('dreadful', -3.9864113085745734), ('haunting', -4.037195006022838), ('lame', -4.090543830740831), ('laughable', -4.113014014158503)]\n",
      "Progress:0.63 [('terrible', -0.0), ('horrible', -2.787066675583338), ('brilliant', -3.5632065874846774), ('fantastic', -3.5686152148042374), ('laughable', -3.7468970296991944), ('magnificent', -3.8059870210497464), ('breathtaking', -3.8409011349625772), ('pathetic', -3.8887431130308676), ('lame', -3.9079178109970987), ('ridiculous', -3.953255269205686)]\n",
      "Progress:0.64 [('terrible', -0.0), ('horrible', -2.7877712401459362), ('ridiculous', -3.5506963125858175), ('brilliant', -3.6385712465624067), ('fantastic', -3.6454199289205698), ('lame', -3.7502776389820998), ('breathtaking', -3.8052086953075173), ('pathetic', -3.8069043453211315), ('laughable', -3.8819285563482944), ('fabulous', -3.9172018691776067)]\n",
      "Progress:0.65 [('terrible', -0.0), ('horrible', -2.793892106663677), ('ridiculous', -3.6347211271878526), ('brilliant', -3.6608966673549728), ('fantastic', -3.684451923067048), ('pathetic', -3.697735092884331), ('laughable', -3.7504626668525454), ('breathtaking', -3.8168018042374188), ('lame', -3.852569158591643), ('fabulous', -3.941128045507945)]\n",
      "Progress:0.66 [('terrible', -0.0), ('horrible', -2.8782063448813595), ('brilliant', -3.6458256010587387), ('ridiculous', -3.7279932102516655), ('pathetic', -3.7911036406493213), ('fantastic', -3.886606532425245), ('laughable', -3.9337856656553454), ('magnificent', -3.9471817113704724), ('lame', -3.9571954589952023), ('fabulous', -4.053792408916167)]\n",
      "Progress:0.67 [('terrible', -0.0), ('horrible', -2.7129508631504513), ('brilliant', -3.5474335419476457), ('ridiculous', -3.7496043118441547), ('fantastic', -3.83715747064835), ('lame', -3.881438551775267), ('pathetic', -3.8870390179907077), ('magnificent', -3.923162866611613), ('wonderful', -4.012897557187215), ('breathtaking', -4.014507686455303)]\n",
      "Progress:0.68 [('terrible', -0.0), ('horrible', -2.6303087341832994), ('brilliant', -3.3464548670962717), ('superb', -3.8329287208694915), ('pathetic', -3.866426361942947), ('fantastic', -3.8834879982664225), ('magnificent', -3.887196543128739), ('ridiculous', -3.9121218075608786), ('lame', -3.92424998107895), ('breathtaking', -3.9746015420165093)]\n",
      "Progress:0.69 [('terrible', -0.0), ('horrible', -2.6135103973992764), ('brilliant', -3.402121998175835), ('magnificent', -3.7733815813331697), ('superb', -3.8378321159447055), ('dreadful', -3.8650581097411965), ('fantastic', -3.906230684167045), ('ridiculous', -3.9089948271258073), ('pathetic', -4.0062557823786875), ('lame', -4.020710730093272)]\n",
      "Progress:0.7 [('terrible', -0.0), ('horrible', -2.4979833559425173), ('brilliant', -3.255865552391195), ('magnificent', -3.6999127347411185), ('superb', -3.7178295174620364), ('fantastic', -3.7272627016252904), ('pathetic', -3.7969017784742873), ('dreadful', -3.830064573021702), ('laughable', -3.857336414688362), ('breathtaking', -3.8755912022341588)]\n",
      "Progress:0.71 [('terrible', -0.0), ('horrible', -2.5284100120738633), ('brilliant', -3.281729320379148), ('dreadful', -3.777737120506343), ('pathetic', -3.8512302324837657), ('superb', -3.885190532804791), ('fantastic', -3.947458970406462), ('breathtaking', -3.9507679021853135), ('laughable', -4.006033259239), ('wonderful', -4.021386739658449)]\n",
      "Progress:0.72 [('terrible', -0.0), ('horrible', -2.9874598365243594), ('brilliant', -3.102390289998375), ('superb', -3.828935308506984), ('fantastic', -3.8582121721695666), ('wonderful', -3.9848351082158473), ('breathtaking', -3.9978475436751055), ('dreadful', -4.007342335405401), ('pathetic', -4.009034482760945), ('lame', -4.089250774799445)]\n",
      "Progress:0.73 [('terrible', -0.0), ('horrible', -2.827466524277995), ('brilliant', -3.1615349004980704), ('pathetic', -3.754761363634609), ('fantastic', -3.887052414431559), ('breathtaking', -3.8989167249630725), ('dreadful', -3.9225908526432476), ('wonderful', -3.9252357707748766), ('horrendous', -3.935624525206476), ('lame', -3.9971526569790785)]\n",
      "Progress:0.74 [('terrible', -0.0), ('horrible', -2.871174611250882), ('brilliant', -3.2662304871374914), ('fantastic', -3.7126372149354068), ('stupid', -3.7610427186750437), ('pathetic', -3.868718523938397), ('wonderful', -3.915861343292705), ('horrendous', -3.937878970536581), ('breathtaking', -4.091099000317098), ('dreadful', -4.115363164662117)]\n",
      "Progress:0.75 [('terrible', -0.0), ('horrible', -2.8784825662505376), ('brilliant', -3.420317591893649), ('fantastic', -3.700711426575647), ('wonderful', -3.7476649070734327), ('pathetic', -3.85414560874648), ('stupid', -4.064642115058354), ('breathtaking', -4.107069097594986), ('horrendous', -4.117593607371204), ('marvelous', -4.198533120267949)]\n",
      "Progress:0.76 [('terrible', -0.0), ('horrible', -2.7351023602221867), ('brilliant', -3.3292753788333735), ('pathetic', -3.4741009784980177), ('fantastic', -3.543004745742391), ('wonderful', -3.5569360231845004), ('laughable', -3.729303288052469), ('breathtaking', -3.7621278851937414), ('superb', -3.7771646516298927), ('horrendous', -3.8312169831815592)]\n",
      "Progress:0.77 [('terrible', -0.0), ('horrible', -2.634408523974374), ('brilliant', -3.257299890095159), ('laughable', -3.505325447662299), ('pathetic', -3.587732281294579), ('fantastic', -3.6096890456186035), ('wonderful', -3.757919038189487), ('horrendous', -3.776848542456144), ('breathtaking', -3.8020651604271873), ('stupid', -3.849843133097568)]\n",
      "Progress:0.78 [('terrible', -0.0), ('horrible', -2.9083021453159272), ('brilliant', -3.5637570785813986), ('laughable', -3.6023141537641754), ('fantastic', -3.718684342471911), ('wonderful', -3.8676216665717167), ('pathetic', -3.893534918480537), ('marvelous', -3.902633404629294), ('breathtaking', -3.9268296014769914), ('stupid', -3.931904354148739)]\n",
      "Progress:0.79 [('terrible', -0.0), ('horrible', -2.8409967481042555), ('laughable', -3.6629784212957084), ('brilliant', -3.703520343106405), ('breathtaking', -3.7692122106047203), ('dire', -3.7782376454489426), ('marvelous', -3.830430524244488), ('fantastic', -3.8413756531561343), ('dreadful', -3.851062913917513), ('fabulous', -3.9000800155318336)]\n",
      "Progress:0.8 [('terrible', -0.0), ('horrible', -3.304029397344581), ('laughable', -3.9187750021242604), ('dire', -3.928264774622541), ('fantastic', -3.9402315106396992), ('fabulous', -3.9654548186590945), ('brilliant', -3.9754621595729556), ('breathtaking', -4.022526705173569), ('dreadful', -4.030098805496747), ('superb', -4.074185181979759)]\n",
      "Progress:0.81 [('terrible', -0.0), ('horrible', -3.5261949117771114), ('wonderful', -3.799829704025303), ('fantastic', -3.8538411000533688), ('fabulous', -3.9514302007708912), ('superb', -4.115822303940166), ('phenomenal', -4.138693154862801), ('breathtaking', -4.174791226611573), ('dire', -4.178953114752066), ('brilliant', -4.182497237485823)]\n",
      "Progress:0.82 [('terrible', -0.0), ('horrible', -3.3003685747668956), ('fantastic', -3.9340853614913183), ('superb', -3.961260915008417), ('fabulous', -4.007878658764461), ('brilliant', -4.065608464985201), ('dreadful', -4.14251077807687), ('fine', -4.155192490709154), ('wonderful', -4.164533641845059), ('breathtaking', -4.168242751216982)]\n",
      "Progress:0.83 [('terrible', -0.0), ('horrible', -3.6313490178846157), ('superb', -3.984190496238942), ('brilliant', -4.111249415296756), ('horrendous', -4.137903058749896), ('fantastic', -4.155635728560963), ('fabulous', -4.160124679896802), ('dire', -4.206372073063586), ('pathetic', -4.207383673828694), ('breathtaking', -4.249720735920193)]\n",
      "Progress:0.84 [('terrible', -0.0), ('horrible', -3.121141925622775), ('fantastic', -3.916777891314931), ('superb', -3.9307132554726247), ('brilliant', -3.9879462156412235), ('breathtaking', -4.06663366317498), ('fabulous', -4.091532394710557), ('dire', -4.139806758700985), ('horrendous', -4.218820758657895), ('dreadful', -4.2555072604343485)]\n",
      "Progress:0.85 [('terrible', -0.0), ('horrible', -3.157930083625574), ('brilliant', -3.6797470933005036), ('horrendous', -3.88339005898192), ('fantastic', -3.941912246143473), ('dreadful', -3.9502361637672676), ('horrid', -3.9696733158315585), ('fabulous', -3.9899814321583613), ('breathtaking', -4.004698806658384), ('laughable', -4.019195837727104)]\n",
      "Progress:0.86 [('terrible', -0.0), ('horrible', -3.063809454191829), ('brilliant', -3.1224228106701593), ('superb', -3.7417939983859947), ('fabulous', -3.794002107267695), ('fantastic', -3.8288748193929747), ('horrendous', -3.8494958345798422), ('breathtaking', -3.870590254656486), ('dreadful', -3.8980986000751825), ('laughable', -3.9107135846608503)]\n",
      "Progress:0.87 [('terrible', -0.0), ('horrible', -2.77589461895389), ('brilliant', -3.0476630873570847), ('superb', -3.724609319591195), ('laughable', -3.8380294977070912), ('fabulous', -3.840082370775877), ('horrendous', -3.8733595209462375), ('breathtaking', -3.9712340785452014), ('pathetic', -3.9712948804772825), ('dreadful', -3.9933202253857556)]\n",
      "Progress:0.88 [('terrible', -0.0), ('horrible', -2.6091484654509958), ('brilliant', -2.8341314506077437), ('horrendous', -3.7801426568925804), ('fabulous', -3.7980211630192757), ('dreadful', -3.8374300331569726), ('dire', -3.9740454560000367), ('pathetic', -4.005503635622233), ('superb', -4.00790633550221), ('breathtaking', -4.010321189622527)]\n",
      "Progress:0.89 [('terrible', -0.0), ('horrible', -2.8061487481057923), ('brilliant', -3.117537335035275), ('dreadful', -3.9046270341911518), ('fabulous', -3.9212021966587542), ('superb', -3.9224484111277884), ('fantastic', -3.9287205969094043), ('horrendous', -3.9334385805439975), ('marvelous', -4.028031467981574), ('magnificent', -4.032040729296659)]\n",
      "Progress:0.9 [('terrible', -0.0), ('horrible', -2.8674102428193162), ('brilliant', -3.414279465320048), ('horrendous', -3.7415830823871037), ('dreadful', -3.8115691010949386), ('fabulous', -3.9524421290768608), ('magnificent', -3.959911192029792), ('dire', -4.010516349244781), ('fantastic', -4.0262126324032455), ('phenomenal', -4.036118789453933)]\n",
      "Progress:0.91 [('terrible', -0.0), ('horrible', -2.8560766000871163), ('brilliant', -3.5576203352569715), ('horrendous', -3.8140242304555803), ('fantastic', -3.8653865854004907), ('great', -3.886657278111274), ('magnificent', -4.0056578358077894), ('dreadful', -4.024818023296332), ('wonderful', -4.0362906053936936), ('laughable', -4.048849544410786)]\n",
      "Progress:0.92 [('terrible', -0.0), ('horrible', -3.1738162915562578), ('brilliant', -3.7497642722399624), ('superb', -3.8339194439996747), ('horrendous', -3.897149114990593), ('dreadful', -4.127503205318968), ('breathtaking', -4.12898889826799), ('fantastic', -4.146463740527068), ('magnificent', -4.170777140195071), ('marvelous', -4.176383921671143)]\n",
      "Progress:0.93 [('terrible', -0.0), ('horrible', -3.0325921858551794), ('brilliant', -3.4473822511866015), ('superb', -3.843042470925995), ('horrendous', -4.064650432595171), ('dreadful', -4.091815955645666), ('wonderful', -4.092865130956425), ('magnificent', -4.132468047765273), ('breathtaking', -4.148548961546576), ('stupid', -4.162609277625845)]\n",
      "Progress:0.94 [('terrible', -0.0), ('horrible', -3.029565843674032), ('brilliant', -3.228121816011993), ('superb', -3.4715404782760153), ('horrendous', -3.941855977226284), ('pathetic', -3.9443682048041704), ('phenomenal', -4.025218871578845), ('dreadful', -4.0751642715119205), ('breathtaking', -4.099616272332179), ('hilarious', -4.1039233085929405)]\n",
      "Progress:0.95 [('terrible', -0.0), ('horrible', -2.9906497815320066), ('brilliant', -3.6020470634169026), ('superb', -3.690858780493361), ('pathetic', -3.8686169638822925), ('stupid', -3.9721304495763086), ('dreadful', -4.018190049438689), ('horrendous', -4.0453415306557385), ('bad', -4.08823253895453), ('breathtaking', -4.089245498585952)]\n",
      "Progress:0.96 [('terrible', -0.0), ('horrible', -2.896330508329976), ('superb', -3.6345600224826558), ('brilliant', -3.6816738329442513), ('pathetic', -3.7847597216119606), ('masterful', -3.95700657653414), ('breathtaking', -4.090595290864077), ('phenomenal', -4.105380828499431), ('fantastic', -4.107556763961269), ('horrendous', -4.125436987345343)]\n",
      "Progress:0.97 [('terrible', -0.0), ('horrible', -2.8950082913538573), ('brilliant', -3.4129231922796706), ('superb', -3.5726131055652344), ('masterful', -3.927594458513282), ('phenomenal', -3.9642706192398345), ('pathetic', -3.97682621835208), ('fantastic', -4.024495902074088), ('marvelous', -4.145307274629247), ('magnificent', -4.150151948367563)]\n",
      "Progress:0.98 [('terrible', -0.0), ('horrible', -2.993295607765446), ('superb', -3.512848882523045), ('brilliant', -3.5357941464290548), ('phenomenal', -3.8630501153381434), ('masterful', -3.875947385844044), ('pathetic', -3.9550301077227847), ('marvelous', -4.0257674836281385), ('terrific', -4.039200475424797), ('miserable', -4.047453981705279)]\n",
      "Progress:0.99 [('terrible', -0.0), ('horrible', -2.996206906543707), ('brilliant', -3.2519744548350356), ('superb', -3.695620862068613), ('pathetic', -3.771017703675723), ('phenomenal', -3.795246889115822), ('terrific', -3.930512873228861), ('masterful', -4.022962019748555), ('fantastic', -4.037999652020295), ('marvelous', -4.1087754152647085)]\n",
      "[('terrible', -0.0), ('horrible', -2.8727079868976175), ('brilliant', -3.3788871655393007), ('pathetic', -3.877319797611651), ('phenomenal', -3.892570702195353), ('superb', -3.9095109680194473), ('bad', -3.949674801907263), ('masterful', -4.017110299342879), ('marvelous', -4.117363658337549), ('dreadful', -4.2133555584662705)]\n"
     ]
    }
   ],
   "source": [
    "for review_i, review in enumerate(input_dataset * epochs):\n",
    "    for target_i in range(len(review)):\n",
    "        # predict only a random subset, because it's really expensive to predict every vocab\n",
    "        # We can't do a softmax over all possible words, we will predict for the target word + a subset of the total vocab\n",
    "        target_samples = [review[target_i]] + list(concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "        \n",
    "        # get tokens on the right & on Left of target word\n",
    "        left_context = review[max(0, target_i-window):target_i]\n",
    "        right_context = review[target_i+1: min(len(review), target_i+window)]\n",
    "        \n",
    "        # feed forward\n",
    "        # context words w/o target word\n",
    "        # mean instead of sum, interesting\n",
    "        layer_1 = np.mean(W0[left_context+right_context], axis=0)\n",
    "        # using sigmoid here is kind of weird because there is only one true target token\n",
    "        layer_2 = sigmoid(layer_1.dot(W1[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(W1[target_samples])\n",
    "        \n",
    "        # update weights\n",
    "        W0[left_context+right_context] -= layer_1_delta*lr\n",
    "        W1[target_samples] -= np.outer(layer_2_delta, layer_1)*lr\n",
    "        \n",
    "    if(review_i % 500 == 0):\n",
    "        print('\\rProgress:'+str(review_i/float(len(input_dataset)*epochs)) + \" \" + str(similar('terrible')))\n",
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Word Embeddings get trained according to the task the neural network is trained on:\n",
    "    - Sentiment Analysis: Embeddings are grouped together depending on how Positive/Negative they are\n",
    "        - Or depending on How they effect a Review being good or bad.\n",
    "    - Filling the Blank: Embeddings are grouped together depending on how close/far they are when filling blanks.\n",
    "        - Solve: \"*I ___ You so much!*\"\n",
    "            - Possible Solution — \"I hate You so much!\"\n",
    "            - Possible Solution — \"I love You so much!\"\n",
    "        - In this sense, hate & love are pretty close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaning is Derived from Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:50%\" src=\"static/imgs/11/loss_matters.png\"></div>\n",
    "\n",
    "- Before, words were clustered according to the likelihood that the review is positive/negative.\n",
    "- Now, they are clustered based on the likelihood that they will occur on the same phrase.\n",
    "    - Regardless of sentiment.\n",
    "- The key takeaway is that even though you are training on the same dataset, using a very similar network architecture, you can influence what the network learns by changing the loss function (task).\n",
    "- Even though it's looking at the same information, you can alter its learning behavior by simply changing the input/output structure.\n",
    "- Let's call the process of choosing what the network should learn: *Intelligence Targeting**.\n",
    "- You can also change how the network measures error, its architetcure, and regularization, this is also a way of performing Intelligence targeting.\n",
    "- In deep learning research, all of the above techniques fall under the umbrella term: **Loss function construction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks don't really **LEARN** Data; they minimize Loss Functions\n",
    "### The Choice of Loss Function Determines the Network's Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considering Learning is all about Minimizing a Loss Function gives a broader understanding of how neural networks work.\n",
    "- Different Kinds of Layers, Activations, Regularization Techniques, datasets, aren't really that different.\n",
    "    - For Example: If the network is overfitting, you can augment the loss fucntion by choosing simpler non-linearities, adding dropout, enforcing regularizations, adding more data and so on..\n",
    "    - All of these techniques will have a similar effect on the loss function and the learning behavior.\n",
    "- With learning, everything is contained within the loss function.\n",
    "    - & **If something is going wrong, remember that the solution is in the Loss Function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## King - Man + Woman ~= Queen\n",
    "### Word Analogies are an interesting consequence of the previously trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This represents one of the famous properties of word embeddings (or trained vectors).\n",
    "- The task of filling in the blank creates an interesting property called \"word analogies\".\n",
    "    - whereas you can take different embeddings and perform algebric operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive=['terrible', 'good'], negative=['bad']):\n",
    "    norms = np.sum(W0*W0, axis=1)\n",
    "    norms.resize((norms.shape[0], 1))\n",
    "    # normalize weights for vector-level operations\n",
    "    normed_weights = W0 * norms\n",
    "    query_vect = np.zeros(W0.shape[1])\n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[word2index[word]]\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[word2index[word]]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = W0[index] - query_vect\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christopher', -196.14901513962437),\n",
       " ('tom', -196.63100891837615),\n",
       " ('william', -196.71139862402313),\n",
       " ('mr', -196.78191513069368),\n",
       " ('simon', -196.82182610970864),\n",
       " ('it', -196.82971818242368),\n",
       " ('him', -196.83517620210762),\n",
       " ('been', -196.85949829130325),\n",
       " ('gary', -196.90340914909922)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(['elizabeth', 'he'], ['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies\n",
    "### Linear Compression of an Existing Property in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though the \"Word Analogy\" Discovery was initially very exciting, the deep learning NLP paradigm didn't move forward from that to discover new features, instead, current language models rely on ~~Recurrent Neural Networks to do language modeling~~\n",
    "    - *Book was released before ELMO, BERT, GPT-2,.. that is why he considers RNNs to be the SoTA in Language modeling.*\n",
    "- Nevertheless, we need to understand why this concept emerged out of the network as a result of us training the network to fill in the blank?\n",
    "- If you imagine the word embeddings to have two dimensions, then it would be easier to know why word analogies work:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:33%\" src=\"static/imgs/11/word_analogies.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = [.6, .1]\n",
    "man = [.5, .0]\n",
    "woman = [.0, .8]\n",
    "queen = [.1, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09999999999999998, 0.1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_i - y_i for (x_i, y_i) in zip(king, man)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.19999999999999996]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_i - y_i for (x_i, y_i) in zip(queen, woman)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The relative usefulness to the final prediction between \"Man\"/\"King\" & \"Woman\"/\"Queen\" is similar, Why?\n",
    "    - The difference between \"King\" and \"Man\" Leaves a vector of Royalty.\n",
    "    - **There are a bunch of male/female related words in one grouping, & a bunch of king/queen related words in another grouping.**\n",
    "        - & because the relative distance between the two group is constant, it means that the distances between each grouping items will be relatively the same.\n",
    "    - This can be traced back to the chosen loss.\n",
    "- **This is more about the properties of language than deep learning**.\n",
    "    - Any linear compression of these co-occurent statistics will yield the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### You've learned a lot about Word embeddings & the impact of loss on learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've unpacked the principles of using neural networks to model language.\n",
    "- **I encourage you to build the examples of this chapter from scratch.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
