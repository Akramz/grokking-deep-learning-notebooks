{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks that write like Shakespeare\n",
    "## Recurrent Layers for Varible-length data\n",
    "\n",
    "In this Chapter:\n",
    "- The Challenge of arbitrary length\n",
    "- The surprising power of averaged word vectors\n",
    "- The limitations of bag-of-words vectors\n",
    "- Using identity vectors to sum word embeddings\n",
    "- Learning the transition matrices\n",
    "- Learning to create useful sentence vectors\n",
    "- Forward Propagation in Python\n",
    "- Forward Propagation and backpropagation with arbitrary length\n",
    "- Weight update with arbitrary length\n",
    "\n",
    "> \"There is something magical about recurrent neural networks.\" â€” Andrej Karpathy \"[The unreasonable effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Arbitrary Length\n",
    "### Let's Model Arbitrary Long Sequences of Data using Neural Networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this chapter, we'll expand on the intuition of an embedding conveying the meaning of a word by creating embeddings that convey the meaning of variable-length phrases and setences.\n",
    "- If you want to create a vector that held an entire sequence of symbols within its contents in the same way a word embedding holds information about the word in the form of a vector, how could you do that?\n",
    "    - If you concatenated or stacked each word embedding in the sentence, you will have a vector of sorts, that represents the whole sentence.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/12/stacked_embeddings.png\" /></div>\n",
    "\n",
    "    - But this approach leaves something to be desired, bigger sentences will have more stacked embeddings.\n",
    "        - for example, we can't compare two sentences because they have different shapes.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 66%;\" src=\"static/imgs/12/bigger_stacking.png\" /></div>\n",
    "\n",
    "- In Theory, **These two sentences should be very similar**.\n",
    "    - comparing their vectors should indicate high similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do comparisons really matter?\n",
    "### Why you should care about whether you can compare two sentence vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The act of comparing two vectors is useful because it allows us to understand **what the network sees**.\n",
    "    - Even though you can't read two vectors, but you can check their similarity.\n",
    "- **We're trying to take the perspective of the Neural Network**.\n",
    "- We want to create sentence vectors that are useful for predicting things about the sentence.\n",
    "    - meaning, at the minimum, similar setences should in theory result in similar vector representations.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:33%\" src=\"static/imgs/12/sentence_avg.png\" /></div>\n",
    "\n",
    "- What if you take the vector for each word in the sentence, & average them?\n",
    "    - This technique loses words order information.\n",
    "    - You don't have to worry about alignment because each word vector is in the same length.\n",
    "    - The sentences \"The Cat Sat\" & \"The Cat Sat Still\" will have similar vector representations because most words are the same.\n",
    "    - Even Better, it's likely that \"A Dog Walked\" will be similar to \"A Cat Sat\" because **The Actual word representations are similar**.\n",
    "    - We can conclude that the techniuqe of averaging words is a strong one, although not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Suprising Power of Averaged Word Vectors\n",
    "### It's the amazingly powerful go-to tool for neural prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To play w/ the word embeddings, let's first train them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "IMDB_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/reviews.txt'\n",
    "IMDB_LABEL_PATH = '/Users/mohamedakramzaytar/data/2019/Q2/kaggle/IMDB/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(IMDB_PATH, 'r')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(lambda x: x.split(\" \"), raw_reviews))\n",
    "word_counter = Counter()\n",
    "for review in tokens:\n",
    "    for token in review:\n",
    "        word_counter[token] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = word_counter.most_common()\n",
    "vocab = list(set(map(lambda x: x[0], word_counter.most_common())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for review in tokens:\n",
    "    review_indices = list()\n",
    "    for token in review:\n",
    "        try:\n",
    "            review_indices.append(word2index[token])\n",
    "            concatenated.append(word2index[token])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(review_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = (.05, 2)\n",
    "hidden_size, window, negative = 50, 2, 5\n",
    "W0 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\n",
    "W1 = np.random.rand(len(vocab), hidden_size)*0\n",
    "layer_2_target = np.zeros(negative+1)\n",
    "layer_2_target[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = W0[index] - W0[target_index]\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0 [('terrible', -0.0), ('blonds', -0.3654412638186205), ('attorneys', -0.367364060533589), ('scholl', -0.37857095718170775), ('wobbled', -0.3846620588625614), ('amigos', -0.38828169492535225), ('everone', -0.39031209722516463), ('liven', -0.3916415195643817), ('dalmations', -0.39196846770427124), ('songmaking', -0.39706417374877395)]\n",
      "Progress:0.1 [('terrible', -0.0), ('unique', -2.102853834978891), ('deeply', -2.1269095545304975), ('fantastic', -2.134201302034517), ('teenager', -2.141660779348933), ('student', -2.159428257086483), ('l', -2.193384034424099), ('magnificent', -2.2123980782900663), ('chair', -2.2322289168112253), ('tough', -2.234419307670878)]\n",
      "Progress:0.2 [('terrible', -0.0), ('horrible', -2.9320848761835907), ('fantastic', -3.2986277104215103), ('brilliant', -3.39900604739518), ('lame', -3.5567762284556257), ('superb', -3.58729188938931), ('compelling', -3.650515192612599), ('remarkable', -3.661610806593889), ('fascinating', -3.7767323824299095), ('tedious', -3.7822610442162423)]\n",
      "Progress:0.3 [('terrible', -0.0), ('horrible', -3.442134112634085), ('laughable', -3.794718218181005), ('marvelous', -3.8202060659252752), ('remarkable', -3.8489043019109346), ('dreadful', -3.8523152880445566), ('fantastic', -3.8793518032202665), ('forgettable', -3.8825373704875488), ('breathtaking', -3.888474155120369), ('tremendous', -3.9092436158012336)]\n",
      "Progress:0.4 [('terrible', -0.0), ('horrible', -3.019398168600297), ('brilliant', -3.555897858020765), ('pathetic', -3.759742016568757), ('superb', -3.818176728146027), ('breathtaking', -3.832505411197472), ('remarkable', -3.861882094518598), ('laughable', -3.8626760955820854), ('fantastic', -3.9829597922734505), ('lame', -3.985391021953064)]\n",
      "Progress:0.5 [('terrible', -0.0), ('brilliant', -3.263409473564162), ('horrible', -3.3257291932923043), ('superb', -3.386505815808378), ('pathetic', -3.7188214193483278), ('fantastic', -3.8043822546206107), ('lame', -3.958477886124808), ('breathtaking', -3.9864987802579437), ('wonderful', -4.069469563974056), ('remarkable', -4.098199445710329)]\n",
      "Progress:0.6 [('terrible', -0.0), ('horrible', -2.8820283809129323), ('fantastic', -3.153066697809395), ('brilliant', -3.1718054206060753), ('magnificent', -3.625771965133177), ('pathetic', -3.67401816209855), ('breathtaking', -3.683531786913266), ('superb', -3.7919301655102373), ('ridiculous', -3.816617747025522), ('haunting', -3.877146969135773)]\n",
      "Progress:0.7 [('terrible', -0.0), ('horrible', -2.487184770113473), ('brilliant', -3.2608184466592416), ('superb', -3.7209070138265594), ('magnificent', -3.752719885109285), ('fantastic', -3.779011571886598), ('pathetic', -3.9118019539504534), ('breathtaking', -3.932868343976472), ('laughable', -3.957784479381653), ('dreadful', -3.9657950074030124)]\n",
      "Progress:0.8 [('terrible', -0.0), ('horrible', -3.2084712099485304), ('dire', -3.828204505116444), ('brilliant', -3.8445598322595105), ('laughable', -3.8971270807551788), ('superb', -3.9378133975662104), ('fantastic', -3.946023227306032), ('breathtaking', -3.9796126583730755), ('fabulous', -4.047230017186402), ('horrid', -4.050682228588375)]\n",
      "Progress:0.9 [('terrible', -0.0), ('horrible', -2.914691624455943), ('brilliant', -3.2874781332349823), ('phenomenal', -3.826553075420352), ('dreadful', -3.8266455497639122), ('horrendous', -3.8684057341209255), ('dire', -3.8977971769868556), ('marvelous', -3.941399910237248), ('breathtaking', -3.9705020504168718), ('fabulous', -4.03673712983776)]\n",
      "[('terrible', -0.0), ('horrible', -2.753529112565692), ('brilliant', -3.2977000768673816), ('pathetic', -3.734530531813245), ('phenomenal', -3.785243297594829), ('superb', -3.8438918745684942), ('masterful', -3.9395594508693987), ('marvelous', -3.9873328037364986), ('mediocre', -4.031029456908627), ('bad', -4.05999901148217)]\n"
     ]
    }
   ],
   "source": [
    "for review_i, review in enumerate(input_dataset * epochs):\n",
    "    for target_i in range(len(review)):\n",
    "        target_samples = [review[target_i]] + list(concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "        left_context = review[max(0, target_i-window):target_i]\n",
    "        right_context = review[target_i+1: min(len(review), target_i+window)]\n",
    "        layer_1 = np.mean(W0[left_context+right_context], axis=0)\n",
    "        layer_2 = sigmoid(layer_1.dot(W1[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(W1[target_samples])\n",
    "        W0[left_context+right_context] -= layer_1_delta*lr\n",
    "        W1[target_samples] -= np.outer(layer_2_delta, layer_1)*lr\n",
    "    if(review_i % 5000 == 0):\n",
    "        print('\\rProgress:'+str(review_i/float(len(input_dataset)*epochs)) + \" \" + str(similar('terrible')))\n",
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's experiment with average sentence embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.sum(W0*W0, axis=1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "normed_weights = W0*norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sent_vect(words):\n",
    "    # 1st part get index of each word\n",
    "    # 2nd part filters only words in vocab\n",
    "    indices = list(map(lambda x: word2index[x], filter(lambda x: x in word2index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_vectors = list()\n",
    "for review in tokens:\n",
    "    reviews_to_vectors.append(make_sent_vect(review))\n",
    "reviews_to_vectors = np.array(reviews_to_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Counter()\n",
    "    for i, val in enumerate(reviews_to_vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    most_similar = list()\n",
    "    for idx, score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0:40])\n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you havn  t seen this movie i highly ',\n",
       " 'i have never seen such terrible performa',\n",
       " 'i think this show is definitely the grea']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews(['beautiful', 'amazing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **there appear to be interesting statistical information within these vectors, such as when negative & embeddings cluster together**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is information stored in these embeddings?\n",
    "### When you average word embeddings, average shapes remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I recommend digesting this kind of information over a period of time.\n",
    "- For a moment, I want you to consider that a word vector can be represented visually as a squiggly line, like this one:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 66%;\" src=\"static/imgs/12/word_vector.png\" /></div>\n",
    "\n",
    "- Instead of thinking that a vector is just a list of numbers, visualize it as a line that goes through low/high points that corresponds to the low/high numbers in the vector.\n",
    "- If you selected several words from the corpus, they might look like these:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/12/word_vectors.png\" /></div>\n",
    "\n",
    "- Consider the similaries between the various words & notice that each word shape is unique.\n",
    "- But \"terrible\" & \"boring\" have certain similarities in their shapes.\n",
    "- The most interesting thing is that **part of these squiggles have meaning in and of themselves**.\n",
    "- **Consider the negative words, there is nothing magical about the common spike these words have in common, if I retrained the network, the spide would appear somewhere else, what gives the \"negative\" sensation to the shape is that all negative words have it.**\n",
    "- These shapes were molded by training so that every part of the curve convery a certain meaning that would prove useful in the correlation summarization task.\n",
    "- **When you take an average curve over multiple word embeddings, common meaning holds while other meanings cancel out or get weakened**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a Neural Network use embeddings?\n",
    "### Neural Networks detect the curves that have correlation with the target label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Neural Network simply looks for various bumps and variations in the input layer and compresses that information to predict the target label (missing word/sentiment/...)\n",
    "- Through training, the Neural Network shapes the word embeddings in such a way that it clusters them, all in the goal for it to make easier predictions of the target label.\n",
    "    - It compresses the signal in a way that allows it to minimize its loss function -> correctly predict the target label.\n",
    "- Think about when you'll have longer sentences, the loger the sentence, the more the true signal will be averaged to zero, since you will have a lot of average words to links concepts & ideas in the sentence, so avereging word embeddings is a bit mushy.\n",
    "    - The longer the sentence, the more likely that it will average out to be a straight line.\n",
    "        - A Vector of near zeros.\n",
    "- In short, this concept of storing a sentence in a fixed length vector doesn't decay nicely.\n",
    "- That being said, typical sentences aren't that long anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Limitations of Bag-of-Words Vectors\n",
    "### Order becomes irrelevant when you average word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The biggest issue with averaged word embeddings is that they lose the order information**.\n",
    "- This approach also ignores grammar and syntax.\n",
    "- This way of averaging or summing a bunch of words to have a sentence/text representation is called the bag-of-words approach.\n",
    "    - A Bag, because order isn't preserved.\n",
    "- We want a way to represent sentence vectors that preserve order and yield different representations for different word order.\n",
    "- More importantly, **the way in which order changes the resulting vector should be learned**.\n",
    "    - In this way, the neural network will find an order representation that minimizes its loss function and yield interesting properties about word order in general.\n",
    "- One of the most famous and successful ways of generating vectors for sequences are Recurrent Neural Networks (RNNs).\n",
    "- Identity matrices have one property: if you multiply any vector by them, you'll get the same vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Identity Vectors to sum word embeddings\n",
    "### Let's implement the same logic using a different approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width: 33%;\" src=\"static/imgs/12/standard_sum.png\" /></div>\n",
    "\n",
    "- This is the standard way of simply summing up the word embeddings to form a fixed length sentence embedding.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 33%;\" src=\"static/imgs/12/ID_sum.png\" /></div>\n",
    "\n",
    "- the example above adds another step while summing, it multiplies each element by the identity matrix.\n",
    "- Not that we're not doing anything special here, this indentity way of summing up the elements yields the same resulting vector as the first example.\n",
    "- but consider this: **If the matrices used are anything but the identity matrix, changing the order of the word embeddings will change the resulting vector**.\n",
    "- Let's see this in Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices that change absolutely nothing\n",
    "### Let's create sentence embeddings using the Identity matrix in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([.1, .2, .3])\n",
    "c = np.array([-1, -.5, 0])\n",
    "d = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = np.eye(3)\n",
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3.]),\n",
       " array([0.1, 0.2, 0.3]),\n",
       " array([-1. , -0.5,  0. ]),\n",
       " array([0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(identity), b.dot(identity), c.dot(identity), d.dot(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "this = np.array([2, 4, 6])\n",
    "movie = np.array([10, 10, 10])\n",
    "rocks = np.array([1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "print(this + movie + rocks)\n",
    "print(((this.dot(identity) + movie).dot(identity) + rocks).dot(identity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we yielded the same results just because the identity matrix is a very special type of matrix.\n",
    "    - In fact, **the identity matrix is the only matrix capable of returning the same vector as doing direct sum**, no other matrix has this guarantee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Transition Matrices\n",
    "### What if you allowed the identity matrices to change to minimize the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's remember the goal: generating sequence embeddings that cluster according to the meaning of the sentence. \n",
    "    - Such that give a sentence vector, we can find sentences with similar meaning.\n",
    "- More specifically, these sentence embeddings should care about the word order in their corresponding sentences\n",
    "- Our hypothesis is that if we used the word vectors dotted with corresponding identity matrices, and turned the identity matrices into **weight matrices**, the network will preserve order and extract meaningful sentence representations that make use of words order.\n",
    "- So we'll going to learn this matrices?\n",
    "- Whenever you train a neural network, you should set a task for it to learn useful representations, in our case, what is the task that will allow the network to extract useful sentence embeddings?\n",
    "    - **Training a Network that takes a list of words & learns to predict the Next Word**.\n",
    "        - In Other words, A **Language Model**.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 66%;\" src=\"static/imgs/12/LM.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Create Useful Sentence Vectors\n",
    "### Create the Sentence Vector, Make a Prediction, & Modify the sentence vector via its parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I don't want you to think about this network as the previous neural networks.\n",
    "- Instead, **think about creating a sentence embedding, and use it to predict the next word, and then modifying the respective parts (word embeddings, weight matrices) to make the prediction a little bit better**.\n",
    "- The Neural Network will look like something in the figure:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/12/primitive_RNN.png\" /><img style=\"width: 50%;\" src=\"static/imgs/12/sketch_RNN.jpg\" /></div>\n",
    "\n",
    "- RNNs are composed of two steps\n",
    "    - Create the input sentence embedding using the weight matrices and the word vector representations.\n",
    "    - Use the input vector representation to predict the probability for each vocab token to be the next word.\n",
    "- Will will initialize the weight matrices as identity matrices, however, they will change through training just as the word embeddings & weights corresponding to the dense classification layers.\n",
    "- **You'll also constrain the transition matrices to be the same, meaning:**\n",
    "    - $W_0=W_1=W_2...$\n",
    "    - Whatever logic learned in one transition is use in any other transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation in Python\n",
    "### Let's take this Idea & see how to perform a forward propagation for our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that you have a conceptual idea of what you're trying to build, let's check out a toy version in Python.\n",
    "- First, let's create the weights (I am using a limited vocab of 9 words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    v = np.atleast_2d(v)\n",
    "    return np.exp(v)/np.sum(np.exp(v), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial word embeddings\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0., 0., 0.]])\n",
    "word_vects['bears'] = np.array([[0., 0., 0.]])\n",
    "word_vects['braves'] = np.array([[0., 0., 0.]])\n",
    "word_vects['red'] = np.array([[0., 0., 0.]])\n",
    "word_vects['sox'] = np.array([[0., 0., 0.]])\n",
    "word_vects['lose'] = np.array([[0., 0., 0.]])\n",
    "word_vects['defeat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['beat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['tie'] = np.array([[0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding to output classification weights\n",
    "sent2output = np.random.rand(3, len(word_vects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial weight matrices\n",
    "identity = np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code creates 3 sets of weights:\n",
    "    - A Dictionnary of Word Embeddings.\n",
    "    - The Identity or Transition Matrix.\n",
    "    - A Classification Layer.\n",
    "- The Classification layer serves to predict the next word over the word vocab, using the sentence vector representation.\n",
    "- With these tools, forward propagation is trivial, let's take an example:\n",
    "    - \"red sox defeat\" -> \"yankees\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# akramz solution\n",
    "sent_repr = ((word_vects['red'].dot(identity) + word_vects['sox']).dot(identity) + word_vects['defeat']).dot(identity)\n",
    "pred = sent_repr.dot(sent2output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book's solution\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "pred = softmax(layer_2.dot(sent2output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you backpropagate into this?\n",
    "### It might seem trickier, but they're the same steps you already learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width: 66%;\" src=\"static/imgs/12/RNN_Architecture.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which direction should I backprop in?\n",
    "    - You could go back through the identity matrix and further, & you could also calculate the gradients over the individual word embeddings injected in each layer.\n",
    "- When you add two vectors together during forward propagation, you will backpropagate the same gradient into both sides of addition.\n",
    "- When you generate `layer_2_delta`, you backpropagate it twice, once across the identity matrix to create `layer_1_delta` and again to `word_vectors['defeat']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0])  # one-hot vector for yankees\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "defeat_delta = layer_2_delta * 1\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "sox_delta = layer_1_delta * 1\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)  # which is `red_delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "word_vects['red'] -= layer_0_delta*lr\n",
    "word_vects['sox'] -= sox_delta*lr\n",
    "word_vects['defeat'] -= defeat_delta*lr\n",
    "identity -= np.outer(layer_0, layer_1_delta)*lr\n",
    "identity -= np.outer(layer_1, layer_2_delta)*lr\n",
    "sent2output -= np.outer(layer_2, pred_delta)*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bears': array([[0., 0., 0.]]),\n",
       " 'beat': array([[0., 0., 0.]]),\n",
       " 'braves': array([[0., 0., 0.]]),\n",
       " 'defeat': array([[0.00431805, 0.00286998, 0.00065565]]),\n",
       " 'lose': array([[0., 0., 0.]]),\n",
       " 'red': array([[0.00431805, 0.00286998, 0.00065565]]),\n",
       " 'sox': array([[0.00431805, 0.00286998, 0.00065565]]),\n",
       " 'tie': array([[0., 0., 0.]]),\n",
       " 'yankees': array([[0., 0., 0.]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Train it!\n",
    "### You have all the tools; let's train the network on a toy corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's first train the network on a toy dataset called \"Babi dataset\".\n",
    "- This dataset is a synthetically generated Q/A corpus to teach machines how to answer questions about an environment.\n",
    "- First let's download and decompress the Babi dataset using bash commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-17 10:31:47--  http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz\n",
      "Resolving www.thespermwhale.com (www.thespermwhale.com)... 69.65.3.213\n",
      "Connecting to www.thespermwhale.com (www.thespermwhale.com)|69.65.3.213|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1282454 (1.2M) [application/x-gzip]\n",
      "Saving to: 'tasks_1-20_v1-1.tar.gz'\n",
      "\n",
      "tasks_1-20_v1-1.tar 100%[===================>]   1.22M   871KB/s    in 1.4s    \n",
      "\n",
      "2019-05-17 10:31:49 (871 KB/s) - 'tasks_1-20_v1-1.tar.gz' saved [1282454/1282454]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv tasks_1-20_v1-1.tar.gz static/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tasksv11/\n",
      "x tasksv11/en/\n",
      "x tasksv11/._LICENSE\n",
      "x tasksv11/LICENSE\n",
      "x tasksv11/README\n",
      "x tasksv11/shuffled/\n",
      "x tasksv11/shuffled/qa10_indefinite-knowledge_test.txt\n",
      "x tasksv11/shuffled/qa10_indefinite-knowledge_train.txt\n",
      "x tasksv11/shuffled/qa11_basic-coreference_test.txt\n",
      "x tasksv11/shuffled/qa11_basic-coreference_train.txt\n",
      "x tasksv11/shuffled/qa12_conjunction_test.txt\n",
      "x tasksv11/shuffled/qa12_conjunction_train.txt\n",
      "x tasksv11/shuffled/qa13_compound-coreference_test.txt\n",
      "x tasksv11/shuffled/qa13_compound-coreference_train.txt\n",
      "x tasksv11/shuffled/qa14_time-reasoning_test.txt\n",
      "x tasksv11/shuffled/qa14_time-reasoning_train.txt\n",
      "x tasksv11/shuffled/qa15_basic-deduction_test.txt\n",
      "x tasksv11/shuffled/qa15_basic-deduction_train.txt\n",
      "x tasksv11/shuffled/qa16_basic-induction_test.txt\n",
      "x tasksv11/shuffled/qa16_basic-induction_train.txt\n",
      "x tasksv11/shuffled/qa17_positional-reasoning_test.txt\n",
      "x tasksv11/shuffled/qa17_positional-reasoning_train.txt\n",
      "x tasksv11/shuffled/qa18_size-reasoning_test.txt\n",
      "x tasksv11/shuffled/qa18_size-reasoning_train.txt\n",
      "x tasksv11/shuffled/qa19_path-finding_test.txt\n",
      "x tasksv11/shuffled/qa19_path-finding_train.txt\n",
      "x tasksv11/shuffled/qa1_single-supporting-fact_test.txt\n",
      "x tasksv11/shuffled/qa1_single-supporting-fact_train.txt\n",
      "x tasksv11/shuffled/qa20_agents-motivations_test.txt\n",
      "x tasksv11/shuffled/qa20_agents-motivations_train.txt\n",
      "x tasksv11/shuffled/qa2_two-supporting-facts_test.txt\n",
      "x tasksv11/shuffled/qa2_two-supporting-facts_train.txt\n",
      "x tasksv11/shuffled/qa3_three-supporting-facts_test.txt\n",
      "x tasksv11/shuffled/qa3_three-supporting-facts_train.txt\n",
      "x tasksv11/shuffled/qa4_two-arg-relations_test.txt\n",
      "x tasksv11/shuffled/qa4_two-arg-relations_train.txt\n",
      "x tasksv11/shuffled/qa5_three-arg-relations_test.txt\n",
      "x tasksv11/shuffled/qa5_three-arg-relations_train.txt\n",
      "x tasksv11/shuffled/qa6_yes-no-questions_test.txt\n",
      "x tasksv11/shuffled/qa6_yes-no-questions_train.txt\n",
      "x tasksv11/shuffled/qa7_counting_test.txt\n",
      "x tasksv11/shuffled/qa7_counting_train.txt\n",
      "x tasksv11/shuffled/qa8_lists-sets_test.txt\n",
      "x tasksv11/shuffled/qa8_lists-sets_train.txt\n",
      "x tasksv11/shuffled/qa9_simple-negation_test.txt\n",
      "x tasksv11/shuffled/qa9_simple-negation_train.txt\n",
      "x tasksv11/en/qa10_indefinite-knowledge_test.txt\n",
      "x tasksv11/en/qa10_indefinite-knowledge_train.txt\n",
      "x tasksv11/en/qa11_basic-coreference_test.txt\n",
      "x tasksv11/en/qa11_basic-coreference_train.txt\n",
      "x tasksv11/en/qa12_conjunction_test.txt\n",
      "x tasksv11/en/qa12_conjunction_train.txt\n",
      "x tasksv11/en/qa13_compound-coreference_test.txt\n",
      "x tasksv11/en/qa13_compound-coreference_train.txt\n",
      "x tasksv11/en/qa14_time-reasoning_test.txt\n",
      "x tasksv11/en/qa14_time-reasoning_train.txt\n",
      "x tasksv11/en/qa15_basic-deduction_test.txt\n",
      "x tasksv11/en/qa15_basic-deduction_train.txt\n",
      "x tasksv11/en/qa16_basic-induction_test.txt\n",
      "x tasksv11/en/qa16_basic-induction_train.txt\n",
      "x tasksv11/en/qa17_positional-reasoning_test.txt\n",
      "x tasksv11/en/qa17_positional-reasoning_train.txt\n",
      "x tasksv11/en/qa18_size-reasoning_test.txt\n",
      "x tasksv11/en/qa18_size-reasoning_train.txt\n",
      "x tasksv11/en/qa19_path-finding_test.txt\n",
      "x tasksv11/en/qa19_path-finding_train.txt\n",
      "x tasksv11/en/qa1_single-supporting-fact_test.txt\n",
      "x tasksv11/en/qa1_single-supporting-fact_train.txt\n",
      "x tasksv11/en/qa20_agents-motivations_test.txt\n",
      "x tasksv11/en/qa20_agents-motivations_train.txt\n",
      "x tasksv11/en/qa2_two-supporting-facts_test.txt\n",
      "x tasksv11/en/qa2_two-supporting-facts_train.txt\n",
      "x tasksv11/en/qa3_three-supporting-facts_test.txt\n",
      "x tasksv11/en/qa3_three-supporting-facts_train.txt\n",
      "x tasksv11/en/qa4_two-arg-relations_test.txt\n",
      "x tasksv11/en/qa4_two-arg-relations_train.txt\n",
      "x tasksv11/en/qa5_three-arg-relations_test.txt\n",
      "x tasksv11/en/qa5_three-arg-relations_train.txt\n",
      "x tasksv11/en/qa6_yes-no-questions_test.txt\n",
      "x tasksv11/en/qa6_yes-no-questions_train.txt\n",
      "x tasksv11/en/qa7_counting_test.txt\n",
      "x tasksv11/en/qa7_counting_train.txt\n",
      "x tasksv11/en/qa8_lists-sets_test.txt\n",
      "x tasksv11/en/qa8_lists-sets_train.txt\n",
      "x tasksv11/en/qa9_simple-negation_test.txt\n",
      "x tasksv11/en/qa9_simple-negation_train.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf static/data/tasks_1-20_v1-1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With some simple Python, you can open, clean, & preprocess the data to feed it to the recurrent neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('static/data/tasksv11/en/qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for line in raw[:1000]:\n",
    "    sentences.append(line.lower().replace(\"\\n\", \"\").split(\" \")[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mary', 'moved', 'to', 'the', 'bathroom.'],\n",
       " ['john', 'went', 'to', 'the', 'hallway.'],\n",
       " ['where', 'is', 'mary?', '\\tbathroom\\t1']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset contains a lot of simple statements and questions.\n",
    "    - Each Question is followed by the correct answer.\n",
    "- When used in the Context of QA, the neural network reads the statements then answers the question.\n",
    "- For now, your network will attempt to finish each sentence when given one or more starting words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Things Up\n",
    "### Before you can create matrices, you need to learn how many parameters you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2indices(sent):\n",
    "    indices = list()\n",
    "    for word in sent:\n",
    "        indices.append(word2index[word])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    e_v = np.exp(v - np.max(v))\n",
    "    return e_v / e_v.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we set the word embedding size to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "word_embeddings = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1\n",
    "W0 = np.eye(embed_size)\n",
    "empty_sentence_embedding = np.zeros(embed_size)\n",
    "W1 = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "# target vocab vectors for the loss function\n",
    "y_hots = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10), (10, 82))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W0.shape, W1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation with Arbitrary Length\n",
    "### You'll forward propagate using the same logic described earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"static/imgs/12/enhanced_RNNs.jpg\" /></div>\n",
    "\n",
    "- Instead of predicting only the last word, you will make a prediction for each timestep using all previous words.\n",
    "    - This is more efficient than doing a new forward propagation everytime you want to predict a new word (from the beginning of the phrase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = empty_sentence_embedding\n",
    "    layers.append(layer)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # forward propagates\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "        # tries to predict the next term\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(W1))\n",
    "        # `sent[target_i]` gets actual word, which is a number that represent word in vocab, then we get its prediction in the proba distribution\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        # generates the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(W0) + word_embeddings[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The List called \"layers\" represent a new way of doing forward propagation.**\n",
    "- Notice that you end up doing more forward propagation if the sentence length is longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation with Arbitrary Length\n",
    "### You'll backpropagate using the same logic described earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's implement backpropagation over arbitrary sequence lengths.\n",
    "- the most important object is the **layers** list, which has two vectors\n",
    "    - `layer['state']`\n",
    "    - `layer['previous_hidden']`\n",
    "- In order to backpropagate, you'll take the output's gradient and add a new objec to each list called `layer['state_delta']` which will represent the gradient at that layer.\n",
    "- You're building the same logic in a way that it can consume the variable-length sequences from the forward propagation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went', 'to', 'the', 'hallway.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(30000):\n",
    "    # forward propagation\n",
    "    lr = .001\n",
    "    # getting sentence indices w/o 1st word\n",
    "    # why it leaves the first word of each sentence? -> to use it instead of \"NaN\" as first layer['hidden']\n",
    "    sent = sent2indices(sentences[iter%len(sentences)][1:])  \n",
    "    layers, loss = predict(sent)  # predicts, returns hidden layer embeddings + loss\n",
    "    \n",
    "    # backpropagation\n",
    "    for layer_i in reversed(range(len(layers))):  # loop over hidden states (layers)\n",
    "        layer = layers[layer_i]  # get corresponding layer\n",
    "        target = sent[layer_i - 1]  # get target word\n",
    "        \n",
    "        # If not the 1st layer\n",
    "        if (layer_i > 0):  # if not first layer\n",
    "            layer['output_delta'] = layer['pred'] - y_hots[target]  # delta\n",
    "            new_hidden_delta = layer['output_delta'].dot(W1.transpose())  # gradient\n",
    "            \n",
    "            # If last layer, don't pull from a later one, because it doesn't exist\n",
    "            # seems that for each hidden layer, its hidden delta is depedent upon the last decoding operation + next layer gradient\n",
    "            if(layer_i == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_i+1]['hidden_delta'].dot(W0.transpose())\n",
    "        else:  # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_i+1]['hidden_delta'].dot(W0.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trying to Visualize what is happening:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"static/imgs/12/storing_info_RNN.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update with arbitrary length\n",
    "### You'll update weights using the same logic describe earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After having stored the gradients for each layer, now we need to add the Weight Update Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity :81.91346989524257\n",
      "Perplexity :81.79164804556522\n",
      "Perplexity :81.59136670120363\n",
      "Perplexity :81.21617876879513\n",
      "Perplexity :80.46101504475695\n",
      "Perplexity :78.81151765899324\n",
      "Perplexity :74.57593512893804\n",
      "Perplexity :58.196352288038895\n",
      "Perplexity :30.665797467081802\n",
      "Perplexity :20.852192551772724\n",
      "Perplexity :19.04945943831619\n",
      "Perplexity :17.626854858639998\n",
      "Perplexity :15.61871693979982\n",
      "Perplexity :12.630565276246934\n",
      "Perplexity :9.303686888390308\n",
      "Perplexity :7.371916951350519\n",
      "Perplexity :6.33961157289624\n",
      "Perplexity :5.634372087983678\n",
      "Perplexity :5.208649397251203\n",
      "Perplexity :4.937183016678388\n",
      "Perplexity :4.758495401236964\n",
      "Perplexity :4.648590542164584\n",
      "Perplexity :4.578039220744333\n",
      "Perplexity :4.514585657013284\n",
      "Perplexity :4.44146200313975\n",
      "Perplexity :4.356558965071526\n",
      "Perplexity :4.2645645179658525\n",
      "Perplexity :4.1708183240915195\n",
      "Perplexity :4.079941605049494\n",
      "Perplexity :3.9976836650012704\n"
     ]
    }
   ],
   "source": [
    "for iter in range(30000):\n",
    "    # forward propagation\n",
    "    lr = .001\n",
    "    # getting sentence indices w/o 1st word\n",
    "    # why it leaves the first word of each sentence? -> to use it instead of \"NaN\" as first layer['hidden']\n",
    "    sent = sent2indices(sentences[iter%len(sentences)][1:])  \n",
    "    layers, loss = predict(sent)  # predicts, returns hidden layer embeddings + loss\n",
    "    \n",
    "    # backpropagation\n",
    "    for layer_i in reversed(range(len(layers))):  # loop over hidden states (layers)\n",
    "        layer = layers[layer_i]  # get corresponding layer\n",
    "        target = sent[layer_i - 1]  # get target word\n",
    "        \n",
    "        # If not the 1st layer\n",
    "        if (layer_i > 0):  # if not first layer\n",
    "            layer['output_delta'] = layer['pred'] - y_hots[target]  # delta\n",
    "            new_hidden_delta = layer['output_delta'].dot(W1.transpose())  # gradient\n",
    "            \n",
    "            # If last layer, don't pull from a later one, because it doesn't exist\n",
    "            # seems that for each hidden layer, its hidden delta is depedent upon the last decoding operation + next layer gradient\n",
    "            if(layer_i == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_i+1]['hidden_delta'].dot(W0.transpose())\n",
    "        else:  # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_i+1]['hidden_delta'].dot(W0.transpose())\n",
    "    \n",
    "    # Update weights of NaN Embedding\n",
    "    empty_sentence_embedding -= layers[0]['hidden_delta'] * lr / float(len(sent))\n",
    "    for layer_i, layer in enumerate(layers[1:]):\n",
    "        # update decoder\n",
    "        W1 -= np.outer(layers[layer_i][\"hidden\"], layer['output_delta'])*lr / float(len(sent))\n",
    "        embed_i = sent[layer_i]\n",
    "        # update embeddings\n",
    "        word_embeddings[embed_i] -= layers[layer_i]['hidden_delta']*lr/float(len(sent))\n",
    "        # update encoder\n",
    "        W0 -= np.outer(layers[layer_i]['hidden'], layer['hidden_delta']) * lr / float(len(sent))\n",
    "    \n",
    "    if (iter % 1000 == 0):\n",
    "        print(\"Perplexity :\" + str(np.exp(loss/len(sent))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is Perplexity ?**\n",
    "    - In information theory, preplexity is a measurement that predicts how well a probability distribution or a probability model predicts a sample.\n",
    "        - It may be used to compare probability models.\n",
    "    - A low Perplexity indicate that the model is good at predicting a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution and Output Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perplexity is the probability of the corrent label (word), passed through a log function, negated, and exponentiated (e^x).\n",
    "- But what it represents theoritically is the difference between two probability distributions.\n",
    "- Perplexity is high when two probability distributions doesn't match, and it is low (approaching 1) when the two distributions are close to each other.\n",
    "- But this metric hardly tells you what's going on in the weights.\n",
    "    - Perplexity has faced some critisicm over the years (particularly in the language modeling community)\n",
    "        - for being overused as a metric\n",
    "- Let's look a little more closely at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra', 'moved', 'to', 'the', 'garden.']\n"
     ]
    }
   ],
   "source": [
    "sent_index = 4\n",
    "l, _ = predict(sent2indices(sentences[sent_index]))\n",
    "print(sentences[sent_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev Input:sandra      True: moved          Pred: is\n",
      "Prev Input:moved       True: to             Pred: to\n",
      "Prev Input:to          True: the            Pred: the\n",
      "Prev Input:the         True: garden.        Pred: bedroom.\n"
     ]
    }
   ],
   "source": [
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    input = sentences[sent_index][i]\n",
    "    true = sentences[sent_index][i+1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) + \"True: \" + true + (' ' * (15 - len(true))) + \"Pred: \" + pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code takes a sentence and predicts the word the model think is most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at predictions can help you understand what's going on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can look at the predictions of the neural network as it trains to understand not only what kind of patterns it picks up, but also in the order in which it does so.\n",
    "- Neural Networks tend to start off random.\n",
    "- After some amount of training, the neural network picks up the most frequent word and predicts it as a default.\n",
    "    - This is an extremely common error in recurrent neural networks.\n",
    "- It's important to know that there is almost no way this network can predict the next word perfectly.\n",
    "    - More context is needed to solve this task.\n",
    "    - But the fact that it's unsolvable, creates educational analysis for the ways in which it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Recurrent Neural Networks Predict over arbitrary Length Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this chapter, you learned how to create a vector representaion of arbitrary long sentences.\n",
    "- How does a neural network fit a variable length sequence into a fixed size vector?\n",
    "    - The truth is, sentence vectors don't encode everything in the vector.\n",
    "- The name of the game in recurrent neural networks is not just what these vectors remember, but also what they forget.\n",
    "- In the case of predicting the next word, RNNs learn that only the last few words matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
