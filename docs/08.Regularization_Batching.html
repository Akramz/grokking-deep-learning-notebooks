---

title: Learning Signal & Ignoring Noise: Introduction to Regularization & Batching
keywords: fastai
sidebar: home_sidebar


---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 08.Regularization_Batching.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this Chapter:</p>
<ul>
<li>Overfitting</li>
<li>Dropout</li>
<li>Batch Gradient Descent</li>
</ul>
<blockquote><p>"With four Parameters I can fit an Elephant, &amp; with five I can make him wiggle his trunk." â€” John von Neumann, Mathematician, Physicist, Computer Scientist, &amp; Polymath.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3-Layer-Network-on-MNIST">3-Layer Network on MNIST<a class="anchor-link" href="#3-Layer-Network-on-MNIST">&#182;</a></h2><h3 id="Let's-return-to-the-MNIST-Dataset-&amp;-Attempt-to-Classify-it-with-the-New-Network">Let's return to the MNIST Dataset &amp; Attempt to Classify it with the New Network<a class="anchor-link" href="#Let's-return-to-the-MNIST-Dataset-&amp;-Attempt-to-Classify-it-with-the-New-Network">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>How do you know the network is creating good correlation?</li>
<li>If we froze one weight, train the network until the error was sufficiently small, than unfroze the weight in an attempt to optimize against it, it won't change because the network had already learnt the existing correlation in the data.</li>
<li><strong>What if the network had figured out a way to accurately predict the games in the training dataset, but it somehow forgot to include a valuable input?</strong></li>
<li><strong>Overfitting is extremely common in Neural Networks</strong>.</li>
<li>The More Powerful the Neural Networks expressive power (more layers &amp; weights), the more prone the network is to overfit.</li>
<li>We're going to study the basics of <strong>Regularization</strong>, which is key to combatting overfitting in neural networks.</li>
<li>We're going to train our latest &amp; greatest neural network with 3 layers on the MNIST Dataset:</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">keras</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">datasets</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((60000, 28, 28), (60000,))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">];</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(1000, 784)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># one-hot encoding y</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">y_</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y_</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># same for test</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="n">y_test</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">one_hot_y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">one_hot_y_test</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="n">bottom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">v_i</span> <span class="ow">in</span> <span class="n">v</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v_i</span><span class="p">)</span><span class="o">/</span><span class="n">bottom</span> <span class="k">for</span> <span class="n">v_i</span> <span class="ow">in</span> <span class="n">v</span><span class="p">])</span>
<span class="n">relu</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">grad_relu</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> 
<span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">total_pixels</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">784</span><span class="p">,</span> <span class="mi">10</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initializing two weight matrices</span>
<span class="n">W_1</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">total_pixels</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
<span class="n">W_2</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">num_labels</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="p">))</span>
        
        <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">one_hot_y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">one_hot_y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">grad_relu</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
        
        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">layer_2_delta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">W_2</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span> 
        <span class="n">W_1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="o">+</span><span class="s2">&quot; I:&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot; Error:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; Correct:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">correct_count</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre> I:0 Error:0.802 Correct:0.501
 I:1 Error:0.451 Correct:0.766
 I:2 Error:0.270 Correct:0.837
 I:3 Error:0.203 Correct:0.88
 I:4 Error:0.168 Correct:0.891
 I:5 Error:0.145 Correct:0.912
 I:6 Error:0.128 Correct:0.921
 I:7 Error:0.115 Correct:0.929
 I:8 Error:0.104 Correct:0.936
 I:9 Error:0.095 Correct:0.945
 I:10 Error:0.086 Correct:0.952
 I:11 Error:0.079 Correct:0.96
 I:12 Error:0.072 Correct:0.962
 I:13 Error:0.065 Correct:0.967
 I:14 Error:0.059 Correct:0.972
 I:15 Error:0.053 Correct:0.974
 I:16 Error:0.048 Correct:0.977
 I:17 Error:0.043 Correct:0.984
 I:18 Error:0.038 Correct:0.986
 I:19 Error:0.034 Correct:0.99
 I:20 Error:0.030 Correct:0.992
 I:21 Error:0.026 Correct:0.993
 I:22 Error:0.023 Correct:0.996
 I:23 Error:0.020 Correct:0.996
 I:24 Error:0.017 Correct:0.996
 I:25 Error:0.014 Correct:0.997
 I:26 Error:0.012 Correct:1.0
 I:27 Error:0.010 Correct:1.0
 I:28 Error:0.009 Correct:1.0
 I:29 Error:0.008 Correct:1.0
 I:30 Error:0.007 Correct:1.0
 I:31 Error:0.006 Correct:1.0
 I:32 Error:0.005 Correct:1.0
 I:33 Error:0.005 Correct:1.0
 I:34 Error:0.004 Correct:1.0
 I:35 Error:0.004 Correct:1.0
 I:36 Error:0.003 Correct:1.0
 I:37 Error:0.003 Correct:1.0
 I:38 Error:0.002 Correct:1.0
 I:39 Error:0.002 Correct:1.0
 I:40 Error:0.002 Correct:1.0
 I:41 Error:0.002 Correct:1.0
 I:42 Error:0.002 Correct:1.0
 I:43 Error:0.001 Correct:1.0
 I:44 Error:0.001 Correct:1.0
 I:45 Error:0.001 Correct:1.0
 I:46 Error:0.001 Correct:1.0
 I:47 Error:0.001 Correct:1.0
 I:48 Error:0.001 Correct:1.0
 I:49 Error:0.001 Correct:1.0
 I:50 Error:0.001 Correct:1.0
 I:51 Error:0.001 Correct:1.0
 I:52 Error:0.000 Correct:1.0
 I:53 Error:0.000 Correct:1.0
 I:54 Error:0.000 Correct:1.0
 I:55 Error:0.000 Correct:1.0
 I:56 Error:0.000 Correct:1.0
 I:57 Error:0.000 Correct:1.0
 I:58 Error:0.000 Correct:1.0
 I:59 Error:0.000 Correct:1.0
 I:60 Error:0.000 Correct:1.0
 I:61 Error:0.000 Correct:1.0
 I:62 Error:0.000 Correct:1.0
 I:63 Error:0.000 Correct:1.0
 I:64 Error:0.000 Correct:1.0
 I:65 Error:0.000 Correct:1.0
 I:66 Error:0.000 Correct:1.0
 I:67 Error:0.000 Correct:1.0
 I:68 Error:0.000 Correct:1.0
 I:69 Error:0.000 Correct:1.0
 I:70 Error:0.000 Correct:1.0
 I:71 Error:0.000 Correct:1.0
 I:72 Error:0.000 Correct:1.0
 I:73 Error:0.000 Correct:1.0
 I:74 Error:0.000 Correct:1.0
 I:75 Error:0.000 Correct:1.0
 I:76 Error:0.000 Correct:1.0
 I:77 Error:0.000 Correct:1.0
 I:78 Error:0.000 Correct:1.0
 I:79 Error:0.000 Correct:1.0
 I:80 Error:0.000 Correct:1.0
 I:81 Error:0.000 Correct:1.0
 I:82 Error:0.000 Correct:1.0
 I:83 Error:0.000 Correct:1.0
 I:84 Error:0.000 Correct:1.0
 I:85 Error:0.000 Correct:1.0
 I:86 Error:0.000 Correct:1.0
 I:87 Error:0.000 Correct:1.0
 I:88 Error:0.000 Correct:1.0
 I:89 Error:0.000 Correct:1.0
 I:90 Error:0.000 Correct:1.0
 I:91 Error:0.000 Correct:1.0
 I:92 Error:0.000 Correct:1.0
 I:93 Error:0.000 Correct:1.0
 I:94 Error:0.000 Correct:1.0
 I:95 Error:0.000 Correct:1.0
 I:96 Error:0.000 Correct:1.0
 I:97 Error:0.000 Correct:1.0
 I:98 Error:0.000 Correct:1.0
 I:99 Error:0.000 Correct:1.0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Well,-That-was-easy!">Well, That was easy!<a class="anchor-link" href="#Well,-That-was-easy!">&#182;</a></h2><h3 id="The-Neural-Network-learned-to-predict-all-1,000-images">The Neural Network learned to predict all 1,000 images<a class="anchor-link" href="#The-Neural-Network-learned-to-predict-all-1,000-images">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We've reached 100% accuracy on the sample of 1000 images.</li>
<li>But how well will it do on an image that wasn't part of the original sample of 1,000 images?</li>
<li>Let's Evaluate the network on the test set:</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
    <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="p">))</span>
    
    <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">one_hot_y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">one_hot_y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">layer_2_delta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; Correct:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">correct_count</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Error:0.292 Correct:0.823
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Book's Results is:<ul>
<li>Error: .653</li>
<li>Correct: .7073</li>
</ul>
</li>
<li>I think that my results are better due to the use of the softmax function on the last layer.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>The Network did horribly!</strong></li>
<li>Why does it do so terribly on these new testing images when it learned to predict with a 100% accuracy on the training set?</li>
<li>This <strong>0.823</strong> is called the <strong>test accuracy</strong>.</li>
<li>This number is important because it simulates how the neural network will do in production (the real world).<ul>
<li><strong>This is the score that matters, the test accuracy.</strong></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Memorization-vs.-Generalizatio">Memorization vs. Generalizatio<a class="anchor-link" href="#Memorization-vs.-Generalizatio">&#182;</a></h2><h3 id="Memorizing-1,000-is-easier-than-generalizing-to-all-images">Memorizing 1,000 is easier than generalizing to all images<a class="anchor-link" href="#Memorizing-1,000-is-easier-than-generalizing-to-all-images">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's rewind on how a neural network actually learns:<ul>
<li>It Adjusts each number on each weight matrix so the overall error is minimized.</li>
</ul>
</li>
<li><strong>If we trained the NN to predict labels on 1,000 images, which it did perfectly, why does it work on other images at all</strong>?</li>
<li>The NN is guaranteed to work well on a new image only if the new image is nearly identical images in the training data set.<ul>
<li>Because the NN learned to transform the input data to output data for a specific data set with a specific overall configuration</li>
</ul>
</li>
<li>If The NN works only on nearly identical data points (in comparison to training set), then what's the purpose of it anyway.<ul>
<li><strong>We want a Neural Network that can work well on images different from the training set data points</strong></li>
<li>That's what we call: <strong>Generalization</strong></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overfitting-in-Neural-Networks">Overfitting in Neural Networks<a class="anchor-link" href="#Overfitting-in-Neural-Networks">&#182;</a></h2><h3 id="Neural-Networks-can-get-worse-if-you-train-them-too-much!">Neural Networks can get worse if you train them too much!<a class="anchor-link" href="#Neural-Networks-can-get-worse-if-you-train-them-too-much!">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For some reason, the test accuracy went up for the first 20 iterations &amp; then slowly decreased as the network trained more and more.</li>
<li>This is common in neural networks.</li>
<li>Overfitting is over-optimizing for the training data points,<ul>
<li>**Just like when you're molding a material for 3 forks you keep molding them until you get a very specific shape that works for all 3 but has nothing to do with the shape of a general-purpose fork.</li>
</ul>
</li>
<li><strong>You can Visualize Weights as High Dimensional Shapes</strong>.</li>
<li>As you train, this shapes molds around the shape of the data, learning one pattern after another.</li>
<li>A more official definition of a neural network that overfits:<ul>
<li>A Neural Network that has <strong>learned the noise</strong> in the dataset.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Where-Overfitting-Comes-from?">Where Overfitting Comes from?<a class="anchor-link" href="#Where-Overfitting-Comes-from?">&#182;</a></h2><h3 id="What-causes-neural-networks-to-overfit?">What causes neural networks to overfit?<a class="anchor-link" href="#What-causes-neural-networks-to-overfit?">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:50%" file="static/imgs/08/Dogs.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>Everything</strong> that makes these pictures <strong>unique</strong> <strong>beyond</strong> what captures <strong>the essense of "Dog" is</strong> included in the term <strong>"Noise"</strong>.</li>
<li>On the Left: The Pillow &amp; the background are both Noise.</li>
<li>On the Right: the Blackness can also be considered Noise.<ul>
<li>It's really the edges that tells you it's a Dog.</li>
</ul>
</li>
<li>How do you get NNs to train only on the Signal (the essense of a dog) &amp; Ignore the Noise?<ul>
<li>One Way is <strong>Early Stopping</strong>.<ul>
<li>It turns out a large amount of noise comes in the fine grained details of an image.</li>
<li>&amp; most of the signal is found on the general shape &amp; perhaps color of the image.</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Simplest-Regularization:-Early-Stopping">The Simplest Regularization: Early Stopping<a class="anchor-link" href="#The-Simplest-Regularization:-Early-Stopping">&#182;</a></h2><h3 id="Stop-training-the-network-what-it-starts-getting-worse">Stop training the network what it starts getting worse<a class="anchor-link" href="#Stop-training-the-network-what-it-starts-getting-worse">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>You don't let the network train long enough to learn the details.</strong></li>
<li><strong>Early Stopping</strong> is the cheapest form of regularization, and you're in a pinch, it can be quite effective.</li>
<li><strong>Regularization</strong> is a subset of methods for getting a model to generalize to new data points.<ul>
<li>Instead of just memorizing the training data.</li>
</ul>
</li>
<li>It's a subset of methods that helps the neural network learn the signal and ignore the noise.<ul>
<li>Often done by <strong>increasing the difficulty</strong> for a model to learn the fine-grained details in teh training data.</li>
</ul>
</li>
<li>You know how to stop by using the <strong>validation set</strong> while training, and stop when the validation score gets worse.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Industry-Standard-Regularization:-Dropout">Industry Standard Regularization: Dropout<a class="anchor-link" href="#Industry-Standard-Regularization:-Dropout">&#182;</a></h2><h3 id="The-Method:-Randomly-turn-off-neurons-(setting-them-to-0)-during-training">The Method: Randomly turn off neurons (setting them to 0) during training<a class="anchor-link" href="#The-Method:-Randomly-turn-off-neurons-(setting-them-to-0)-during-training">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>This causes the neural network to train execlusively using <strong>random subsections of the neural network</strong>.</li>
<li>This Regularization method is generally accepted as the go-to, state-of-the-art regularization technique for the vast majority of networks.</li>
<li>Its methodology is simple &amp; inexpensive, although the intuitions behind why it works are a bit more complex.</li>
<li><strong>Why Dropout Works?</strong><ul>
<li>**Dropout makes a big network train like a little one by randomly training little subsections of the network at a time.<ul>
<li><strong>and little networks don't overfit</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>The Smaller a Neural Network is, the less it's able to overfit.<ul>
<li><strong>Because small neural network have a smaller number of weights, meaning the network's hypothesis space is small</strong></li>
<li><strong>Because small neural networks don't have much expressive power</strong>.</li>
</ul>
</li>
<li>Small Neural Networks have enough room to only capture the big, obvious, high-level features.</li>
<li>The Notion of room/capacity is very important to keep in your mind.</li>
<li>Remember the Clay analogy?<ul>
<li><strong>Imagine if the clay was made of sticky rocks the size of dimes</strong>.<ul>
<li>Those stones are much like <strong>weights</strong>.</li>
</ul>
</li>
<li><strong>Now Imagine a clay made of millions &amp; millions of small stones.</strong><ul>
<li>This is a big Model.</li>
</ul>
</li>
</ul>
</li>
<li>How do you get the power of a large neural network with the resistance to overfitting of the small neural network?<ul>
<li><strong>Dropout</strong>.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-Dropout-works?-Ensembling-works?">Why Dropout works? Ensembling works?<a class="anchor-link" href="#Why-Dropout-works?-Ensembling-works?">&#182;</a></h2><h3 id="Dropout-is-a-form-of-training-a-bunch-of-networks-and-averaging-them">Dropout is a form of training a bunch of networks and averaging them<a class="anchor-link" href="#Dropout-is-a-form-of-training-a-bunch-of-networks-and-averaging-them">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Although it's likely that large, unregularized neural networks will overfit to noise, It's unlikely they will overfit to the same noise.<ul>
<li>Because they start randomly.</li>
</ul>
</li>
<li>Neural networks, even though they're randomly generated, still start by learning the biggest, most broadly sweeping features before learning much about the noise</li>
<li>If you allowed a bunch of overfitted neural networks to vote equally, their noise will tend to cancel out, revealing only what they all learned in common ..<ul>
<li><strong>The Signal</strong>.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dropout-in-Code">Dropout in Code<a class="anchor-link" href="#Dropout-in-Code">&#182;</a></h2><h3 id="Here's-how-to-use-dropout-in-the-real-world">Here's how to use dropout in the real world<a class="anchor-link" href="#Here's-how-to-use-dropout-in-the-real-world">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span>
        <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">layer_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">layer_1</span> <span class="o">*=</span> <span class="n">dropout_mask</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="p">)</span>
    
        <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">one_hot_y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">one_hot_y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">grad_relu</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
        <span class="n">layer_1_delta</span> <span class="o">*=</span> <span class="n">dropout_mask</span>
        
        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">layer_2_delta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">W_2</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span> 
        <span class="n">W_1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">test_error</span><span class="p">,</span> <span class="n">test_correct_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
            <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span>
            <span class="n">layer_2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="p">))</span>

            <span class="n">test_correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">one_hot_y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">one_hot_y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">layer_2_delta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
    
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;I:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; Test-Err:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_error</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; Test-Acc:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_correct_count</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span><span class="o">+</span> <span class="s2">&quot; Train-Err:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; Train-Acc:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">correct_count</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
I:0 Test-Err:0.0 Test-Acc:0.6 Train-Err:1.764 Train-Acc:0.386

I:10 Test-Err:0.0 Test-Acc:0.745 Train-Err:1.286 Train-Acc:0.765

I:20 Test-Err:0.0 Test-Acc:0.776 Train-Err:1.228 Train-Acc:0.81

I:30 Test-Err:0.0 Test-Acc:0.794 Train-Err:1.208 Train-Acc:0.822

I:40 Test-Err:0.0 Test-Acc:0.804 Train-Err:1.174 Train-Acc:0.843

I:50 Test-Err:0.0 Test-Acc:0.825 Train-Err:1.162 Train-Acc:0.854

I:60 Test-Err:0.0 Test-Acc:0.814 Train-Err:1.145 Train-Acc:0.866

I:70 Test-Err:0.0 Test-Acc:0.814 Train-Err:1.144 Train-Acc:0.882

I:80 Test-Err:0.0 Test-Acc:0.807 Train-Err:1.134 Train-Acc:0.864

I:90 Test-Err:0.0 Test-Acc:0.823 Train-Err:1.133 Train-Acc:0.869

I:100 Test-Err:0.0 Test-Acc:0.814 Train-Err:1.133 Train-Acc:0.866

I:110 Test-Err:0.0 Test-Acc:0.809 Train-Err:1.125 Train-Acc:0.893

I:120 Test-Err:0.0 Test-Acc:0.817 Train-Err:1.127 Train-Acc:0.879

I:130 Test-Err:0.0 Test-Acc:0.801 Train-Err:1.125 Train-Acc:0.888

I:140 Test-Err:0.0 Test-Acc:0.796 Train-Err:1.105 Train-Acc:0.896

I:150 Test-Err:0.0 Test-Acc:0.803 Train-Err:1.096 Train-Acc:0.892

I:160 Test-Err:0.0 Test-Acc:0.802 Train-Err:1.101 Train-Acc:0.902

I:170 Test-Err:0.0 Test-Acc:0.798 Train-Err:1.125 Train-Acc:0.892

I:180 Test-Err:0.0 Test-Acc:0.786 Train-Err:1.108 Train-Acc:0.904

I:190 Test-Err:0.0 Test-Acc:0.79 Train-Err:1.107 Train-Acc:0.892

I:200 Test-Err:0.0 Test-Acc:0.792 Train-Err:1.106 Train-Acc:0.896

I:210 Test-Err:0.0 Test-Acc:0.791 Train-Err:1.092 Train-Acc:0.907

I:220 Test-Err:0.0 Test-Acc:0.792 Train-Err:1.091 Train-Acc:0.89

I:230 Test-Err:0.0 Test-Acc:0.774 Train-Err:1.082 Train-Acc:0.908

I:240 Test-Err:0.0 Test-Acc:0.789 Train-Err:1.084 Train-Acc:0.902

I:250 Test-Err:0.0 Test-Acc:0.8 Train-Err:1.077 Train-Acc:0.899

I:260 Test-Err:0.0 Test-Acc:0.801 Train-Err:1.063 Train-Acc:0.916

I:270 Test-Err:0.0 Test-Acc:0.795 Train-Err:1.079 Train-Acc:0.91

I:280 Test-Err:0.0 Test-Acc:0.796 Train-Err:1.071 Train-Acc:0.916

I:290 Test-Err:0.0 Test-Acc:0.793 Train-Err:1.067 Train-Acc:0.922
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>A dropout mask uses what's called a 50% bernoulli distribution.</li>
<li>You multiply <code>layer_1</code> by 2, why do you do this ?<ul>
<li>Remember that <code>layer_2</code> will perform a weighted sum of <code>layer_1</code><ul>
<li>Even though it's weighted, It's still a sum over the values of <code>layer_1</code>.</li>
</ul>
</li>
<li>If you turn off half of the nodes in <code>layer_1</code>, <code>layer_2</code> would increase its sensitivity to <code>layer_1</code>.</li>
<li>But @ test time, you no longer would need dropout, this would throw off <code>layer_2</code> sensitivity.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>After introducing Dropout, Not only the network peak at a score of 80%, it also doesn't overfit nearly as badly.</li>
<li>Notice also that the dropout slows down training accuracy, it previously converged to 100% pretty fastly, now, it finishes at 90%.</li>
<li><strong>Dropout is Noise</strong>, we are introducing noise to the network to help it concentrate its training on the true signal and avoid memorizing data-point-specific noise.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Batch-Gradient-Descent">Batch Gradient Descent<a class="anchor-link" href="#Batch-Gradient-Descent">&#182;</a></h2><h3 id="Here's-a-Method-for-increasing-the-Speed-of-training-&amp;-the-rate-of-convergence">Here's a Method for increasing the Speed of training &amp; the rate of convergence<a class="anchor-link" href="#Here's-a-Method-for-increasing-the-Speed-of-training-&amp;-the-rate-of-convergence">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Also called: mini-batched stochastic gradient descent.</li>
<li>It's something that's largely taken for granted in neural network training.</li>
<li>It's a simple concept that doesn't get more advanced even with the most state-of-the-art neural networks.</li>
<li>Previously, we trained one training example each iteration.</li>
<li>Now, let's train 100 training examples at a time, <strong>averaging the weight updates among all 100 examples</strong>.</li>
<li>As it turns out, <strong>individual training examples are very noisy</strong> in terms of the weight updates they generate.<ul>
<li>Thus, averaging them makes for a smoother learning process.</li>
</ul>
</li>
<li>Let's do this in code:</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="o">.</span><span class="mi">001</span><span class="p">,</span> <span class="mi">300</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>prev &amp; for every epoch, we predict a data point @ a time</strong></li>
<li><strong>now &amp; for every epoch, we predict a batch @ a time</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># let&#39;s do the book implementation from scratch while debugging.</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">keras</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># getting the necessary data.</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">datasets</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="mf">255.</span><span class="p">),</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="mf">255.</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># one-hotting y</span>
<span class="n">one_hots</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">one_hots</span><span class="p">):</span>
    <span class="n">one_hots</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">one_hots</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># one-hotting y</span>
<span class="n">one_hots</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">one_hots</span><span class="p">):</span>
    <span class="n">one_hots</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">one_hots</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">grad_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="o">.</span><span class="mi">001</span><span class="p">,</span> <span class="mi">300</span>
<span class="n">pixels_per_image</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize Weights</span>
<span class="n">W_1</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">pixels_count</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span> <span class="o">-</span> <span class="o">.</span><span class="mi">1</span>
<span class="n">W_2</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">))</span> <span class="o">-</span> <span class="o">.</span><span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">error</span><span class="p">,</span> <span class="n">correct_cnt</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
        <span class="n">batch_start</span><span class="p">,</span> <span class="n">batch_end</span> <span class="o">=</span> <span class="p">((</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">),((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">))</span>

        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_0</span><span class="p">,</span><span class="n">W_1</span><span class="p">))</span>
        <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">layer_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">layer_1</span> <span class="o">*=</span> <span class="n">dropout_mask</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span><span class="n">W_2</span><span class="p">)</span>

        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span> <span class="o">-</span> <span class="n">layer_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">correct_cnt</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">batch_start</span><span class="o">+</span><span class="n">k</span><span class="p">:</span><span class="n">batch_start</span><span class="o">+</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>

            <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span><span class="o">-</span><span class="n">layer_2</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
            <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">*</span> <span class="n">grad_relu</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
            <span class="n">layer_1_delta</span> <span class="o">*=</span> <span class="n">dropout_mask</span>

            <span class="n">W_2</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span>
            <span class="n">W_1</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
            
    <span class="k">if</span><span class="p">(</span><span class="n">j</span><span class="o">%</span><span class="k">10</span> == 0):
        <span class="n">test_error</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">test_correct_cnt</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
            <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_0</span><span class="p">,</span> <span class="n">W_1</span><span class="p">))</span>
            <span class="n">layer_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">W_2</span><span class="p">)</span>

            <span class="n">test_error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">layer_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">test_correct_cnt</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>

        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> \
                         <span class="s2">&quot;I:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> \
                         <span class="s2">&quot; Test-Err:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_error</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span>\
                         <span class="s2">&quot; Test-Acc:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_correct_cnt</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span><span class="o">+</span>\
                         <span class="s2">&quot; Train-Err:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span>\
                         <span class="s2">&quot; Train-Acc:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">correct_cnt</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
I:0 Test-Err:0.815 Test-Acc:0.3832 Train-Err:1.284 Train-Acc:0.165
I:10 Test-Err:0.568 Test-Acc:0.7173 Train-Err:0.591 Train-Acc:0.672
I:20 Test-Err:0.510 Test-Acc:0.7571 Train-Err:0.532 Train-Acc:0.729
I:30 Test-Err:0.485 Test-Acc:0.7793 Train-Err:0.498 Train-Acc:0.754
I:40 Test-Err:0.468 Test-Acc:0.7877 Train-Err:0.489 Train-Acc:0.749
I:50 Test-Err:0.458 Test-Acc:0.793 Train-Err:0.468 Train-Acc:0.775
I:60 Test-Err:0.452 Test-Acc:0.7995 Train-Err:0.452 Train-Acc:0.799
I:70 Test-Err:0.446 Test-Acc:0.803 Train-Err:0.453 Train-Acc:0.792
I:80 Test-Err:0.451 Test-Acc:0.7968 Train-Err:0.457 Train-Acc:0.786
I:90 Test-Err:0.447 Test-Acc:0.795 Train-Err:0.454 Train-Acc:0.799
I:100 Test-Err:0.448 Test-Acc:0.793 Train-Err:0.447 Train-Acc:0.796
I:110 Test-Err:0.441 Test-Acc:0.7943 Train-Err:0.426 Train-Acc:0.816
I:120 Test-Err:0.442 Test-Acc:0.7966 Train-Err:0.431 Train-Acc:0.813
I:130 Test-Err:0.441 Test-Acc:0.7906 Train-Err:0.434 Train-Acc:0.816
I:140 Test-Err:0.447 Test-Acc:0.7874 Train-Err:0.437 Train-Acc:0.822
I:150 Test-Err:0.443 Test-Acc:0.7899 Train-Err:0.414 Train-Acc:0.823
I:160 Test-Err:0.438 Test-Acc:0.797 Train-Err:0.427 Train-Acc:0.811
I:170 Test-Err:0.440 Test-Acc:0.7884 Train-Err:0.418 Train-Acc:0.828
I:180 Test-Err:0.436 Test-Acc:0.7935 Train-Err:0.407 Train-Acc:0.834
I:190 Test-Err:0.434 Test-Acc:0.7935 Train-Err:0.410 Train-Acc:0.831
I:200 Test-Err:0.435 Test-Acc:0.7972 Train-Err:0.416 Train-Acc:0.829
I:210 Test-Err:0.434 Test-Acc:0.7923 Train-Err:0.409 Train-Acc:0.83
I:220 Test-Err:0.433 Test-Acc:0.8032 Train-Err:0.396 Train-Acc:0.832
I:230 Test-Err:0.431 Test-Acc:0.8036 Train-Err:0.393 Train-Acc:0.853
I:240 Test-Err:0.430 Test-Acc:0.8047 Train-Err:0.397 Train-Acc:0.844
I:250 Test-Err:0.429 Test-Acc:0.8028 Train-Err:0.386 Train-Acc:0.843
I:260 Test-Err:0.431 Test-Acc:0.8038 Train-Err:0.394 Train-Acc:0.843
I:270 Test-Err:0.428 Test-Acc:0.8014 Train-Err:0.384 Train-Acc:0.845
I:280 Test-Err:0.430 Test-Acc:0.8067 Train-Err:0.401 Train-Acc:0.846
I:290 Test-Err:0.428 Test-Acc:0.7975 Train-Err:0.383 Train-Acc:0.851</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The Concept behind this is the fact that we don't alter the network's parameters each data point prediction at a time.<ul>
<li>we instead predict a whole batch, get an average delta and update all parameters.<ul>
<li>This means we get rid of point level noise, by averging a batch of prediction, we have a better sense of what direction we should move the internal parameters.</li>
</ul>
</li>
</ul>
</li>
<li>Notice that the learning rate is 20 times larger than before.<ul>
<li>Because we are much more confident in the direction the weights should take to change.</li>
</ul>
</li>
<li>Because the example take an average of a noisy signal (the average weight change over 100 training examples), it can take bigger steps.</li>
<li>You'll generally see <strong>batching ranging from 8 to as high as 256</strong>.</li>
<li>Generally, Researchers take numbers randomly until they find a <code>batch_size</code> &amp; <code>lr</code> pair that seems to work well.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In the Following Chapters, we'll pivot from sets of tools that are universally applicable to nearly all neural networks, to special purpose architectures that are advantageous for modeling specific types of phenomena in data. </li>
</ul>

</div>
</div>
</div>
</div>
 

