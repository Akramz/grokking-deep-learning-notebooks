---

title: Modeling Probabilities & Non-Linearities: Activation Functions
keywords: fastai
sidebar: home_sidebar


---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 09.Probabilities_Non-Linearities.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this Chapter:</p>
<ul>
<li>What is an Activation Function?</li>
<li>Standard Hidden Activation Functions<ul>
<li>Sigmoid</li>
<li>Tanh</li>
</ul>
</li>
<li>Standard output activation functions<ul>
<li>Softmax</li>
</ul>
</li>
<li>Activation Function Installation Instructions</li>
</ul>
<blockquote><p>"I Know that 2 and 2 make 4 –– &amp; should be glad to prove it too if I could –– though I must say if by any sort of process I could convert 2 &amp; 2 into 5 it would give me much greater pleasure" –– George Gordon Byron, Letter to Anabella Milbanke. November 10, 1813.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-an-Activation-Function?">What is an Activation Function?<a class="anchor-link" href="#What-is-an-Activation-Function?">&#182;</a></h2><h3 id="It's-a-function-applied-to-the-neurons-in-a-layer-during-prediction.">It's a function applied to the neurons in a layer during prediction.<a class="anchor-link" href="#It's-a-function-applied-to-the-neurons-in-a-layer-during-prediction.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>You've been using an activation function called <code>ReLU</code>.</li>
<li>the ReLU function had the effect of turning all negative numbers to zero.</li>
<li>An activation function is any function that can take one number and return another number.<ul>
<li><strong>but there are an infinite number of functions in the universe, &amp; not all of them are useful as activation functions</strong>.</li>
</ul>
</li>
<li>There are many constraints on the nature of activation functions:</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:66%;" file="static/imgs/09/continuous-vs-non.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>It must have an output number for any input.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:66%;" file="static/imgs/09/monotonic.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The activation function must <strong>never change direction</strong><ul>
<li>It must be always increasing or always decreasing.</li>
</ul>
</li>
<li>This particular constraint isn't technically a requirement.<ul>
<li><strong>But consider the implication of having multiple input values map to the same output value</strong>.<ul>
<li>Multiple input values "mean" the same thing.</li>
</ul>
</li>
</ul>
</li>
<li><strong>If there are multiple ways to get the correct answer, then the network has multiple possible perfect configurations.</strong><ul>
<li>You can't know the correct direction to go, because multiple direction will give a positive signal while the optimal minima is in one location.</li>
</ul>
</li>
<li><strong>For an advanced look into this subject, look more into convex versus non-convex optimization.</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:66%;" file="static/imgs/09/non-linearity.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Linear Functions Scale Values, they don't effect how correlated a neuron is to various inputs.</li>
<li>It makes the collective correlation that's represented louder or softer.<ul>
<li>But the linear activation function <strong>doesn't allow one weight to effect how correlated the neuron is to other weights.</strong></li>
</ul>
</li>
<li>What you really want is <strong>selective correlation</strong>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Constraint-4:-Good-Activation-Functions-(&amp;-their-derivatives)-should-be-efficiently-computable">Constraint 4: Good Activation Functions (&amp; their derivatives) should be efficiently computable<a class="anchor-link" href="#Constraint-4:-Good-Activation-Functions-(&amp;-their-derivatives)-should-be-efficiently-computable">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>you'll be calling this function and its gradient a lot</li>
<li><code>relu</code> has become very popular mostly because it's (and its derivative) are effecient to compute.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Standard-Hidden-Layer-Activation-Functions">Standard Hidden-Layer Activation Functions<a class="anchor-link" href="#Standard-Hidden-Layer-Activation-Functions">&#182;</a></h2><h3 id="Of-the-infinite-possible-functions,-which-ones-are-most-commonly-used?">Of the infinite possible functions, which ones are most commonly used?<a class="anchor-link" href="#Of-the-infinite-possible-functions,-which-ones-are-most-commonly-used?">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:33%;" file="static/imgs/09/sigmoid.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>sigmoid is great because it smoothly squinshes the infinite amount of input to an output between 0 and 1.</li>
<li>This lets you interpret the output of any neuron as a <strong>probability</strong>.</li>
<li>People use this non-linearity both in hidden and outputs layers.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:33%;" file="static/imgs/09/tanh.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>tanh is the same as sigmoid except it's between -1 and 1.</li>
<li>This means it can also throw in some <strong>negative</strong> correlation.</li>
<li><strong>this aspect of negative correlation is powerful for hidden layers.</strong></li>
<li><strong>on many problems, tanh will outperform sigmoid in hidden layers.</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Standard-output-layer-activation-functions">Standard output layer activation functions<a class="anchor-link" href="#Standard-output-layer-activation-functions">&#182;</a></h2><h3 id="Choosing-the-best-one-depends-on-what-you're-trying-to-predict">Choosing the best one depends on what you're trying to predict<a class="anchor-link" href="#Choosing-the-best-one-depends-on-what-you're-trying-to-predict">&#182;</a></h3><ul>
<li>Broadly speaking, there are 3 major types of output layers.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Configuration-1:-Predicting-Raw-Data-Values-(Regression)-&#8212;-No-activation-function">Configuration 1: Predicting Raw Data Values (Regression) &#8212; No activation function<a class="anchor-link" href="#Configuration-1:-Predicting-Raw-Data-Values-(Regression)-&#8212;-No-activation-function">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>One example might be predicting the average temperature in colorado given the average temperature in surrounding states.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Configuration-2:-Predicting-Unrelated-Yes/No-Probabilities-(Binary-Classification)-&#8212;-Sigmoid">Configuration 2: Predicting Unrelated Yes/No Probabilities (Binary Classification) &#8212; Sigmoid<a class="anchor-link" href="#Configuration-2:-Predicting-Unrelated-Yes/No-Probabilities-(Binary-Classification)-&#8212;-Sigmoid">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>It's best to use the sigmoid function, Because it models individual probabilities separately for each output node.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Configuration-3:-Predicting-which-one-probabilities-(Categorical-Classification)-&#8212;-Softmax">Configuration 3: Predicting which-one probabilities (Categorical Classification) &#8212; Softmax<a class="anchor-link" href="#Configuration-3:-Predicting-which-one-probabilities-(Categorical-Classification)-&#8212;-Softmax">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>By far the common use case in neural networks is predicting a single label out of many. </li>
<li>It's better to have an activation function that models the idea that "The more likely it's one label, The less likely it's any of the other labels".</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:33%;" file="static/imgs/09/similarity.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The average 2 shares quite a bit with the average 3.</li>
<li>As a general rule, similar inputs create similar outputs.<ul>
<li>When you take some numbers and multiply them by a matrix, if the starting numbers are pretty similar, the ending numbers will be pretty similar.</li>
</ul>
</li>
<li>Sigmoid will penelize the network for recognizing a 2 by anything other than features that are exclusively related to 2s.<ul>
<li>It penelizes the network for recognizing a 2 based on, say, the top curve.</li>
</ul>
</li>
<li>Most Images share lots of pixels in the middle of images.<ul>
<li><strong>So the network will start trying to focus on the edges</strong>.</li>
</ul>
</li>
<li>As you can see on the Weight Image, The light areas are probably the best individual indicators of a 2.<ul>
<li><strong>But the best overall is a network that sees the entire shape for what it is.</strong></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Softmax-Computation">Softmax Computation<a class="anchor-link" href="#Softmax-Computation">&#182;</a></h2><h3 id="Softmax-raises-each-input-value-exponentially-and-then-divides-by-the-layer's-sum">Softmax raises each input value exponentially and then divides by the layer's sum<a class="anchor-link" href="#Softmax-raises-each-input-value-exponentially-and-then-divides-by-the-layer's-sum">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The nice thing about softmax is that the higher the network predicts one value, the lower it predicts all the others.</li>
<li><strong>It encourages the network to predict one output with very high probability.</strong><ul>
<li>To adjust how aggresively it does this:<ul>
<li>use numbers slightly higher or lower than $e$.<ul>
<li>Lower numbers will result in lower <em>attenuation</em> &amp; higher numbers will result in bigger <em>attenuation</em>.</li>
</ul>
</li>
</ul>
</li>
<li>but most people will stick to $e$.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Activation-Installation-Instructions">Activation Installation Instructions<a class="anchor-link" href="#Activation-Installation-Instructions">&#182;</a></h2><h3 id="How-do-you-add-your-favorite-activation-function-to-any-layer?">How do you add your favorite activation function to any layer?<a class="anchor-link" href="#How-do-you-add-your-favorite-activation-function-to-any-layer?">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The Input to a layer refers to the value before the nonlinearity.</li>
<li>The Slope of ReLU for positive numbers is exactly 1.</li>
<li>The Slope of ReLU for negative numbers is exactly 0.</li>
<li>Modifying the input to this function (by a tiny amount) will have a 1:1 effect if it was predicting positively.<ul>
<li>&amp; a 0:1 effect if it was predicting negatively.</li>
</ul>
</li>
<li>This slope is a measure of how much the output of relu will change given a change in the input.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Thus, when you backpropagate, in order to generate <code>layer_1_delta</code>, multiply the backpropagated <code>delta</code> from <code>layer_2</code> (<code>layer_2_delta.dot(W_2.T)</code>) by the slope of ReLU at the point predicted in forward propagation.</li>
<li>For some deltas the slope is 1 (positive numbers) &amp; for others it's 0 (negative numbers).</li>
<li><strong>The important thing to remember is that the slop is an indicator of how much a tiny change to the input effects the output.</strong></li>
<li><strong>The update effect encourages the network to leave weights alone if adjusting them will have little to no effect</strong>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multiplying-Delta-by-the-Slope">Multiplying Delta by the Slope<a class="anchor-link" href="#Multiplying-Delta-by-the-Slope">&#182;</a></h2><h3 id="To-compute-layer_delta,-multiply-the-backpropagated-delta-by-the-layer's-slope">To compute layer_delta, multiply the backpropagated delta by the layer's slope<a class="anchor-link" href="#To-compute-layer_delta,-multiply-the-backpropagated-delta-by-the-layer's-slope">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:33%;" file="static/imgs/09/layer_delta.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:66%;" file="static/imgs/09/activations.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Converting-Output-to-Slope-(Derivative)">Converting Output to Slope (Derivative)<a class="anchor-link" href="#Converting-Output-to-Slope-(Derivative)">&#182;</a></h2><h3 id="Most-great-activations-can-convert-their-output-to-their-slope">Most great activations can convert their output to their slope<a class="anchor-link" href="#Most-great-activations-can-convert-their-output-to-their-slope">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html style="width:75%;" file="static/imgs/09/Pariston.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Upgrading-the-MNIST-Network">Upgrading the MNIST Network<a class="anchor-link" href="#Upgrading-the-MNIST-Network">&#182;</a></h2><h3 id="Let's-Upgrade-the-MNIST-Network-to-reflect-what-you've-learned.">Let's Upgrade the MNIST Network to reflect what you've learned.<a class="anchor-link" href="#Let's-Upgrade-the-MNIST-Network-to-reflect-what-you've-learned.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Theoretically, the <code>tanh</code> function should make for a better hidden-layer activation</li>
<li>and <code>softmax</code> should make for a better output layer activation function.<ul>
<li>But things aren't always as simple as they seem.</li>
</ul>
</li>
<li>For <code>Tanh</code> i had to reduce the standard diviation for the incoming weights.<ul>
<li>I adjusted weigth values to be between -.01 &amp; +.01.</li>
</ul>
</li>
<li>I Had to revisit the <strong>Alpha</strong> tuning:</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">keras</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((1000, 784), (1000,))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">one_hot_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">one_hot_labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(1000, 10)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">one_hot_labels</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">one_hot_labels</span>
<span class="n">labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(1000, 10)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_images</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">/</span><span class="mi">255</span>
<span class="n">test_images</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(10000, 784)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">one_hot_test_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">one_hot_test_labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(10000, 10)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">one_hot_test_labels</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">one_hot_test_labels</span>
<span class="n">test_labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(10000, 10)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># activation functions.</span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tanh2deriv</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">output</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">temp</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">pixels_per_image</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span> <span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">W_1</span> <span class="o">=</span> <span class="mf">0.02</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">pixels_per_image</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.01</span>
<span class="n">W_2</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">num_labels</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
<span class="n">W_1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">W_2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((784, 100), (100, 10))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Training Loop</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>  <span class="c1"># epoches</span>
    <span class="n">correct_cnt</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>  <span class="c1"># batches</span>
        <span class="n">batch_start</span><span class="p">,</span> <span class="n">batch_end</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">),</span> <span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        
        <span class="c1"># Forward Propagation</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_0</span><span class="p">,</span> <span class="n">W_1</span><span class="p">))</span>
        <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">layer_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">layer_1</span> <span class="o">*=</span> <span class="n">dropout_mask</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">W_2</span><span class="p">))</span>
        
        <span class="c1"># benchmarking</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">correct_cnt</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">batch_start</span><span class="o">+</span><span class="n">k</span><span class="p">:</span><span class="n">batch_start</span><span class="o">+</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="c1"># backpropagation</span>
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span> <span class="o">-</span> <span class="n">layer_2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">layer_2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">tanh2deriv</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
        <span class="n">layer_1_delta</span> <span class="o">*=</span> <span class="n">dropout_mask</span>
        
        <span class="c1"># optimization</span>
        <span class="n">W_2</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span>
        <span class="n">W_1</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
    
    <span class="n">test_correct_cnt</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_images</span><span class="p">)):</span>  <span class="c1"># test images</span>
        <span class="c1"># predict</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">test_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_0</span><span class="p">,</span><span class="n">W_1</span><span class="p">))</span> 
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span><span class="n">W_2</span><span class="p">)</span> 
        
        <span class="c1"># benchmark</span>
        <span class="n">test_correct_cnt</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_labels</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">j</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span> <span class="s2">&quot;I:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; Test-Acc:&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">test_correct_cnt</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_images</span><span class="p">)))</span><span class="o">+</span><span class="s2">&quot; Train-Acc:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">correct_cnt</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
I:0 Test-Acc:0.3806 Train-Acc:0.154

I:10 Test-Acc:0.677 Train-Acc:0.699

I:20 Test-Acc:0.7042 Train-Acc:0.734

I:30 Test-Acc:0.7408 Train-Acc:0.77

I:40 Test-Acc:0.7724 Train-Acc:0.796

I:50 Test-Acc:0.7932 Train-Acc:0.816

I:60 Test-Acc:0.8101 Train-Acc:0.841

I:70 Test-Acc:0.8232 Train-Acc:0.849

I:80 Test-Acc:0.8295 Train-Acc:0.876

I:90 Test-Acc:0.8347 Train-Acc:0.879

I:100 Test-Acc:0.8402 Train-Acc:0.883

I:110 Test-Acc:0.8429 Train-Acc:0.897

I:120 Test-Acc:0.8467 Train-Acc:0.896

I:130 Test-Acc:0.8496 Train-Acc:0.91

I:140 Test-Acc:0.8525 Train-Acc:0.904

I:150 Test-Acc:0.8537 Train-Acc:0.911

I:160 Test-Acc:0.8568 Train-Acc:0.921

I:170 Test-Acc:0.8588 Train-Acc:0.911

I:180 Test-Acc:0.8603 Train-Acc:0.924

I:190 Test-Acc:0.8607 Train-Acc:0.926

I:200 Test-Acc:0.8633 Train-Acc:0.929

I:210 Test-Acc:0.8634 Train-Acc:0.928

I:220 Test-Acc:0.865 Train-Acc:0.937

I:230 Test-Acc:0.8668 Train-Acc:0.935

I:240 Test-Acc:0.8666 Train-Acc:0.929

I:250 Test-Acc:0.8684 Train-Acc:0.935

I:260 Test-Acc:0.8682 Train-Acc:0.948

I:270 Test-Acc:0.8707 Train-Acc:0.941

I:280 Test-Acc:0.869 Train-Acc:0.943

I:290 Test-Acc:0.8704 Train-Acc:0.944
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section, we're going to make sure that we understand batch stochastic gradient descent + the new activation function by <strong>Understanding the Implementation</strong> of both:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">datasets</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load Data.</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># light data pre-processing</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span><span class="o">/</span><span class="mf">255.</span><span class="p">),</span> <span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># one hotting `y_train`</span>
<span class="n">labels_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_train</span><span class="p">):</span>
    <span class="n">labels_train</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">labels_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># same to testing data.</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span><span class="o">/</span><span class="mf">255.</span>
<span class="n">labels_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">labels_test</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">labels_test</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((1000, 784), (1000, 10), (10000, 784), (10000, 10))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># activation functions.</span>
<span class="k">def</span> <span class="nf">ReLU</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
<span class="k">def</span> <span class="nf">grad_ReLU</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tanh2deriv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Softmax works when we provide it with lines and rows, each line represents a sample ..</span>
<span class="sd">     ... &amp; each row is a feature.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">temp</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># configuration parameters</span>
<span class="n">lr</span><span class="p">,</span> <span class="n">epoches</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span>
<span class="n">pixels_count</span><span class="p">,</span> <span class="n">labels_count</span> <span class="o">=</span> <span class="mi">784</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Random Wights Initialization</span>
<span class="n">W_0</span> <span class="o">=</span> <span class="mf">0.02</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.01</span>
<span class="n">W_1</span> <span class="o">=</span> <span class="mf">0.02</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.01</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">W_0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">W_1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((784, 100), (100, 10))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># now to the training loop</span>
<span class="c1"># Add -&gt; Dropout!</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
    
    <span class="c1"># cuz each epoch passes through all training data, we calc error each epoch</span>
    <span class="n">correct_count</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">batch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)):</span>
        <span class="c1"># get batch</span>
        <span class="n">batch_start</span><span class="p">,</span> <span class="n">batch_end</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">),</span> <span class="p">((</span><span class="n">batch_i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">labels_train</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
        
        <span class="c1"># forward propagation</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">X</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_0</span><span class="p">,</span> <span class="n">W_0</span><span class="p">))</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span> <span class="n">W_1</span><span class="p">))</span>
        
        <span class="c1"># Benchmarking, loop over the batch</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># we want to loop over the batch images.</span>
            <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">y_i_hat</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer_2</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_i_hat</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_i</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()):</span>
                <span class="n">correct_count</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">correct_count</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># backpropagation</span>
        <span class="c1"># TODO: dividing by 10,000 solved the problem .. KNOW WHY!</span>
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># / (batch_size * layer_2.shape[0])</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">grad_ReLU</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
        
        <span class="c1"># Optimization</span>
        <span class="n">W_1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">))</span>
        <span class="n">W_0</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">))</span>
        
    <span class="n">test_correct_count</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># evaluate over test dataset.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># get data</span>
        <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">labels_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># forward propagation</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_i</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_0</span><span class="p">))</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_i</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()):</span>
            <span class="n">test_correct_count</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">test_correct_count</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span> <span class="s2">&quot;Epoch:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> \
              <span class="s2">&quot; Test-Acc:&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_correct_count</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_correct_count</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span> \
              <span class="s2">&quot; Train-Acc:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">correct_count</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">correct_count</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp
  
/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide
  from ipykernel import kernelapp as app
/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater
  This is separate from the ipykernel package so we can avoid doing imports until
/Users/mohamedakramzaytar/.envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater
  &#34;&#34;&#34;
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Epoch:0 Test-Acc:0.098 Train-Acc:0.088

Epoch:10 Test-Acc:0.098 Train-Acc:0.097

Epoch:20 Test-Acc:0.098 Train-Acc:0.097

Epoch:30 Test-Acc:0.098 Train-Acc:0.097

Epoch:40 Test-Acc:0.098 Train-Acc:0.097

Epoch:50 Test-Acc:0.098 Train-Acc:0.097

Epoch:60 Test-Acc:0.098 Train-Acc:0.097

Epoch:70 Test-Acc:0.098 Train-Acc:0.097

Epoch:80 Test-Acc:0.098 Train-Acc:0.097

Epoch:90 Test-Acc:0.098 Train-Acc:0.097
</pre>
</div>
</div>

</div>
</div>

</div>
</div>
 

